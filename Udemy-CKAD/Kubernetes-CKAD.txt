Resources:
https://github.com/lucassha/CKAD-resources
https://github.com/dgkanatsios/CKAD-exercises


PODS
===============================
- kubernetes ultimate aim is to deploy applications in the form of containers on a set of machines that are configured as worker nodes in a cluster. however k8s does not deploy containers directly on node, containers are encapsulated in kubernets object known as PODs.

run an image into pod. image name: nginx and pod name: nginx. pulls the image from docker hub and runs within pod.
$ kubectl run nginx --image nginx 

$ kubectl run mosquito --image nginx
pod/mosquito created

$ kubectl run bee --image=nginx --restart=Never --dry-run=client -o yaml > bee-pod.yaml


- to know the Pod configs help. 
$ kubectl explain pod --recursive | less

$ kubectl explain pod --recursive | grep -A5 tolerations
tolerations       <[]Object>
	 effect <string>
	 key    <string>
	 operator       <string>
	 tolerationSeconds      <integer>
	 value  <string>


- list of pods available
$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
bee        1/1     Running   0          3m
mosquito   0/1     Pending   0          15m

- shows more details:
$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE    IP           NODE     NOMINATED NODE   READINESS GATES
bee        1/1     Running   0          3m5s   10.244.1.2   node01   <none>           <none>
mosquito   0/1     Pending   0          15m    <none>       <none>   <none>           <none>

$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
bee        1/1     Running   0          8m34s   10.244.1.2   node01         <none>           <none>
mosquito   1/1     Running   0          20m     10.244.0.4   controlplane   <none>           <none>


create pod
- kubectl create -f <pod-definition yml file name>
- kubectl run <pod_name> --image <image_name>  // pod name and container name will be the given <pod_name>

describe pod and look at its containers
- kubectl describe pod <pod_name>


create a manifest file using dryrun flag, dry-run=client allows not create the pod instead redirect the definition to yaml file.
- kubectl run redis --image=redis123 --dry-run=client -o yaml < redis-pod-definition.yaml
- kubectl apply -f <redis-pod-definition.yaml>  // create the pod using apply

edit and rerun pod creation
- kubectl apply -f <pod-definition yml file name>
or
- kubectl edit pod <pod_name>  // it opens the manifest in vi editor, (i) change the file and save (escape + :wq!)
kubernetes will immediately trigger a re-deploy of the pod, check the state and events using 'kubectl get pod <pod_name>' and 'kubectl describe pod <pod_name>'


If you are not given a pod definition file, you may extract the definition to a file using the below command, Then edit the file to make the necessary changes, delete and re-create the pod.
- kubectl get pod <pod-name> -o yaml > pod-definition.yaml


- Opening a shell when a Pod has more than one container
	- --container or -c to specify a container, If omitted, the first container in the pod will be chosen
	- -i, --stdin=false: Pass stdin to the container
	- -t, --tty=false: Stdin is a TTY
	- $ kubectl exec --help

$ kubectl exec -i -t my-pod --container main-app -- /bin/bash


- viwing logs
$ kubectl exec -i -t webapp -- cat /log/app.log

- list the containers under a pod.
$ kubectl get pods <pod_name> -o jsonpath='{.spec.containers[*].name}'
simple-webapp db


- list pods withour the header column in output
$ kubectl get pods --no-headers

- select specific pods by it's labels.
$ kubectl get pod --no-headers --selector bu=finance | wc -l


- get pods on all namespaces
$ kubectl get pods --all-namespaces


- show pods by its label
$ kubectl get pods -l <label-key>=<label-value>
ex:
$ kubectl get pods -l name=internal


- shows pods along with its labels
$ kubectl get pods --show-labels

NAME       READY   STATUS              RESTARTS   AGE   LABELS
external   0/1     ContainerCreating   0          22s   name=external
internal   0/1     ContainerCreating   0          21s   name=internal
mysql      0/1     ContainerCreating   0          22s   name=mysql
payroll    0/1     ContainerCreating   0          21s   name=payroll




Manifest file
------------------
- every k8s manifest files have 4 sections
- apiVersion, kind, metadata, spec.
	- apiVersion is the K8s API version using which we create any object of given kind.
	- kind is various type of objects available in kubernetes.
	- metadata is like a dictionary, has kubernetes defined set of key values pairs, name, labels (label can have user defined key-value pair)
	- spec is unique to each type of kind. it defines whats we are creating inside the object if the given kind.
	
	
ReplicaSets
===================================
- Controllers are the brain behind kubernetes, these are processes that monitor kubernetes objects and responds accordingly.
- one of such controller is 'Replication Controller'

- we need more than one pod instance to increase availability of our application  in case some pod fails or node crash.
- Replication Controller helps in running multiple instances of a single pod in kubernetes cluster thus provinding high availability.
- even we have a single pod the replication controller can help by automatically bringing up a new pod when the existing one fails, ensures the specified no. of pods are running all times.

- Replication controller used to create mutiple pod to share the application load balancing.

- Replication controller and Replica Set both have the same purpose but they are not same. Replication controller is older technology that is been replaced by Replica Set. Replica Set is new recommended way to setup replication.

Replication controller:
------------------------

rc-definition.yml
-------------------
apiVersion: v1
kind: ReplicationController
metedata: 
	name: myapp-rc
	labels: 
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-pod
			labels:
				app: myapp
				type: front-end
		spec:
			containers:
			-	name: nginx-container
				image: nginx			
	replicas:3
	
in above line 67-75 is talen from the below pod-definition.yaml (line 87-95)

pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
	labels:
		app: myapp
		type: front-end
spec:
	containers:
	-	name: nginx-container
		image: nginx
		

- kubectl create -f rc-definition.yml //this creates replication controller by first creating the pod.

- kubectl get replicationcontroller // shows desired/current/ready no. of pod replicas.

- kubectl get pods // shows 3 pod running. names for these pod starts with replication controller name i.e. myapp-rc-* indicating they are created via replication controller


Replica Set:
------------------------
- very similar to replication controller. the spec has template section to provide the pod definition.

- it requires a selector section to indentify whats pods fall under it, but why need to add the selector even if we provided the pod definition under its spec, its because, replica set can also manage pods that are not created as part of replica set creation. pods created before replica set, matched with the labels specified in selector, replica set include those pods for replication.

	- we still need to add the pod spec even we must add selector, as in case any of these pod dies, replica set creates a new one by using the pod spec given under template section.

- selector section is mandatory for replica set but optional for replication controller (it makes the selector same as given pod definition)

replicaset-definition.yml
--------------------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
	name: myapp-replicaset
	labels:
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-pod
			labels:
				app: myapp
				type: front-end
		spec:
			containers:
			-	name: nginx-container
				image: nginx
	replicas: 3
	selector: 
		matchLabels:
			type: front-end
			
- kubectl create -f replicaset-definition.yml

- kubectl get replicasets // shows desired/current/ready no. of pod replicas.
NAME              DESIRED   CURRENT   READY   AGE
new-replica-set   4         4         0       16m
replicaset-1      2         2         2       3m21s
replicaset-2      2         2         2       58s

- kubectl describe replicasets new-replica-set // to the image used to create the pods using replicaset

- kubectl get pods

- kubectl delete replicaset <replicaset_name>
replicaset.apps "replicaset-1" deleted

steps to edit any ecisting replicaset:
- kubectl edit replicaset new-replica-set 
replicaset.apps/new-replica-set edited

- kubectl get pods
NAME                    READY   STATUS             RESTARTS   AGE
new-replica-set-rpfr7   0/1     ImagePullBackOff   0          20m
new-replica-set-gvvlv   0/1     ImagePullBackOff   0          20m
new-replica-set-h9t8f   0/1     ImagePullBackOff   0          20m
new-replica-set-49d4f   0/1     ImagePullBackOff   0          12m

delete all pods.
- kubectl delete pod new-replica-set-rpfr7
pod "new-replica-set-rpfr7" deleted

- kubectl delete pod new-replica-set-gvvlv
pod "new-replica-set-gvvlv" deleted

- kubectl delete pod new-replica-set-h9t8f
pod "new-replica-set-h9t8f" deleted

- kubectl delete pod new-replica-set-49d4f
pod "new-replica-set-49d4f" deleted

- kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
new-replica-set-z9xf6   1/1     Running   0          46s
new-replica-set-jgwtr   1/1     Running   0          35s
new-replica-set-kknnd   1/1     Running   0          21s
new-replica-set-q4lm8   1/1     Running   0          9s

- kubectl get replicasets.apps 
NAME              DESIRED   CURRENT   READY   AGE
new-replica-set   4         4         4       23m



- ways to sclae the replicas.
	1/
	- update the replica set manifest file to scale the replicas. ex: from 3 to 6
	- kubectl replace -f replicaset-definition.yml
	
	2/
	- kubectl scale --replicas=6 -f replicaset-definition.yml
	
	3/
	- kubectl scale --replicas=6 replicaset myapp-replicaset  // type (replicaset) and value (name of replicaset)
	
	4/
	- kubectl edit replicaset new-replica-set // vi editor and change the replicas and save.
	
	
	
Deployments
=====================================
- Pods which deploy single instance of our application such as webapp, each container is encapsulated withing pods, multiple such pods are deployed through replicaset or replication controller.
- then comes deployment which is kubernetes object that comes higher in hierarchy, deployment provides us with the capability to upgrade the underlying instances seamlessly with rolling updates, roll backs.
- deployment manifest looks similar to replicaset.

deployment-definition.yaml
---------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
	name: myapp-deployment
	labels:
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-pod
			labels:
				app: myapp
				type: front-end
		spec:
			containers:
			-	name: nginx-container
				image: nginx
	replicas: 3
	selector: 
		matchLabels:
			type: front-end
			

kubectl create/apply creates the deployment which automatically creates a replicaset in the name of depoloyment (ex: myapp-deployment-56d8ff5458), the replicaset ultimately create pods and the pods names are in the name of replicaset (ex: myapp-deployment-56d8ff5458-5hjhr, myapp-deployment-56d8ff5458-5gb8q)
- kubectl create -f deployment-definition.yaml
- kubectl get deployments
- kubectl get replicasets
- kubectl get pods

- kubetcl get all // to see all the objects in the given namespace.

- kubectl describe deployments.apps frontend-deployment
Name:                   frontend-deployment
Namespace:              default
CreationTimestamp:      Tue, 21 Dec 2021 17:18:10 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=busybox-pod
Replicas:               4 desired | 4 updated | 4 total | 0 available | 4 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=busybox-pod
  Containers:
   busybox-container:
    Image:      busybox888
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      echo Hello Kubernetes! && sleep 3600
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   frontend-deployment-56d8ff5458 (4/4 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  5m30s  deployment-controller  Scaled up replica set frontend-deployment-56d8ff5458 to 4
  
  
- kubectl describe deployments.apps frontend-deployment | grep -i image // to know image used in deployment.


deployment-definition-1.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  replicas: 2
  selector:
    matchLabels:
      name: busybox-pod
  template:
    metadata:
      labels:
        name: busybox-pod
    spec:
      containers:
      - name: busybox-container
        image: busybox888
        command:
		- sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600	
		

- kubectl [command] [TYPE] [NAME] -o <output_format>
Here are some of the commonly used formats:

-o jsonOutput a JSON formatted API object.
-o namePrint only the resource name and nothing else.
-o wideOutput in the plain-text format with any additional information.
-o yamlOutput a YAML formatted API object.

- kubectl create namespace test-123 --dry-run -o yaml
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: test-123
spec: {}
status: {}

Output with wide (additional details):
Probably the most common format used to print additional details about the object:
- kubectl get pods -o wide
NAME      READY   STATUS    RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
busybox   1/1     Running   0          3m39s   10.36.0.2   node01   <none>           <none>
ningx     1/1     Running   0          7m32s   10.44.0.1   node03   <none>           <none>
redis     1/1     Running   0          3m59s   10.36.0.1   node01   <none>           <none>


namespace
=======================================
- kubernetes automatically creates a namespace during the cluster setup, known 'default' and all the objects gets created in it if not specified explicitly.

- kubernetes also creates a set of pods and services for its internal purpose such as those required by its network solutions, DNS service etc, kubernets creates these interla objects in its own namespace (kube-system) to prevent from users accidentally deleting or modifying.

- A 3rd namespace created by kubernetes: kube-public where such resources that should be made available to all users are created.

- we can create our own namespace to isolate resources/objects among them.

- each of these namespaces can have its own set of policies that define who can do what

- can also define quota to each namespaces to limit the certain amount of resource usage.

- just like members within a house can refer them by simply calling their name, objects within namespace can refer them by their name. ex: webapp-pod can use the db-service name to establish jdbc connection. i.e. mysql.connect("db-service")

	- object can also refer another object cross namespace by using the namespace_name.object_name
	ex: mysql.connect("db-service.dev-namespace.svc.cluster.local"), here: dev-namespace - the other namespace, svc - sub-domain for service, cluster.local is the default domain name, db-service is the name of service.

- kubectl get pods // shows pods from default namespace.
- kubectl get pods --namespace=<namespace_name>	
- kubectl create -f pod-definition.yaml -n dev-namespace

- can also move the namespace into the manifest file.

pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
	namespace: dev
	name: myapp-pod
	labels:
		app: myapp
		type: front-end
spec:
	containers:
	-	name: nginx-container
		image: nginx


namespace-definition.yaml
-------------------------
apiVersion: v1
kind: Namespace
metadata:
	name: dev
	
- kubectl create -f namespace-definition.yaml
or
- kubectl create namespace <namespace_name>

 
Resource Quota
=====================================
- to limit resources in a given namespace, we need to create a resource quota.

resource-quota-definition.yaml
------------------------------
apiVersion: v1
kind: ResourceQuota
metadata:
	name: compute-quota
	namespace: dev
spec:
	hard: 
		pods: "10"
		requests.cpu: "4"
		requests.memory: 5Gi
		limits.cpu: "10"
		limits.memory: 10Gi
		

- kubectl create -f resource-quota-definition.yaml
	

KUBECTL imperative command
======================================
- kubectl create namespace <namespace_name>

- kubectl config current-context // Displays the current-context
- kubectl config set-context $(kubectl config current-context) --namespace=dev  // gets the current context and set the namespace to it.

- kubectl get pods --all-namespace  // lists all pods from all namepaces

- kubectl get namespaces

- kubectl run redis --image redis --namespace finance // creates a pod in a given namespace
- kubectl run redis --image redis --dry-run=client -o yaml > pod-redis.yaml  // now open the yaml file add the namespace and create.

- kubectl get ns --no-headers | wc -l // counst all namaspaces
- kubectl -n <namespace_name> get pods --no-headers

- kubectl get pods --all-namespaces | grep blue  // Which namespace has the blue pod in it?
marketing       blue                                     1/1     Running            0               9m38s
 
- kubectl run nginx --image=nginx --dry-run=client -o yaml  // Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

- kubectl create deployment --image=nginx nginx --dry-run -o yaml > nginx-deployment.yaml  // Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

- kubectl create deployment nginx --image=nginx --replicas=4  // Generate Deployment with 4 Replicas

- kubectl scale deployment nginx --replicas=5  //  scale a deployment using the kubectl scale command.
 
Deploy a redis pod using the redis:alpine image with the labels set to tier=db 
- kubectl run redis --image redis:alpine --labels tier=db

Create a service redis-service to expose the redis application within the cluster on port 6379
- kubectl expose pod redis --name redis-service --port 6379 --target-port 6379
- kubectl describe svc redis-service 
Name:              redis-service
Namespace:         default
Labels:            tier=db
Annotations:       <none>
Selector:          tier=db
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.43.154.70
IPs:               10.43.154.70
Port:              <unset>  6379/TCP
TargetPort:        6379/TCP
Endpoints:         10.42.0.10:6379
Session Affinity:  None
Events:            <none>


Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas, webapp is deployment name.
- kubectl create deployment webapp --image=kodekloud/webapp-color
deployment.apps/webapp created

- kubectl get deployments webapp
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
webapp   1/1     1            1           62s

- kubectl scale deployment --replicas=3 webapp
deployment.apps/webapp scaled

- kubectl get deployments webapp
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
webapp   3/3     3            3           108s


Create a new pod called custom-nginx using the nginx image and expose it on container port 8080.
- kubectl run custom-nginx --image nginx --port 8080
pod/custom-nginx created

Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2 replicas.
- kubectl --namespace dev-ns create deployment redis-deploy --image redis --replicas 2
deployment.apps/redis-deploy created
- kubectl -n dev-ns get deployments.apps redis-deploy 
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
redis-deploy   2/2     2            2           16s


Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80.
--expose: creates a service for the pod.
- kubectl run httpd --image httpd:alpine --port 80 --expose --dry-run=client -o yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  name: httpd
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: httpd
status:
  loadBalancer: {}
---
---
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: httpd
  name: httpd
spec:
  containers:
  - image: httpd:alpine
    name: httpd
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}



Image and Docker vs Kubernetes
=================================
- lets say we have an image: ubuntu-sleeper

FROM ubuntu
ENTRYPOINT["sleep"]
CMD["5"]

docker
---------
$ docker run --name ubuntu-sleeper --entrypoint sleep2.0 ubuntu-sleeper 10

kubernetes
-----------
pod-definition.yaml

apiVersion: v1
kind: Pod
metadata: 
	name: ubuntu-sleeper-pod
spec:
	containers:
	-	name: ubuntu-sleeper
		image: ubuntu-sleeper
		command: ["sleep2.0"]   // this is corresponding to --entrypoint sleep2.0 in docker world i.e. overriding the command used in ENTRYPOINT instruction in docker file.
		args: ["10"]  // this to override the args mentioned in CMD instruction in docker file.


------------------------
example1:

Dockerfile2
------------
FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]


webapp-color-pod-2.yaml
-----------------------
apiVersion: v1
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]
	

pod-green.yaml // Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green
-----------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: webapp-green
  name: webapp-green
spec:
  containers:
  - image: kodekloud/webapp-color
    name: webapp-green
    args: [--color=green]
  dnsPolicy: ClusterFirst
  restartPolicy: Always


Editing PODs and Deployments
======================================
Edit a POD
-----------
Remember, you CANNOT edit specifications of an existing POD other than the below.

spec.containers[*].image
spec.initContainers[*].image
spec.activeDeadlineSeconds
spec.tolerations

For example you cannot edit the environment variables, service accounts, resource limits
But if you really want to, you have 2 options:

1/
Run the kubectl edit pod <pod name> command.  This will open the pod specification in an editor (vi editor). Then edit the required properties. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable.

A copy of your changes has been stored to "/tmp/kubectl-edit-1554090010.yaml"
error: Edit cancelled, no valid changes were saved.

Delete the existing pod by running the command:
- kubectl delete pod webapp

Then create a new pod with your changes using the temporary file
- kubectl create -f /tmp/kubectl-edit-ccvrq.yaml


2/
The second option is to extract the pod definition in YAML format to a file using the command
- kubectl get pod webapp -o yaml > my-new-pod.yaml

Then make the changes to the exported file using an editor (vi editor). Save the changes
- vi my-new-pod.yaml

Then delete the existing pod
- kubectl delete pod webapp

Then create a new pod with the edited file
- kubectl create -f my-new-pod.yaml



Edit Deployments
-----------------
With Deployments you can easily edit any field/property of the POD template. Since the pod template is a child of the deployment specification,  with every change the deployment will automatically delete and create a new pod with the new changes. So if you are asked to edit a property of a POD part of a deployment you may do that simply by running the command:

- kubectl edit deployment <deployment_name>



ENV variables in Kubernetes
=============================
- docker run -e APP_COLOR=red simple-webapp-color

- set the environment variables under env section, its an array and each item has name and value

pod-definition.yaml
--------------------
apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp-color
spec:
	containers:
	-	name: simple-webapp-color
		image: simple-webapp-color
ports:
	-	containerPort: 8080
env:
	- name: APP_COLOR
      value: pink
	  

1/ plain key-value types:
env:
 -	name: APP_COLOR
	value:
	
2/ configMap
env:
 -	name: APP_COLOR
	valueFrom:
		configMapKeyRef: 
	
3/ Secrets
env:
 -	name: APP_COLOR
	valueFrom:
		secretKeyRef:
		


ConfigMap
===================================
ConfigMap
-----------
APP_COLOR: blue
APP_MODE: prod


Create configMap using command inline:
---------------------------------------
- kubectl create configmap <configmap_name> --from-literal=<key>=<value>
ex:
- kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod

- from-literal is uded to specify the key/value pair in the command itself.
- adding multiple key/value pair using from-literal 


- empty configMap
$ kubectl create configmap <configmap_name> -n <namespace_name>

ex:
$ kubectl create configmap nginx-configmap -n ingress-space 
configmap/nginx-configmap created



Create configMap using command from file:
------------------------------------------
- kubectl create configmap <configmap_name> --from-file=<file_path>

ex:
- kubectl create configmap app-config --from-file=/opt/app_config.properties


config-map.yaml (data instead of spec)
----------------
apiVersion: v1
kind: Config
metadata: 
	name: app-config
data:
	APP_COLOR: blue
	APP_MODE: prod
	
- kubectl create -f config-map.yaml

- use these configMap names while associating to pods

- kubectl get configmaps | kubectl get cm
NAME               DATA   AGE
kube-root-ca.crt   1      16m
db-config          3      10s

- kubectl describe configmaps <name_of_configmap> | kubectl describe cm <name_of_configmap>
Name:         db-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
DB_PORT:
----
3306
DB_HOST:
----
SQL01.example.com
DB_NAME:
----
SQL01

BinaryData
====

Events:  <none>




configMap in Pods
------------------
pod-definition.yaml
--------------------
apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp-color
spec:
	containers:
	- name: simple-webapp-color
	  image: simple-webapp-color
	  ports:
	  - containerPort: 8080
	  - envFrom:
		- configMapRef:
			name: webapp-config-map // inject a specific configMap as env variables into the pod created earlier. 
			
- we can inject specific env values from the earlier created configMap object:
env:
	- name: APP_COLOR
	  valueFrom:
		configMapKeyRef:
			name: app-config
			key: APP_COLOR
			

- we cal also inject the configMap data as a file in volume:
volumes:
	- name: app-config-volume
	  configMap: 
		name: app-config
		



Secrets
=====================================
mysql.connector.connect(host="mysql", database="mysql", user="root", password="passwd")

- the code above is hard coding all the db connection details. we can move them to configMap but configMap store the key/value in plain text which is not good for storing passwords.

config-map-definition.yaml
---------------------------
apiVersion: v1
kind: ConfigMap
metadata:
	name: app-config
data:
	DB_host: mysql
	DB_user: root
	DB_password: passwd
	

- this where secret is useful, store password like sensitive information. similar to configMap except they ate store in encoded or hashed format.
- similar to configMaps, create secret and inject into pod

Create secret using command inline:
---------------------------------------
- kubectl create secret generic <secret_name> --from-literal=<key>=<value>
ex:
- kubectl create secret generic app-secret --from-literal=DB_password=passwd --from-literal=DB_host=mysql --from-literal=DB_user=root


Create secret using command from file:
------------------------------------------
- kubectl create secret <secret_name> --from-file=<file_path>

ex:
- kubectl create secret app-secret --from-file=/opt/app_secret.properties


secret.yaml (data instead of spec)
----------------
apiVersion: v1
kind: Secret
metadata: 
	name: app-secret
data:
	DB_host: mysql
	DB_user: root
	DB_password: passwd
	
- kubectl create -f secret.yaml

- now these secrets in manifest file are in plantext, hence we should hash these secrets from plan text to an encoded format and then add into the manifest file like below:

$ echo -n "mysql" | base64
bC1hsjk=

$ echo -n "root" | base64
cm9vD=

$ echo -n "passwd" | base64
cGFzagh

secret-hashed.yaml (data instead of spec)
----------------
apiVersion: v1
kind: Secret
metadata: 
	name: app-secret
data:
	DB_host: bC1hsjk=
	DB_user: cm9vD=
	DB_password: cGFzagh

- use these configMap names while associating to pods


- keubectl get secrets 

- kubectl describe secrets  // shows the name of the secrets but hides the values.

- kubectl get secret <secret_name> -o yaml // this shows the manifest file along with the secret values (hashed if provided in hashed format else in plain text)

$ echo -n "bC1hsjk=" | base64 --decode
mysql


- Create TLS secret named webhook-server-tls

$ kubectl -n webhook-demo create secret tls webhook-server-tls \
	--cert "/root/keys/webhook-server-tls.crt" \
	--key "/root/keys/webhook-server-tls.key"
	
secret/webhook-server-tls created


Secrets in PODs
---------------------------------
secret-hashed.yaml (data instead of spec)
----------------
apiVersion: v1
kind: Secret
metadata: 
	name: app-secret
data:
	DB_host: bC1hsjk=
	DB_user: cm9vD=
	DB_password: cGFzagh
	
	
pod-definition.yaml
--------------------
apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp-color
spec:
	containers:
	- name: simple-webapp-color
	  image: simple-webapp-color
	  ports:
	  - containerPort: 8080
	  envFrom:
		- secretRef:
			name: app-secret // inject a specific secret as env variables into the pod. 


- we can inject specific secret values as env variables from the earlier created configMap object:
env:
	- name: DB_password
	  valueFrom:
		secretKeyRef:
			name: app-secret
			key: DB_password
			

- we can also inject the secret data as a file in volume:
volumes:
	- name: app-secret-volume
	  secret: 
		secretName: app-secret
		
- if you want to mount the secret as a volume in the pod, each attribute in the secret is create as a file with value of the secret as its content.

- ls /opt/app-secret-volumes  // 3 files are created for each secret keys.
DB_Host		DB_password		DB_user

- cat /opt/app-secret-volumes/DB_password   // content of these files has the secret value.
passwd

IMP ***** note od Secret
--------------------------
- Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered as not very safe.
- Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:
	- Not checking-in secret object definition files to source code repositories.
	- Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD. 
	
- Also the way kubernetes handles secrets. Such as:
	- A secret is only sent to a node if a pod on that node requires it.
	- Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.
	- Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.
	
	


Seurity Context
================================
Seurity Context in Docker
--------------------------
- We have learned that unlike virtual machines containers are not completely isolated  from their host. Containers and the hosts share the same kernel. Containers are isolated using namespaces in Linux. The host has a namespace and the containers have their own namespace. All the processes run by the containers are in fact run on the host itself, but in their own namespaces.

$ docker run ubuntu sleep 3600

- So when you list the processes from within the docker container you see the sleep process with a process ID of 1. 

$ ps aux
USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND
root 1 0.0 0.0 4528 828 ? Ss 03:06 0:00 sleep 3600

- For the docker host, all processes of its own as well as those in the child namespaces are visible as just another process in the system. So wen you list the processes on the host you see a list of processes including the sleep command, but with a different process ID. This is because the processes can have different process IDs in different namespaces and that’s how Docker isolates containers within a system. So that’s process isolation.

$ ps aux
USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND
project 3720 0.1 0.1 95500 4916 ? R 06:06 0:00 sshd: project@pts/0
project 3725 0.0 0.1 95196 4132 ? S 06:06 0:00 sshd: project@notty
project 3727 0.2 0.1 21352 5340 pts/0 Ss 06:06 0:00 -bash
root 3802 0.0 0.0 8924 3616 ? Sl 06:06 0:00 docker-containershim -namespace m
root 3816 1.0 0.0 4528 828 ? Ss 06:06 0:00 sleep 3600


Security - Users
--------------------
- The docker host has a set of users, a root user as well as a number of non-root users. By default docker runs processes 
within containers as the root user. This can be seen in the output of the commands we ran earlier. Both within the container and outside the container on the host, the process is run as the root user. 
	- Now if you do not want the process within the container to run as the root user, you may set the user using the user option with the docker run command and specify the new user ID. 

$ docker run --user=1001 ubuntu sleep 3600

$ ps aux
USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND
1001 1 0.0 0.0 4528 828 ? Ss 03:06 0:00 sleep 3600


- Another way to enforce user security is to have this defined in the Docker image itself at the time of creation.

Docker file:
FROM ubuntu
USER 1001

$ docker build –t my-ubuntu-image 

$ docker run my-ubuntu-image sleep 3600

$ ps aux
USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND
1001 1 0.0 0.0 4528 828 ? Ss 03:06 0:00 sleep 3600


- Now What happens when you run containers as the root user? Is the root user within the container the same as the root user on the host? Can the process inside the container do anything that the root user can do on the system? If so isn’t that dangerous? Well, docker implements a set of security features that limits the abilities of the root user within the container. So the root user within the container isn’t really like the root user on the host. 

	- Docker uses Linux Capabilities to implement this. ex of linux capabilities are like: CHOWN, DAC, KILL SETFCAP, SETPCAP, SETGID, SETUID, NET_BIND, NEW_RAW, MAC_ADMIN, BROADCAST, NET_ADMIN, SYS_ADMIN, SYS_CHROOT, etc..
	
	- the root user is the most powerful user on a system. The root user can literally do anything. And so does 
a process run by the root user. It has unrestricted access to the system. From modifying files and permissions on files, Access Control, creating or killing processes, setting group id or user ID, performing network related operations such as binding to network ports, broadcasting on a network, controlling network ports; system related operations like rebooting the host, manipulating system clock and many more. All of these are the different capabilities on a Linux system and you can see a full list at this location. You can now control and limit what capabilities are made available to a process.

	- By default Docker runs a container with a limited set of capabilities. And so the processes running within the container do not have the privileges to say, reboot the host or perform operations that can disrupt the host or other containers running on 
the same host.
		- override this behavior and provide additional privileges than what is available use the cap-add option in the docker run command. 
		$ docker run --cap-add MAC_ADMIN ubuntu
		
		- Similarly you can drop privileges as well using the cap drop option.
		$ docker run --cap-drop KILL ubuntu
		
		- to run the container with all privileges enabled, use the privileged flag.
		$ docker run --privileged ubuntu
		
		
	
Seurity Context in Kubernetes
-------------------------------
$ docker run --user=1001 ubuntu sleep 3600
$ docker run --cap-add MAC_ADMIN ubuntu

-  in Kubernetes containers are encapsulated in PODs. You may chose to configure the security settings at a container level or at a POD level.

- If you configure it at a POD level, the settings will carry over to all the containers within the POD. If you configure it at both the POD and the Container, the settings on the container will override the settings on the POD. 

sc-pod-definition.yaml
------------------
apiVersion: v1
kind: Pod
metadata:
	name: web-pod
spec:
	securityContext:
		runAsUser: 1000
	containers:
	- name: ubuntu
	  image: ubuntu
	  command: ["sleep", "3600"]


sc-container-definition.yaml
----------------------------
apiVersion: v1
kind: Pod
metadata:
	name: web-pod
spec:
	securityContext:
		runAsUser: 1000
	containers:
	- name: ubuntu
	  image: ubuntu
	  command: ["sleep", "3600"]
	  securityContext:
		capabilities:
			add: ["MAC_ADMIN"]
		
- to add more: 	 add: ["MAC_ADMIN", "SYS_TIME"]




====================================================================

Service Accounts

====================================================================

- The concept of service accounts is linked to other security related concepts in kubernetes such as Authentication, Authorization, Role based access controls etc. 

- two types of accounts in Kubernetes. A user account and a service account. 
	- A user account could be for an administrator accessing the cluster to perform administrative tasks, a developer accessing the 
	cluster to deploy applications etc.
	- A service account, could be an account used by an application to interact with the kubernetes cluster.  
	For ex:, 1/ a monitoring application like Prometheus uses a service account to poll the kubernetes API for performance metrics, and 2/ An automated build tool like Jenkins uses service accounts to deploy applications on the kubernetes cluster. 

$ kubectl create serviceaccount dashboard-sa
serviceaccount “dashboard-sa” created

$ kubectl get serviceaccount
NAME SECRETS AGE
default 1 218d
dashboard-sa 1 4d

$ kubectl describe serviceaccount dashboard-sa
Name: dashboard-sa
Namespace: default
Labels: <none>
Annotations: <none>
Image pull secrets: <none>
Mountable secrets: dashboard-sa-token-kbbdm
Tokens: dashboard-sa-token-kbbdm
Events: <none>

- When the service account is created, it also creates a token automatically. The service account token is what must be used by the external application while authenticating to the Kubernetes API. The token, however, is stored as a secret object. In this case its named dashboard-sa-token-kbbdm.

- So when a service account is created, it first creates the service account object and then generates a token for the service account. It then creates a secret object and stores that token inside the secret object. The secret object is then linked to the service account. To view the token, view the secret object by running the command kubectl describe secret. 

$ kubectl describe serviceaccount dashboard-sa
Name: dashboard-sa
Namespace: default
Labels: <none>
Annotations: <none>
Image pull secrets: <none>
Mountable secrets: dashboard-sa-token-kbbdm
Tokens: dashboard-sa-token-kbbdm
Events: <none>


$ kubectl describe secret dashboard-sa-token-kbbdm
Name: dashboard-sa-token-kbbdm
Namespace: default
Labels: <none>
Type: kubernetes.io/service-account-token
Data
====
ca.crt: 1025 bytes
namespace: 7 bytes
token: 
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL
3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3V
udC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3

- This token can then be used as an authentication bearer token while making a rest call to the kubernetes API.
$ curl https://192.168.56.70:6443/api --header "Authorization: Bearer <paste the above token>"

- You can create a service account, assign the right permissions using Role based access control mechanisms 
(which is out of scope for this course) and export your service account tokens and use it to configure your third party application to authenticate to the kubernetes API.

	- if your third party application is hosted on the kubernetes cluster itself. For example, we can have our custom-kubernetes-dashboard or the Prometheus application used to monitor kubernetes, deployed on the kubernetes cluster itself. i.e. mounting the service token secret as a volume inside the POD hosting the third party application. the token to access the kubernetes API is already placed inside the POD and can be easily read by the application.

$ kubectl get serviceaccount
NAME SECRETS AGE
default 1 218d
dashboard-sa 1 4d

$ kubectl describe serviceaccount default
Name:                default
Namespace:           default
Labels:              <none>
Annotations:         <none>
Image pull secrets:  <none>
Mountable secrets:   default-token-rspjd
Tokens:              default-token-rspjd
Events:              <none>

pod-definition.yml
-------------------
apiVersion: vi
kind: Pod
metadata: 
	name: my-kubernetes-dashboard
spec:
	containers:
		- name: my-kubernetes-dashboard
		  image: my-kubernetes-dashboard
	  
	  
$ kubectl describe pod my-kubernetes-dashboard
Name: my-kubernetes-dashboard
Namespace: default
Annotations: <none>
Status: Running
IP: 10.244.0.15
Containers:
	nginx:
		Image: my-kubernetes-dashboard
	Mounts:
		/var/run/secrets/kubernetes.io/serviceaccount from default-token-j4hkv (ro)
Conditions:
Type Status
Volumes:
default-token-j4hkv:
Type: Secret (a volume populated by a Secret)
SecretName: default-token-j4hkv     // default service account that exists already.
Optional: false

	  
- For every namespace in kubernetes a service account named default is automatically created. Each namespace has its own 
default service account. Whenever a POD is created the default service account and its token are automatically mounted to that POD as a volume mount.  a volume is automatically created from the secret named default-token-j4hkv, which is in fact the secret containing the token for the default service account. The secret token is mounted at location /var/run/secrets/kubernetes.io/serviceaccount inside the pod. 

- the secret mounted as 3 separate files. The one with the actual token is the file named token.
$ kubectl exec -it my-kubernetes-dashboard ls /var/run/secrets/kubernetes.io/serviceaccount
ca.crt namespace token

- Now remember that the default service account is very much restricted. It only has permission to run basic kubernetes API queries.

$ kubectl exec -it my-kubernetes-dashboard cat /var/run/secrets/kubernetes.io/serviceaccount/token
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3V
udC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tajRoa3Y
iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWF
jY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjcxZGM4YWExLTU2MGMtMTFlOC04YmI0LTA4MDAyNzkzMTA3MiIsInN1YiI6InN5c3RlbTpzZXJ2


- Remember, you cannot edit the service account of an existing pod, so you must delete and re-create the pod. However in case of a 
deployment, you will be able to edit the serviceAccount, as any changes to the pod definition will automatically trigger a new roll-out for the deployment.

pod-definition.yml
-------------------
apiVersion: v1
kind: Pod
metadata:
	name: my-kubernetes-dashboard
spec:
	serviceAccount: dashboard-sa
	containers:
		- name: my-kubernetes-dashboard
		  image: my-kubernetes-dashboard
	

$ kubectl describe pod my-kubernetes-dashboard
Name: my-kubernetes-dashboard
Namespace: default
Annotations: <none>
Status: Running
IP: 10.244.0.15
Containers:
	nginx:
		Image: my-kubernetes-dashboard
Mounts: /var/run/secrets/kubernetes.io/serviceaccount from dashboard-sa-token-kbbdm (ro)


- So remember, kubernetes automatically mounts the default service account if you haven’t explicitly specified any. You may choose not to mount a service account automatically by setting the automountServiceAccountToken: false in the POD spec section. 



pod-definition.yml
-------------------
apiVersion: v1
kind: Pod
metadata:
	name: my-kubernetes-dashboard
spec:
	automountServiceAccountToken: false
	containers:
		- name: my-kubernetes-dashboard
		  image: my-kubernetes-dashboard
	
	
deployment-def.yaml
--------------------
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-deployment
spec:
  template:
    # Below is the podSpec.
    metadata:
      name: ...
    spec:
      serviceAccountName: dashboard-sa
      automountServiceAccountToken: false
      ...
	  


Resources
=================================================
- Each node has a set of CPU, Memory and Disk resources available. Every POD consumes a set of resources
- Whenever a POD is placed on a Node, it consumes resources available to that node.

- If the node has no sufficient resources, the scheduler avoids placing the POD on that node, Instead places the POD on one were sufficient resources are available.

- If there is no sufficient resources available on any of the nodes, Kubernetes holds back scheduling the POD, and you will see the POD in a pending state. If you look at the events, you will see the reason – insufficient cpu.

- By default, kubernetes assumes that a POD or a container within a POD requires .5 CPU & 256 Mebibyte of memory. The minimum amount of CPU or Memory requested by the container. 
- For the POD to pick up those defaults you must have first set those as default values for request and limit by creating a LimitRange in that namespace.

mem-limit-range-definition.yaml
-------------------------------
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container

cpu-limit-range-definition.yaml
-------------------------------
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container


	- When the scheduler tries to place the POD on a Node, it uses these numbers to identify a Node which has sufficient amount of resources available. Now, if you know that your application will need more than these, you can modify these 
values, by specifying them in your POD or deployment definition files. 
	- You can specify any value as low as 0.1. 0.1 CPU can also be expressed as 100m were m stands for milli. 
	- 1 count of CPU is equivalent to 1 vCPU. That’s 1 vCPU in AWS, or 1 Core in GCP or Azure or 1 Hyperthread.
	
1 G (Gigabyte) = 1,000,000,000 bytes
1 M (Megabyte) = 1,000,000 bytes
1 K (Kilobyte) = 1,000 bytes

1 Gi (Gibibyte) = 1,073,741,824 bytes
1 Mi (Mebibyte) = 1,048,576 bytes
1 Ki (Kibibyte) = 1,024 bytes


- G is Gigabyte and it refers to a 1000 Megabytes, whereas Gi refers to Gibibyte and refers to 1024 Mebibyte. 

-  In the Docker world, a docker container has no limit to the resources it can consume on a Node. Say a container 
starts with 1 vCPU on a Node, it can go up and consume as much resource as it requires, suffocating the native processes on the node or other containers of resources. 
	- However, you can set a limit for the resource usage on these PODs. By default Kubernetes sets a limit of 1vCPU to containers. So if you do not specify explicitly, a container will be limited to consume only 1 vCPU from the Node.
	-  By default, kubernetes sets a limit of 512 Mebibyte on containers.
	
pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp-color
labels:
	name: simple-webapp-color
	spec:
	containers:
		- name: simple-webapp-color
		  image: simple-webapp-color
		ports:
			- containerPort: 8080
		resources:
			requests:
				memory: "1Gi"
				cpu: 1
			limits:
				memory: "2Gi"
				cpu: 2

- the limits and requests are set for each container. 

- So what happens when a pod tries to exceed resources beyond its specified limit. In case of the CPU, kubernetes throttles the CPU so that it does not go beyond the specified limit. A container cannot use more CPU resources than its limit. However, this is not the case with memory. A container CAN use more memory resources that its limit. So if a pod tries to consume more memory than its limit constantly, the POD will be terminated.



	
Taints and Tolerations
==========================================================
- This is about the Pod to Node relationships and how to restrict what pod to get scheduled on what nodes. this is used to restrict on what pods can be scheduled on specific nodes.

- when the pods are created, Kubernetes tries to schedule them on the available nodes. by default, there are not restrictions or limitations and as such the scheduler places the pods across all of the nodes to balance them out equaly.

- In case we have a dedicated resources allocated to some specific nodes for some high capacity usage. we would like only those pods to be scheduled on these nodes,
	- first add a taint to those nodes for restrict all nodes getting scheduled on them. ex: taint=blue
	- by default, pods wont have any toleration.
	- add the same toleration to specific pods to be scheduled on those specific nodes.
	- when Scheduler trying to schedule other pods to these taint nodes, the pods get rejected due to the taint. hence scheduler schedules them on other nodes.

Taints - Node
--------------
- there are 3 types of <taint-effect>, i.e. NoSchedule | PreferNoSchedule | NoExecute -- these decides what happens to the PODs that do not have stamped with same tolerence as same as the Node.
	- NoSchedule -- the PODs without tolerence wont be scheduled on the taint nodes.
	- PreferNoSchedule -- the system will try to avoid placing the PODs without tolerence on the taint nodes. i.e. not guranteed.
	- NoExecute  --  New PODs without tolerence wont be scheduled and exsiting PODs (may have been scheduled before the taint applied to the nodes) without tolerence stamped woll be evicted from the node.
	
- add tarint to a node
$ kubectl taint nodes <node-name> <key>=<value>:<taint-effect>

ex: 
$ kubectl taint nodes node1 app=myapp:NoSchedule

$ kubectl taint nodes node01 spray=mortein:NoSchedule
node/node01 tainted

- remove taint from a node
$ kubectl taint nodes <node-name> <taint-name>-   // - symbol at the end

ex:
$ kubectl describe node controlplane | grep -i taint
Taints:             node-role.kubernetes.io/master:NoSchedule

$ kubectl taint node controlplane node-role.kubernetes.io/master:NoSchedule-
node/controlplane untainted

$ kubectl taint nodes node01 node.kubernetes.io/disk-pressure:NoSchedule-
node/node01 untainted



Tolerations - Pod
-----------------
- toleration are added to pods.

$ kubectl taint nodes node1 app=myapp:NoSchedule

pod-definition.yml
-------------------
apiVersion: 
kind: Pod
metadata:
	name: myapp-pod
spec:
	containers:
	- name: nginx-container
	  image: nginx
	tolerations:
	- key: app
	  operator: Equal
	  value: myapp
	  effect: NoSchedule

bee-pod.yaml
------------
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    operator: Equal
    value: mortein
	effect: NoSchedule



- *** Tains and Tolerence does not tell the pods to go to a particular node instead it tell the node to only accept pods with certain tolerations. if we want to restrict a pod to certain nodes, we need to use "Node Afinity"

- as we know in K8s cluster there are master (kubemaster) and worker (kubelet) nodes, master nodes are technically just like workers and have all the capabilities of hosting pods + runs the cluster management softwares. When kubernets cluster is setup, it adds a special taints to its master nodes for preventing the Scheduler not able to schedule any pods to master.

- to see these taint on master node, run the below:

$ kubectl describe node kubemaster | grep Taint
Taints: node-role.kubernetes.io/master:NoSchedule

$ kubectl describe node controlplane | grep Taints
Taints:             node-role.kubernetes.io/master:NoSchedule

$ kubectl describe node node01 | grep Taint
Taints:             <none>

$ kubectl explain pod --recursive | grep -A5 tolerations
tolerations       <[]Object>
	 effect <string>
	 key    <string>
	 operator       <string>
	 tolerationSeconds      <integer>
	 value  <string>
	 



Node Selectors
====================================
- by default any Pods can get scheduled to any nodes. so, even if there are few nodes with higher harware resources allocated, a POD (ex: data processor app) if it requires high resources, ends up being scheduled to any node, whcih is not desired.

- 2 ways to achive this:
	- Node Selector - simple and easire method.
	- Node Afinity - complex usecase.
	
- on Pod, add "nodeSelector" section to limit the pod to run on specific nodes (ex: nodes stamped with size=Large)

	pod-definition.yaml
	-------------------
	apiVersion: v1
	kind: Pod
	metadata:
		name: myapp-pod
	spec:
		containers:
			- name: data-processor
			  image: data-processor
		nodeSelector:
			size: Large
	
	$ kubectl apply -f pod-definition.yaml
	
	- size=Large are labels assigned to the specific nodes, the scheduler uses these labels to match and identify the right node to place the right pods on.
	- we must have label the nodes prior to add them in nodeSelector on pod.
	
- on Nodes - add labels
		$ kubectl label nodes <node-name> <label-key>=<label-value>
		ex:
		$ kubectl label node01 size=Large
	
	$ kubectl apply -f pod-definition.yaml
	

Node Selector - Limitations
----------------------------
- used a single label and nodeSelector ro achieve simple goal, in case of complex requirements like: a) place the Pod on a Large OR Medium nodes, b) Place the Pod on any nodes that are not small.

- These complex usecase can not achieved by Node Selector, use Node Afinity



Node Afinity
============================================================
- primary goal is ensure that pods are scheduled on particular nodes. Node Selector works unless the expression/condition is simple like matching labels ex: size=Large, but we can not provide advanced option like Or/Not expressions/conditions with 'Node Selectors'

- Node Affinity has advanced ability ti limit pod placements on specific nodes. 

pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
spec:
	containers:
		- name: data-processor
		  image: data-processor
	affinity:
		nodeAffinity:
			requiredDuringSchedulingIgnoredDuringExecution:
				nodeSelectorTerms:
					- matchExpressions:
					  - key: size
					    operator: In
						values:
						- Large
						- Medium
						

- Here the 'In' operator ensures, that the pod will be placed on a node whose label size has any value with the list of values specified in Pod definition under affinity section.

- for scheduleing Pods on any nodes other than size=Small, use 'NotIn' operator, nodeAffinity match the pods with node label size not set to 'Small'.

affinity:
  nodeAffinity:
	requiredDuringSchedulingIgnoredDuringExecution:
	  nodeSelectorTerms:
	  - matchExpressions:
	    - key: size
		  operator: NotIn
		  values:
		  - Small
					

- Incase we set the node label to Large and Medium but did not have any label (size) on Small nodes, use 'Exists' operator. it will check the nodes where the label named 'size' exists, and schedule the pods only to those nodes.

- Incase we set the node label to Large and Medium but did not have any label (size) on Small nodes, use 'Exists' operator. it will check if the label named 'size' exists
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
	  nodeSelectorTerms:
	  - matchExpressions:
		- key: size
		  operator: Exists


- when the pods are created, these affinity rules are considered and the pods are placed onto the right node. 

- The type of node affinity defines the behavior of the scheduler.
	- what if the Node affnity could not match with the given expression (ex: no nodes with a specific label)?
	- What if someone changes the label on node after some of the pods were scheduled based on node affinity rule. will the pod continue to run on the same node.
	
	- there are 2 stages in the life cycle of a pod when considering node affinity. 1/ DuringScheduling 2/ DuringExecution
		- requiredDuringScheduling -- schduler mandates the pod to scheduled on nodes with the given affinity rule, if it can't find any nodes with the same labels, pods wont be scheduled. this type is used in cases where the placement of pod is crucial, if there is no matching nodes, pods will not be scheduled. 
		
		- preferredDuringScheduling -- in case the pod placement is less important, if matching nodes are not found, the scheduler will ignore the affinity rule and place the pod on any available nodes. saying the scheduler to try best to place the pod on matching nodes (labels), if can't find one such nodes, just place it anywhere but dont keep the pod in pending state.
		
		- DuringExecution is the stage where the pod has been running and a change is made to the system/node (ex: changing the label of a node). ex: admin removing label (size=Large) from the node on which there are already running pods with similar match expression, as of today, there is only one type affinity available (Ignored) for DuringExecution stage means the already running pods will continue to run and change to node affinity will not have any impact once the pods are scheduled.
		
			- a new option for DuringExecution stage i.e. Required to be introduced soon, to evict already running pods that do not meet affinity rule at any point in time. ex: pod running on nodes (labelled with size=Large) will evicted/terminated if the label size=Large is removed from the node.
			
	- requiredDuringSchedulingIgnoredDuringExecution
	- preferredDuringSchedulingIgnoredDuringExecution
	- requiredDuringSchedulingRequiredDuringExecution


- show the labels on node
$ kubectl get nodes node01 --show-labels
NAME     STATUS   ROLES    AGE     VERSION   LABELS
node01   Ready    <none>   7m21s   v1.20.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,color=blue,kubernetes.io/arch=amd64,kubernetes.io/hostname=node01,kubernetes.io/os=linux

$ kubectl describe node node01

Name:               node01
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node01
                    kubernetes.io/os=linux
					
$ kubectl label nodes node01 color=blue
node/node01 labeled

- create deployment with a image
$ kubectl create deployment blue --image=nginx
deployment.apps/blue created

$ kubectl get deployments.apps 
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   1/1     1            1           2m19s

- scale out and scale in the deployment
$ kubectl scale deployment blue --replicas=6
deployment.apps/blue scaled

$ kubectl get deployments.apps 
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   6/6     6            6           3m7s


- scale out and scale in the deployment
$ kubectl scale deployment blue --replicas=3
deployment.apps/blue scaled

$ kubectl get deployments.apps 
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   3/3     3            3           3m33s


- Set Node Affinity to the deployment to place the pods on node01 only
- 1st get the running deployment to yaml file.



Taints & Tolerations vs Node Affinity
======================================================
- lets say, we have 3 nodes and 3 pods each in 3 colors, blue, red and green. the ultimate aim is to plave blue pod to blue nodem red pod to red node and greem pod to green node.
	- we are sharing the same kubernetes cluster with other teams, so other team has their own pods and other nodes. we dont want other team pod to be placed on our nodes, enither do we want our red/green/pods to be placed on other teams nodes.
	
- solving the problem with 'Taints & Tolerations'
	- apply taint to all of our 3 nodes. i.e. 
		$ kubectl taint nodes node-red color=red:NoSchedule
		$ kubectl taint nodes node-green color=green:NoSchedule
		$ kubectl taint nodes node-blue color=blue:NoSchedule
		
	- set respective tolerations on red/green/blue pods.
		red-pod.yaml
		------------
		apiVersion: v1
		kind: Pod
		metadata:
		  name: red
		spec:
		  containers:
		  - image: nginx
			name: red
		  tolerations:
		  - key: color
			operator: Equal
			value: red
			effect: NoSchedule
			
	- when the pods are created, nodes ensure to accept the pods with right tolerations set. so the green pod gets placed to green nodes and blue pod gets placed on blue node however, taints & tolerations does not gurantee that the pods will only prefr these nodes, so the red node ends up on one of other nodes that dont have any taint set. -- not desired.
	
	
- solving the problem with 'Node Affinity'
	- label the nodes with respective colors.
		$ kubectl label node node-red color=red
		$ kubectl label node node-green color=green
		$ kubectl label node node-blue color=blue
		
	- add nodeSelectors on the 3 pods to tie them to these nodes.
		pod-definition.yaml
		-------------------
		apiVersion: v1
		kind: Pod
		metadata:
		  name: red
		spec:
		  containers:
		  - name: red
		    image: nginx
		  affinity:
			nodeAffinity:
			  requiredDuringSchedulingIgnoredDuringExecution:
			    nodeSelectorTerms:
				- matchExpressions:
				  - key: color
					operator: In
					values:
					- red
								
		- hence red/green/blue ends up on respective nodes, however it does not gurantee other pods being scheduled on these red/green/blue nodes. -- not desired.
		
- *** Hence combination of both 'Taints & Tolerations' and 'Node Affinity' to be used to completely dedicate specific nodes for specific pods.
	- apply taints and tolerations on red/green/blue nodes and pods for preventing other nodes being scheduled on these nodes.
	- use node affinity to prevent our pods from being placed on their nodes. 

	

Multi-Container Pods
======================================================
- 2 pattern of multi-container pods in kubernetes.
	- Ambassador
	- Adaptor
	- Sidecar
	
- idea of decoupling a large monolithic application into sub component known as micro-services enables us to develop and deploy a set of independent small and re-usable code. this architecture can then help us scale up/down as well as modify each service as required as against with modifying the large application.
	- however at times, we need 2 services to work together. ex: a Web server and a Logging Agent service. You need one agent per web server instance paired together. You don’t want to merge and bloat the code of the two services, as each of them target different features, and you’d still like them to be developed and deployed separately.
	- You only need the two functionality to work together. You need one agent per web server instance paired together, that can scale up and down together. And that is why you have multi-container PODs, that share the same lifecycle – which means they are created together and destroyed together.
	- They share the same network space, which means they can refer to each other as localhost. 
	- They have access to the same storage volumes.
	- This way, you do not have to establish, volume sharing or services between the PODs to enable communication between them.
	
	
pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp
labels:
	name: simple-webapp
spec:
	containers:
	- name: simple-webapp
	  image: simple-webapp
	  ports:
	  - containerPort: 8080
	
	- name: log-agent
	  image: log-agent
	  


- There are 3 common patterns.

- Side car
	- The first and what we just saw with the logging service example is known as a side car pattern.
	- A good example of a side car pattern is deploying a logging agent along side a web server to collect logs and forward them to a central log server.
	
- Adaptor
	- Building on that example, say we have multiple applications generating logs in different formats. It would be hard to process the various formats on the central logging server.
	- say container A generating logs formatted like: 12-JULY-2018 16:05:49 "GET /index1.html" 200
	container B generating logs formatted like: 12/JUL/2018:16:05:49 -0800 "GET /index2.html" 200
	container C generating logs formatted like: GET 1531411549 "/index3.html" 200
	
	- So, before sending the logs to the central server, we would like to convert the logs to a common format. For this we deploy an adapter container. The adapter container processes the logs, before sending it to the central server
	12-JULY-2018 16:05:49 "GET /index1.html" 200  		-->  12-JULY-2018 16:05:49 "GET /index1.html" 200
	12/JUL/2018:16:05:49 -0800 "GET /index2.html" 200  	-->  12-JULY-2018 16:05:49 "GET /index2.html" 20
	GET 1531411549 "/index3.html" 200  					-->  12-JULY-2018 16:05:49 "GET /index3.html" 200
	
- Ambassador
	- your application communicates to different database instances at different stages of development. A local database for development, one for testing and another for production.
	-  You must ensure to modify this connectivity depending on the environment you are deploying your application to.
	- You may chose to outsource such logic to a separate container within your POD, so that your application can always refer to a database at localhost, and the new container, will proxy that request to the right database. This is known as an ambassador container.
	
exercise-1:
----------
Create a multi-container pod with 2 containers.
Use the spec given below.
If the pod goes into the crashloopbackoff then add sleep 1000 in the lemon container.
Name: yellow
Container 1 Name: lemon
Container 1 Image: busybox
Container 2 Name: gold
Container 2 Image: redis

- creates pod with name 'yellow' and a containet with name 'yellow' by given image.
$ kubectl run yellow --image=busybox --restart=Never --dry-run=Client -o yaml > pod.yaml

- update the file and the containers.

- check pod logs: incl all contianer within it
$ kubectl -n <namespace_name> logs <pod_name>


$ kubectl get pod -n elastic-stack
NAME             READY   STATUS    RESTARTS   AGE
app              1/1     Running   0          48m
elastic-search   1/1     Running   0          48m
kibana           1/1     Running   0          48m


root@controlplane:~# kubectl -n elastic-stack get pod,svc
NAME                 READY   STATUS    RESTARTS   AGE
pod/app              1/1     Running   0          49m
pod/elastic-search   1/1     Running   0          49m
pod/kibana           1/1     Running   0          49m

NAME                    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                         AGE
service/elasticsearch   NodePort   10.105.218.2     <none>        9200:30200/TCP,9300:30300/TCP   49m
service/kibana          NodePort   10.108.103.235   <none>        5601:30601/TCP                  48m


$ kubectl -n elastic-stack logs app	

- extract the pod definition.
$ kubectl -n elastic-stack get pod app -o yaml > app-pod.yaml


	
	
Observability in Kubernetes
=========================================
- Readiness and Liveness Probes, Logging and Monitoring concepts.

POD Status
----------

- A POD has a pod status and some conditions in the lifecycle of a POD

- 3 Pod success Status: Pending, ContainerCreating, Running. STATUS column in kubectl get pods, at any point in time the POD status can only be one of these values and only gives us a high level summary of a POD.

	- Pending
		- When a POD is first created, it is in a Pending state. This is when the Scheduler tries to figure out were to place the 
	POD. If the scheduler cannot find a node to place the POD, it remains in a Pending state. To find out why it’s stuck in a pending state, run the kubectl describe pod command, and it will tell you exactly why. 

	- ContainerCreating
		- Once the POD is scheduled, it goes into a ContainerCreating status, where the images required for the application are pulled and the container starts.
		
	- Running
		- Once all the containers in a POD starts, it goes into a running state, where it continues to be until the program completes successfully or is terminated. 

	- Completed
		- in case the Pod is created out of Job object (kind: Job).

- Other failure Statues:
	- ImagePullBackOff
		- POD got scheduled on an available node but got stuck dueing image pull stage from docker (ex: incorrect image name)

- Other status:		

$ kubectl get pod
NAME                READY   STATUS    RESTARTS   AGE
simple-webapp-1   	1/1     Running   0          33m
simple-webapp-2   	0/1     Running   0          42s




POD Conditions
---------------
- Conditions compliment POD status. It is an array of true or false values that tell us the 
state of a POD.
- PodScheduled (true/false): When a POD is scheduled on a Node, the PodScheduled condition is set to True.
- Initialized (true/false): When the POD is initialized, it’s value is set to True.
- Containers Ready (true/false): We know that a POD has multiple containers. When all the containers in the POD are ready, the Containers 
Ready condition is set to True.
- Ready (true/false): finally the POD itself is considered to be Ready


$ kubectl describe pod simple-webapp-2

Name:         simple-webapp-2
Namespace:    default
Priority:     0
Node:         controlplane/10.30.83.6
Start Time:   Wed, 29 Dec 2021 12:19:25 +0000
Labels:       name=simple-webapp
Annotations:  <none>
Status:       Running
IP:           10.244.0.7
IPs:
  IP:  10.244.0.7
Containers:
  simple-webapp:
    Container ID:   docker://1c94827d91493f603021fb34e2fed547cd908b09d470935407c7eac7889e98fd
    Image:          kodekloud/webapp-delayed-start
    Image ID:       docker-pullable://kodekloud/webapp-delayed-start@sha256:666b95c2ef8e00a74fa0ea96def8d9d69104ec5d9b9b0f49d8a76bd4c94ad343
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 29 Dec 2021 12:19:30 +0000
    Ready:          False
    Restart Count:  0
    Readiness:      http-get http://:8080/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      APP_START_DELAY:  80
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8bvpf (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True


------ after sometime -----
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
  
-------


- To see the state of POD conditions run the kubectl describe POD command and look for the conditions section. can also see the Ready state of the POD, in the output (READY column) of the kubectl get pods command.


- The ready conditions indicate that the application inside the POD is running and is ready to accept user traffic.

- The containers could be running different kinds of applications in them. It could be a simple script that performs a job. It could be a database service. Or a large web server, serving front end users. The script may take a few milliseconds to get ready. The database service may take a few seconds to power up. Some web servers could take several minutes to warm up. 

	-  If you try to run an instance of a Jenkins server, you will notice that it takes about 10-15 seconds for the server to initialize before a user can access the web UI. Even after the Web UI is initialized, it takes a few seconds for the server to warm up 
and be ready to serve users. During this wait period if you look at the state of the POD, it continues to indicate that the POD is ready, which is not very true.

- **  how does kubernetes know weather that the application inside the container is actually running or not? But before we get into 
that discussion, why does it matter if the state is reported incorrectly. 
	- Let us look at a simple scenario were you create a POD and expose it to external users using a service. The service will route traffic to the POD immediately. The service relies on the pod’s READY condition to route traffic. 
	- By default, Kubernetes assumes that as soon as the container is created, it is ready to serve user traffic. So it sets the value of the “Ready Condition” for each container to True. But if the application within the container took longer to get ready, the service is unaware of it and sends traffic through as the container is already in a ready state, causing users to hit a POD that isn’t yet running a live application. 
	

Readiness Probes
-------------------------
- There are different ways that you can define if an application inside a container is actually ready. You can setup different kinds of tests or Probes.
	- In case of a web application it could be when the API server is up and running. So you could run a HTTP test to see if the API server responds. ex: HTTP Test - /api/ready 
	- In case of database, you may test to see if a particular TCP socket is listening. ex: TCP Test - 3306
	- Or You may simply execute a command within the container to run a custom script that would exit successfully if the application is ready. ex: exec command


- add a new field called readinessProbe and use the httpGet option. Specify the port and the ready api.	
- Now when the container is created, kubernetes does not immediately set the ready condition on the container to true, instead, it performs a test to see if the api responds positively. Until then the service does not forward any traffic to the pod, as it sees that the POD is not ready.

pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
labels:
  name: simple-webapp
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
  ports:
  - containerPort: 8080
  readinessProbe:
    httpGet:
      path: /api/ready
      port: 8080
	  

- For http, use the httpGet option with the path and port.

readinessProbe:
  httpGet:
    path: /api/ready
    port: 8080
  initialDelaySeconds: 10  // delay in sec for app to warm up and then kubernetes will start probing.
  periodSeconds: 5         // interval in sec between each probe
  failureThreshold: 8      // by deafult 3 unsuccessful attemps of probe then the probe will stop.

- For TCP use the tcpSocket option with port.

readinessProbe:
  tcpSocket:
    port: 3306

- For executing a command specify the exec option with the command and options in an array format. 

readinessProbe:
  exec:
    command:
    - cat
    - /app/is_ready initialDelaySeconds: 10

	
How readinessProbes are useful in a multi-pod setup
----------------------------------------------------
- Say you have a replica set or deployment with multiple pods. And a service serving traffic to all the pods. There are two PODs already serving users. Say you were to add an additional pod. And let’s say the Pod takes a minute to warm up. Without the readinessProbe configured correctly, the service would immediately start routing traffic to the new pod. That will result in service disruption to atleast some of the users.

- Instead if the pods were configured with the correct readinessProbe, the service will continue to serve traffic only to the older pods and wait until the new pod is ready. 

- Once ready, traffic will be routed to the new pod as well, ensuring no users are affected.



Liveness Probes
====================================================
- You run the same web application with kubernetes. Every time the application crashes, kubernetes makes an attempt to restart the 
container to restore service to users. You can see the count of restarts increase in the output (RESTART column) of kubectl get pods command.

- However, what if the application is not really working but the container continues to stay alive? 
	- Say for example, due to a bug in the code, the application is stuck in an infinite loop. As far as kubernetes is concerned, the container is up, so the application is assumed to be up. But the users hitting the container are not served.
	-  In that case, the container needs to be restarted, or destroyed and a new container is to be brought up. 
	-  A liveness probe can be configured on the container to periodically test whether the application within the container is actually healthy. If the test fails, the container is considered unhealthy and is destroyed and recreated.
	- as a developer, you get to define what it means for an application to be healthy. 
	

pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
labels:
  name: simple-webapp
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
  ports:
  - containerPort: 8080
  livenessProbe:
    httpGet:
      path: /api/healthy 
      port: 8080
	
- In case of a web application it could be when the API server is up and running. ex: HTTP Test - /api/healthy 
	
livenessProbe:
  httpGet:
    path: /api/ready
    port: 8080
  initialDelaySeconds: 10  // delay in sec for app to warm up and then kubernetes will start probing. 
  periodSeconds: 5         // interval in sec between each probe. default 10 sec
  failureThreshold: 8      // by deafult 3 unsuccessful attemps of probe then the probe will stop.
	
- For TCP use the tcpSocket option with port.

livenessProbe:
  tcpSocket:
    port: 3306

- For executing a command specify the exec option with the command and options in an array format. 

livenessProbe:
  exec:
    command:
    - cat
    - /app/is_ready initialDelaySeconds: 10

	


Container Logging
=================================================
Logs - Docker
--------------
- Running a docker container for the image: kodekloud/event-simulator which generate random events simulating a web server, streamed to the standard output by the application

$ docker run kodekloud/event-simulator

2018-10-06 15:57:15,937 - root - INFO - USER1 logged in
2018-10-06 15:57:16,943 - root - INFO - USER2 logged out
2018-10-06 15:57:17,944 - root - INFO - USER2 is viewing page2
2018-10-06 15:57:18,951 - root - INFO - USER3 is viewing page3
2018-10-06 15:57:19,954 - root - INFO - USER4 is viewing page1
2018-10-06 15:57:20,955 - root - INFO - USER2 logged out
2018-10-06 15:57:21,956 - root - INFO - USER1 logged in


- run the docker container in the background, in a detached mode using the –d option, the logs wont appear on std output/console.
$ docker run -d kodekloud/event-simulator

- use the docker logs command followed by the container ID. The –f option helps us see the live log trail. 
$ docker logs -f <container-id>


Logs - Kubernetes
-----------------
- Once the pod is running, we can view the logs using the kubectl logs command with the pod name. Use the –f option to stream the logs live. 

$ kubectl logs <pod_name>  // if single container pod. use -f option for tail.

$ kubectl logs <pod_name> <container_name>   // if multi container pod. use -f option for tail. passing only the pod_name will throw error.


event-simulator.yaml
--------------------
apiVersion: v1
kind: Pod
metadata:
  name: event-simulator-pod
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
  - name: image-processor
    image: some-image-processor

$ kubectl create –f event-simulator.yaml


$ kubectl logs –f event-simulator-pod

2018-10-06 15:57:15,937 - root - INFO - USER1 logged in
2018-10-06 15:57:16,943 - root - INFO - USER2 logged out
2018-10-06 15:57:17,944 - root - INFO - USER2 is viewing page2
2018-10-06 15:57:18,951 - root - INFO - USER3 is viewing page3
2018-10-06 15:57:19,954 - root - INFO - USER4 is viewing page1
2018-10-06 15:57:20,955 - root - INFO - USER2 logged out
2018-10-06 15:57:21,956 - root - INFO - USER1 logged in


$ kubectl logs –f event-simulator-pod event-simulator

2018-10-06 15:57:15,937 - root - INFO - USER1 logged in
2018-10-06 15:57:16,943 - root - INFO - USER2 logged out
2018-10-06 15:57:17,944 - root - INFO - USER2 is viewing page2
2018-10-06 15:57:18,951 - root - INFO - USER3 is viewing page3
2018-10-06 15:57:19,954 - root - INFO - USER4 is viewing page1
2018-10-06 15:57:20,955 - root - INFO - USER2 logged out
2018-10-06 15:57:21,956 - root - INFO - USER1 logged in


- list the containers under a pod.
$ kubectl get pods <pod_name> -o jsonpath='{.spec.containers[*].name}'
simple-webapp db


$ kubectl logs event-simulator-pod event-simulator


Monitor and Debug Applications
================================================
- How do you monitor resource consumption on Kubernetes?
	- Node level metrics such as the number of nodes in the cluster, how many of them are healthy as well as performance metrics such as CPU. Memory, network and disk utilization.
	- POD level metrics such as the number of PODs, and performance metrics of each POD such the CPU and Memory consumption.

- we need a solution that will monitor these metrics, store them and provide analytics around this data.

- Kubernetes does not come with a full featured built-in monitoring solution. 

- open-source solutions available today, such as the Metrics-Server, Prometheus, the Elastic Stack, and proprietary solutions like Datadog and Dynatrace. 


Metrics-Server
--------------
- You can have one metrics server per kubernetes cluster.

- The metrics server retrieves metrics from each of the kubernetes nodes and pods, aggregates them and stores them in memory. 

- Note that the metrics server is only an in-memory monitoring solution and does not store the metrics on the disk, and as a 
result you cannot see historical performance data. For that you must rely on one of the advanced monitoring solutions we talked about earlier in this lecture. 


How are the metrics generated for the PODs on these nodes?
-----------------------------------------------------------
- Kubernetes runs an agent on each node known as the kubelet, which is responsible for receiving instructions from the kubernetes API master server and running PODs on the nodes. 

- The kubelet also contains a subcomponent known as as cAdvisor or Container Advisor.

- cAdvisor is responsible for retrieving performance metrics from pods, and exposing them through the kubelet API to make the metrics available for the Metrics Server.

Metrics Server – Getting Started
---------------------------------
- If you are using minikube for your local cluster, run the command minikube addons enable metrics-server.

$ minikube addons enable metrics-server


-  For all other environments deploy the metrics server by cloning the metrics-server deployment files from the github repository. And then deploying the required components using the kubectl create command.

$ git clone https://github.com/kubernetes-incubator/metrics-server.git


- deploys a set of pods, services and roles to enable metrics server to poll for performance metrics from the nodes in the cluster.

$ kubectl create –f deploy/1.8+/

clusterrolebinding "metrics-server:system:auth-delegator" created
rolebinding "metrics-server-auth-reader" created
apiservice "v1beta1.metrics.k8s.io" created
serviceaccount "metrics-server" created
deployment "metrics-server" created
service "metrics-server" created
clusterrole "system:metrics-server" created
clusterrolebinding "system:metrics-server" created


$ kubectl get pod
NAME       READY   STATUS    RESTARTS   AGE
elephant   1/1     Running   0          2m9s
lion       1/1     Running   0          2m9s
rabbit     1/1     Running   0          2m9s

- View Nodes:
$ kubectl top node

NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
controlplane   389m         1%     1250Mi          0%        
node01         69m          0%     339Mi           0%


- View Pods
$ kubectl top pod

NAME       CPU(cores)   MEMORY(bytes)   
elephant   23m          32Mi            
lion       1m           18Mi            
rabbit     151m         253Mi




POD Design
=================================================

Labels, Selectors
------------------
- Labels and Selectors are a standard method to group things together

- We have created a lot of different types of Objects in Kuberentes. Pods, Services, ReplicaSets and Deployments. For Kubernetes, all of these are different objects. Over time you may end up having 100s and 1000s of these objects in your cluster. Then you will need a 
way to filter and view different objects by different categories. 

- For each object attach labels as per your needs, like app, function etc.

- Then while selecting, specify a condition to filter specific objects. For example app == 
App1


pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
labels:
  app: App1
  function: Front-end
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
    ports:
    - containerPort: 8080
	

$ kubectl get pod
NAME          READY   STATUS    RESTARTS   AGE
app-1-zzxdf   1/1     Running   0          29s
app-1-4lhxh   1/1     Running   0          30s
app-1-kl4sz   1/1     Running   0          30s
db-1-69mbm    1/1     Running   0          30s
app-1-4bfdp   1/1     Running   0          30s
db-1-btfvv    1/1     Running   0          30s
db-1-8rmd2    1/1     Running   0          30s
auth          1/1     Running   0          30s
app-2-5fkzg   1/1     Running   0          30s
db-1-4kqdg    1/1     Running   0          30s
db-2-9pnts    1/1     Running   0          30s

$ kubectl get pod --selector env=dev
NAME          READY   STATUS    RESTARTS   AGE
app-1-4lhxh   1/1     Running   0          42s
app-1-kl4sz   1/1     Running   0          42s
db-1-69mbm    1/1     Running   0          42s
app-1-4bfdp   1/1     Running   0          42s
db-1-btfvv    1/1     Running   0          42s
db-1-8rmd2    1/1     Running   0          42s
db-1-4kqdg    1/1     Running   0          42s


Usage of labels/selectors: ReplicaSet
--------------------------------------
- ReplicaSet uses labels in order to filter the pods for which the defined replica needs to be created.
- labels mentioned under spec->template is for the filterring pod. 
- On creation, if the labels match, the replicaset is created successfully. 

replicaset-definition.yaml
--------------------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: simple-webapp
  labels:
    app: App1
    function: Front-end
spec:
  replicas: 3
  selector:
    matchLabels:
      app: App1
  template:
    metadata:
      labels:
        app: App1
        function: Front-end
    spec:
      containers:
      - name: simple-webapp
        image: simple-webapp
	  

	
Usage of labels/selectors: Service
--------------------------------------
- When a service is created, it uses the selector defined in the service definition file to match the labels set on the pods in the replicaset-definition file.	

service-definition.yaml
-----------------------
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: App1
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
	
	
	
Annotations
------------
- While labels and selectors are used to group and select objects, annotations are used to record other details for informatory purpose. 	

- For example tool details like name, version build information etc or contact details, phone numbers, email ids etc, that may be used for some kind of integration purpose

replicaset-definition.yaml
--------------------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: simple-webapp
  labels:
    app: App1
    function: Front-end
  annotations:
    buildversion: 1.34
spec:
  replicas: 3
  selector:
    matchLabels:
      app: App1
  template:
    metadata:
      labels:
        app: App1
        function: Front-end
    spec:
      containers:
      - name: simple-webapp
        image: simple-webapp


- select any objects by labels.
$ kubectl get all --selector env=prod

NAME              READY   STATUS    RESTARTS   AGE
pod/app-1-zzxdf   1/1     Running   0          5m49s
pod/auth          1/1     Running   0          5m50s
pod/app-2-5fkzg   1/1     Running   0          5m50s
pod/db-2-9pnts    1/1     Running   0          5m50s

NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/app-1   ClusterIP   10.43.135.206   <none>        3306/TCP   5m49s

NAME                    DESIRED   CURRENT   READY   AGE
replicaset.apps/app-2   1         1         1       5m51s
replicaset.apps/db-2    1         1         1       5m51s

$ kubectl get all --selector env=prod --no-headers | wc -l
7

- Select pods by mutiple labels
$ kubectl get pod --selector env=prod,bu=finance,tier=frontend
NAME          READY   STATUS    RESTARTS   AGE
app-1-zzxdf   1/1     Running   0          8m35s




Rolling Updates & Rollbacks in Deployments
=========================================================
$ kubectl run nginx --image=nginx
deployment "nginx" created

- this creates a deployment and not just a POD. the output of the command says Deployment nginx created.
- creating a deployment by only specifying the image name and not using a definition file. 
- A replicaset and pods are automatically created in the backend.


Deployment
-------------
- The deployment provides us with capabilities to upgrade the underlying instances seamlessly using rolling updates, undo changes, and pause and resume changes to deployments.

- Each container is encapsulated in PODs. Multiple such PODs are deployed using Replication Controllers or Replica Sets. And then comes Deployment which is a kubernetes object that comes higher in the hierarchy. Container -> POD -> ReplicaSet -> Deployment


Create a deployment
--------------------
- deployment-definition file are exactly similar to the replicaset definition file, except for the kind = Deployment
-  it has an apiVersion which is apps/v1, metadata which has name and labels and a spec that has template, replicas and selector. The template has a POD definition inside it.

deployment-definition.yml
--------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx
  replicas: 3
  selector:
  matchLabels:
    type: front-end
	
$  kubectl create –f deployment-definition.yml
deployment "myapp-deployment" created

$ kubectl get deployments
NAME 				DESIRED 	CURRENT 	UP-TO-DATE 	AVAILABLE 	AGE
myapp-deployment 	3 			3 			3 			3 			21s

$ kubectl get replicaset
NAME 						DESIRED 	CURRENT 	READY 	AGE
myapp-deployment-6795844b58 3 			3 			3 		2m

$ kubectl get pods
NAME 								READY 	STATUS 	RESTARTS 	AGE
myapp-deployment-6795844b58-5rbjl 	1/1 	Running 0 			2m
myapp-deployment-6795844b58-h4w55 	1/1 	Running 0 			2m
myapp-deployment-6795844b58-lfjhv 	1/1 	Running 0 			2m


$ kubectl get all
NAME 									DESIRED 	CURRENT 	UP-TO-DATE 	AVAILABLE 	AGE
deploy/myapp-deployment 				3 			3 			3 			3 			9h

NAME 									DESIRED 	CURRENT 	READY 	AGE
rs/myapp-deployment-6795844b58 			3 			3 			3 		9h

NAME 									READY 	STATUS 	RESTARTS 	AGE
po/myapp-deployment-6795844b58-5rbjl 	1/1 	Running 0 			2m
po/myapp-deployment-6795844b58-h4w55 	1/1 	Running 0 			2m
po/myapp-deployment-6795844b58-lfjhv 	1/1 	Running 0 			2m


- T see wha container image is used:
$ kubectl describe deployments.apps frontend | grep -i image
    Image:        kodekloud/webapp-color:v1

Rollout and Versioning
-----------------------
- Whenever you create a new deployment or make any changes (upgrade the images version,  updating the labels or updating the 
number of replicas etc.) in an existing deployment it triggers a Rollout.
	- A rollout is the process of gradually deploying or upgrading your application containers.
	- A new rollout creates a new Deployment revision. ex: revision 1
	- when the container version is updated to a new one – a new rollout is triggered and a new deployment revision is created named Revision 2. This helps us keep track of the changes made to our deployment and enables us to rollback to a previous version of 
deployment if necessary.

Rollout Command
---------------
- to see the status of your rollout for the deployment.

$  kubectl rollout status deployment/myapp-deployment  // given 10 replicas

Waiting for rollout to finish: 0 of 10 updated replicas are available...
Waiting for rollout to finish: 1 of 10 updated replicas are available...
Waiting for rollout to finish: 2 of 10 updated replicas are available...
Waiting for rollout to finish: 3 of 10 updated replicas are available...
Waiting for rollout to finish: 4 of 10 updated replicas are available...
Waiting for rollout to finish: 5 of 10 updated replicas are available...
Waiting for rollout to finish: 6 of 10 updated replicas are available...
Waiting for rollout to finish: 7 of 10 updated replicas are available...
Waiting for rollout to finish: 8 of 10 updated replicas are available...
Waiting for rollout to finish: 9 of 10 updated replicas are available...
deployment "myapp-deployment" successfully rolled out
	

- To see the revisions and history of rollout 

$ kubectl rollout history deployment/myapp-deployment
deployments "myapp-deployment"
REVISION CHANGE-CAUSE
1 <none>
2 kubectl apply --filename=deployment-definition.yml --record=true


Deployment Strategy
-------------------
- 2 deployment strategies.
	- Recreate strategy: 
		- Upgrade to newer version all at once. NOT the default.
		- the application will be down in between.
		
	- RollingUpdate 
		- we do not destroy all of them at once. Instead we take down the older version and bring up a newer version one by one. 
		- RollingUpdate is the default Deployment Strategy.
		
- The difference between the recreate and rollingupdate strategies can also be seen when you view the deployments in detail.
	-  kubectl describe deployment command to see detailed information regarding the deployments
	- You will notice when the Recreate strategy was used the events indicate that the old replicaset was scaled down to 0 first and the new replica set scaled up to 5
	- However when the RollingUpdate strategy was used the old replica set was scaled down one at a time simultaneously scaling up the new replica set one at a time.
	

- .spec.strategy specifies the strategy used to replace old Pods by new ones. .spec.strategy.type can be "Recreate" or "RollingUpdate". "RollingUpdate" is the default value.

- All existing Pods are killed before new ones are created when .spec.strategy.type==Recreate

- The Deployment updates Pods in a rolling update fashion when .spec.strategy.type==RollingUpdate. You can specify maxUnavailable and maxSurge to control the rolling update process


Update Deployment
------------------
$  kubectl apply –f deployment-definition.yml
deployment "myapp-deployment" configured

OR if just changing the image version. without updating the definition file.

$ kubectl set image deployment/myapp-deployment nginx-container=nginx:1.9.1  // set image <deployment_name> <container_name>=<container_image_name>:<image_tag>

deployment "myapp-deployment" image is updated


- to see the status of the upgrade.
$ kubectl rollout status deployment.apps/<deployment_name>
OR
$ kubectl rollout status deployment <deployment_name>

ex: 
$ kubectl rollout status deployment.apps/frontend
deployment "frontend" successfully rolled out

- to see the roll out history:
$ kubectl rollout history deployment.apps/<deployment_name>
OR
$ kubectl rollout history deployment <deployment_name>

ex:
$ kubectl rollout history deployment.apps/frontend
deployment.apps/frontend 
REVISION  CHANGE-CAUSE
1         <none>


How a deployment performs an upgrade under the hoods. 
-----------------------------------------------------
- When a new deployment is created, say to deploy 5 replicas, it first creates a Replicasetautomatically, which in turn creates the number of PODs required to meet the number of replicas.

- When you upgrade your application, the kubernetes deployment object creates a NEW replicaset under the hoods and starts deploying the containers there. 

- At the same time taking down the PODs in the old replica-set following a RollingUpdate strategy. 

- This can be seen when you try to list the replicasets using the kubectl get replicasets command. Here we see the old replicaset with 0 PODs and the new replicaset with 5 PODs.


Rollback
--------
- in case Something’s wrong with the new version of build, we can rollback to previous version.
$ kubectl rollout undo deployment.apps/<deployment_name>

- The deployment will then destroy the PODs in the new replicaset and bring the older ones up in the old replicaset. And your 
application is back to its older format.

- kubectl get replicasets command will show, Before the rollback the first replicaset had 0 PODs and the new replicaset had 5 PODs and this is reversed after the rollback is finished

- if we rollback from the current revision (ex: revision:3), it will be rolled back to revision: 2, a new revision will be added (revision: 4) same as revision: 2 but the old revision: 2 will be removed from the history.


- before rollback:
$ kubectl get replicasets
NAME 							DESIRED 	CURRENT 	READY 	AGE
myapp-deployment-67c749c58c 	0 			0 			0 		22m
myapp-deployment-7d57dbdb8d 	5 			5 			5 		20m

$ kubectl rollout undo deployment/myapp-deployment
deployment “myapp-deployment” rolled back

- after rollback
$ kubectl get replicasets
NAME 							DESIRED 	CURRENT 	READY 	AGE
myapp-deployment-67c749c58c 	5 			5 			5 		22m
myapp-deployment-7d57dbdb8d 	0 			0 			0 		20m


Summarize Commands
------------------

- create deployment
> kubectl create –f deployment-definition.yml

> kubectl create deployment nginx --image=nginx:1.16   // deployment_name will nginx, replicaSet: nginx-<alphnu>
deployment.apps/nginx created

> kubectl run <pod_name> --image nginx // this creates deployment,replicaset,pod


- Get deployment
------------------
> kubectl get deployments


- Update deployment
---------------------
> kubectl apply –f deployment-definition.yml

> kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1


- Rollout status for the Deployment
> kubectl rollout status deployment/myapp-deployment

$ kubectl rollout status deployment nginx
Waiting for deployment "nginx" rollout to finish: 0 of 1 updated replicas are available...
deployment "nginx" successfully rolled out

- Rollout history
------------------
> kubectl rollout history deployment/myapp-deployment

$ kubectl rollout history deployment nginx
deployment.extensions/nginx
REVISION CHANGE-CAUSE
1     <none>

- Using the --revision flag:
- check the status of each revision individually by using the --revision flag

$ kubectl rollout history deployment nginx --revision=1
deployment.extensions/nginx with revision #1
 
Pod Template:
 Labels:    app=nginx    pod-template-hash=6454457cdb
 Containers:  nginx:  Image:   nginx:1.16
  Port:    <none>
  Host Port: <none>
  Environment:    <none>
  Mounts:   <none>
 Volumes:   <none>
 

- Using the --record flag:
We can use the --record flag to save the command used to create/update a deployment against the revision number. the same will be shows in "change-cause" in kubectl rollout history command.

$ kubectl create -f deployment-definition.yaml --record


$ kubectl set image deployment nginx nginx=nginx:1.17 --record
deployment.extensions/nginx image updated

$ kubectl rollout history deployment nginx
deployment.extensions/nginx
 
REVISION 	CHANGE-CAUSE
1     		<none>
2     		kubectl set image deployment nginx nginx=nginx:1.17 --record=true


$ kubectl edit deployments. nginx --record
deployment.extensions/nginx edited

$ kubectl rollout history deployment nginx
REVISION 	CHANGE-CAUSE
1     		<none>
2     		kubectl set image deployment nginx nginx=nginx:1.17 --record=true
3     		kubectl edit deployments. nginx --record=true


$ kubectl rollout history deployment nginx --revision=3
deployment.extensions/nginx with revision #3
 
Pod Template: Labels:    app=nginx
    pod-template-hash=df6487dc Annotations: kubernetes.io/change-cause: kubectl edit deployments. nginx --record=true
 
 Containers:
  nginx:
  Image:   nginx:latest
  Port:    <none>
  Host Port: <none>
  Environment:    <none>
  Mounts:   <none>
 Volumes:   <none>



- Rollback
-----------
> kubectl rollout undo deployment/myapp-deployment


	


Jobs
==================================
- There are different types of workloads (Web, application and database) that a container can serve. 

-  We have deployed simple web servers that serve users. These workloads are meant to continue to run for a long period of time, until manually taken down. There are other kinds of workloads such as batch processing, analytics or reporting that are meant to carry out a specific task and then finish. 

- For example, performing a computation, processing an image, performing some kind of analytics on a large data set, generating a report and sending an email etc. These are workloads that are meant to live for a short period of time, perform a set of tasks and then finish.

In Docker
-------
-  The docker container comes up, performs the requested operation, prints the output and exits.

$ docker run ubuntu expr 3 + 2

- The return code of the operation performed is shown in the bracket as well. In this case since the task was completed successfully, the return code is zero. 

$ docker ps -a
CONTAINER ID 	IMAGE 	CREATED 		STATUS 						PORTS
45aacca36850 	ubuntu 	43 seconds ago 	Exited (0) 41 seconds ago


In Kubernetes
--------------
- When the pod is created, it runs a container performs the computation task and exits and the pod goes into a Completed state. But, It then recreates the container in an attempt to leave it running. Again the container performs the required computation task and exits. And kubernetes brings it up again. And this continuous to happen until a threshold is reached. 


	
pod-definition.yaml
--------------------
apiVersion: v1
kind: Pod
metadata:
  name: math-pod
spec:
  containers:
  - name: math-add
    image: ubuntu
    command: ['expr', '3', '+', '2']
	
$ kubectl create –f pod-definition.yaml

$ kubectl get pods
NAME 		READY 	STATUS 		RESTARTS 	AGE
math-pod 	0/1 	Completed 	0 			1d


- RestartPolicy (restartPolicy: Never):
	- Kubernetes wants your applications to live forever. The default behavior of PODs is to attempt to restart the container in an effort to keep it running.
	
	- the property restartPolicy set on the POD, by default set to 'Always'. And that is why the POD ALWAYS recreates the container when it exits. other values are:  'Never' or 'OnFailure'
	
pod-restart-definition.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: math-pod
spec:
  restartPolicy: Never   // default is: Always
  containers:
  - name: math-add
    image: ubuntu
    command: ['expr', '3', '+', '2']
	
	
Kubernetes Jobs
---------------
- We have large data sets that requires multiple pods to process the data in parallel. We want to make sure that all PODs perform the task assigned to them successfully and then exit.

- While a ReplicaSet is used to make sure a specified number of PODs are running at all times, a Job is used to run a set of PODs to perform a given task to completion.

- kind: Job, apiVersion is batch/v1
- under the spec section, just like in replicasets or deployments, we have template. And under template we move all of the content from pod definition specification.  

job-definition.yaml
-------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: math-add-job
spec:  // job spec
  template:
    spec: // pod spec
      restartPolicy: Never   // default is: Always
      containers:
      - name: math-add
        image: ubuntu
        command: ['expr', '3', '+', '2']
	  
$ kubectl create –f job-definition.yaml
OR
$ kubectl create job <job_name> --image <image_name> --dry-run=client -o yaml > job.yaml

$ kubectl get jobs
NAME 			DESIRED 	SUCCESSFUL 	AGE
math-add-job 	1 			1 			38


- We see that it is in a completed state with 0 Restarts, indicating that kubernetes did not try to restart the pod. 

$ kubectl get pods
NAME 				READY 	STATUS 		RESTARTS 	AGE
math-add-job-l87pn 	0/1 	Completed 	0 			2m


$ watch "kubectl get all"  // watch linux command runs the given expression every 2 sec

- Look at pod logs to see the output of the job
- In our case, we just had the addition performed on the command line inside the container. So the output should be in the pods standard output. The standard output of a container can be seen using the logs command.
- For example, if the job was created to process an image, the processed image stored in a persistent volume would be the output or if the job was to generate and email a report, then the email with the report would be the result of the job.

- *** Kubernetes Job will continously attempt to run the job execution untill the job returned success i.e. exit status 0
- ** BUT kubernets has a default limit untill when it will keep on attempting before failing the job (backoffLimit).

$ kubectl logs math-add-job-ld87pn
5


$ kubectl delete job math-add-job
job.batch "math-add-job" deleted


- describe on job shows the total no. of attempt to run the job to completion. (Pods Statuses:  0 Running / 3 Succeeded / 1 Failed) i.e. 3 Succeeded + 1 Failed = 4 attepmt.

$ kubectl describe job throw-dice-job 

Name:           throw-dice-job
Namespace:      default
Selector:       controller-uid=18b2a1b4-a10e-4b8c-88e3-a92b9d58390c
Labels:         controller-uid=18b2a1b4-a10e-4b8c-88e3-a92b9d58390c
                job-name=throw-dice-job
Annotations:    <none>
Parallelism:    1
Completions:    3
Start Time:     Thu, 30 Dec 2021 15:35:49 +0000
Completed At:   Thu, 30 Dec 2021 15:36:06 +0000
Duration:       17s
Pods Statuses:  0 Running / 3 Succeeded / 1 Failed
Pod Template:
  Labels:  controller-uid=18b2a1b4-a10e-4b8c-88e3-a92b9d58390c
           job-name=throw-dice-job
  Containers:
   throw-dice-job:
    Image:        kodekloud/throw-dice
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From            Message
  ----    ------            ----   ----            -------
  Normal  SuccessfulCreate  4m2s   job-controller  Created pod: throw-dice-job-g74kv
  Normal  SuccessfulCreate  3m58s  job-controller  Created pod: throw-dice-job-rgcmq
  Normal  SuccessfulCreate  3m54s  job-controller  Created pod: throw-dice-job-zm588
  Normal  SuccessfulCreate  3m50s  job-controller  Created pod: throw-dice-job-cjmfs
  Normal  Completed         3m45s  job-controller  Job completed


Multiple Jobs - Sequential/Parallelism
---------------------------------------
- To run multiple pods for a given job definition, we set a value for completions under the job specification. And we set it to 3 to run 3 PODs. This time, when we create the job, We see the Desired count is 3, and the successful count is 0. 

- *** by default, the PODs are created one after the other (Sequential JOB execution). The second pod is created only after the first is finished.

job-definition.yaml
-------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: math-add-job
spec:
  completions: 3
  template:
    restartPolicy: Never   // default is: Always
    containers:
    - name: math-add
      image: ubuntu
      command: ['expr', '3', '+', '2']

$ kubectl create –f job-definition.yaml

$ kubectl get jobs
NAME 			DESIRED 	SUCCESSFUL 	AGE
math-add-job 	3 			3 			38

$ kubectl get pods
NAME 				READY 	STATUS 		RESTARTS 	AGE
math-add-job-l87pn 	0/1 	Completed 	0 			2m
math-add-job-87g4m 	0/1 	Completed 	0 			2m
math-add-job-d5z95 	0/1 	Completed 	0 			2m


-  But if the pods fail to run the job? ex: using a image: kodekloud/random-error which randomly completes or fails.

- When I create this job, first pod completes successfully, the second one fails, so a third one is created and that completes  successfully and the fourth one fails, and so does the fifth one and so to have 3 completions, the job creates a new pod which happen to complete successfully. And that completes the job.

job-random-definition.yaml
--------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name:  random-error-job
spec:
  completions: 3
  template:
    restartPolicy: Never   // default is: Always
    containers:
    - name: random-error
      image: kodekloud/random-error


$ kubectl create –f job-random-definition.yaml

$ kubectl get jobs
NAME 				DESIRED 	SUCCESSFUL 	AGE
random-error-job 	3 			 			38s

$ kubectl get pods
NAME 					READY 	STATUS 		RESTARTS
random-exit-job-ktmtt 	0/1 	Completed 	0
random-exit-job-sdsrf 	0/1 	Error 		0
random-exit-job-wwqbn 	0/1 	Completed 	0
random-exit-job-fkhfn 	0/1 	Error 		0
random-exit-job-fvf5t 	0/1 	Error 		0
random-exit-job-nmghp 	0/1 	Completed 	0



Parallelism
-------------
- Instead of getting the pods created sequentially we can get them created in parallel. 


job-parallel-definition.yaml
------------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name:  random-error-job
spec:
  completions: 3
  parallelism: 3  // create 3 pods in parallel, if we dont specify: will create pod sequentially after one is completed.
  template:
    restartPolicy: Never   // default is: Always
    containers:
    - name: random-error
      image: kodekloud/random-error	

$ kubectl create –f job-parallel-definition.yaml

$ kubectl get jobs
NAME 				DESIRED 	SUCCESSFUL 	AGE
random-error-job 	3 			3 			38s

- Two of which completes successfully. So we only need one more, so it’s intelligent enough to create one pod at a time until we get a total of 3 completed pods

$ kubectl get pods
NAME 					READY 	STATUS 		RESTARTS
random-exit-job-ktmtt 	0/1 	Completed 	0
random-exit-job-sdsrf 	0/1 	Error 		0
random-exit-job-wwqbn 	0/1 	Completed 	0
random-exit-job-fkhfn 	0/1 	Error 		0
random-exit-job-fvf5t 	0/1 	Error 		0





CronJobs
====================================================================
- A cronjob is a job that can be scheduled. Just like cron tab in Linux

- Say for example you have a job that generates a report and sends an email. the kind: Job runs instantly. Instead you could create a cronjob (kind: CronJob) to schedule and run it periodically as per the given cron expression

cron-job-definition.yaml
------------------------
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  spec:
    schedule: “*/1 * * * *”
    jobTemplate:
      spec:
		completions: 3
        parallelism: 3
        template:
          spec:
		    restartPolicy: Never
			containers:
			- name: reporting-tool
			  image: reporting-tool
			  
$ kubectl create –f cron-job-definition.yaml
OR
$ kubectl create cronjob throw-dice-cron-job --image kodekloud/throw-dice --schedule "30 21 * * *" --dry-run=client -o yaml > cronjob.yaml

cronjob.yaml
------------
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  jobTemplate:
    metadata:
      name: throw-dice-cron-job
    spec:
      template:
        spec:
          containers:
          - image: kodekloud/throw-dice
            name: throw-dice-cron-job
          restartPolicy: OnFailure
  schedule: 30 21 * * *

$ kubectl get cronjob



Networking Basic in Kubernetes
=========================================================================
- Unlike in the docker world were an IP address is always assigned to a Docker CONTAINER, in Kubernetes the IP address is assigned to a POD. Each POD in kubernetes gets its own internal IP Address.

- When you deploy multiple PODs, they all get a separate IP assigned. The PODs can communicate to each other through this IP. But accessing other PODs using this internal IP address MAY not be a good idea as its subject to change when PODs are recreated. 

- When a kubernetes cluster is SETUP, kubernetes does NOT automatically setup any kind of networking to handle these issues.

	- all the containers or PODs in a kubernetes cluster MUST be able to communicate with one another without having to configure NAT
	
- Fortunately, we don’t have to set it up ALL on our own as there are multiple pre-built solutions available. Some of them are the cisco ACI networks, Cilium, Big Cloud Fabric, Flannel, Vmware NSX-t and Calico.
	- Depending on the platform you are deploying your Kubernetes cluster on you may use any of these solutions.
	
	
	
Kubernetes Services
=========================================================================
- Kubernetes Services enable communication between various components within and outside of the application.

- Kubernetes Services helps us connect applications together with other applications or users, front-end, back-end, external data source
	- Services enable the front-end application to be made available to users via Service (NodePort)
	-  it helps communication between back-end and front-end PODs via service (ClusterIp)
	
- Types of services:
	- NodePort
	-----------
		-  NodePort service listens to a port on the Node and forwards requests to PODs. NodePort were the service makes an internal POD accessible on a Port on the Node
		-  3 ports are involved. 
			- the port on the POD where the actual web server is running is port 80. referred as targetPort, because that is were the service forwards the requests to. 
			- the port on the service itself. referred to as the port. The service is in fact like a virtual server inside the node. Inside the cluster it has its own IP address. And that IP address is called the Cluster-IP of the service. 
			- the port on the Node itself which we use to access the web server externally, known as the NodePort. NodePorts can only be in a valid range which is from 30000 to 32767
			
service-definition.yml
----------------------
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
  - targetPort: 80
	port: 80
	nodePort: 30008  // it should be a free port, if not provided, k8s assigns any random free port from reserved 30000 to 32767
  selector:
	app: front-end


> kubectl create –f service-definition.yml
service "myapp-service" created

> kubectl get services
NAME 			TYPE 		CLUSTER-IP 		EXTERNAL-IP 	PORT(S) 		AGE
kubernetes 		ClusterIP 	10.96.0.1 		<none> 			443/TCP 		16d
myapp-service 	NodePort 	10.106.127.123 	<none> 			80:30008/TCP 	5m
	

> curl http://192.168.1.2:30008  // external user access the pod outside kubernetes cluster., using any node IP and the nodeport
> curl http://192.168.1.3:30008
> curl http://192.168.1.4:30008


			- selector links the service to the pod.

			- ports is an array hence we can have multiple such port mappings within a single service.
			
			- when the service is created, it looks for matching PODs with the labels and finds 3 of them. The service then automatically selects all the 3 PODs as endpoints to forward the external requests coming from the user.
			
			- uses a random algorithm to balance load, acts as a built-in load balancer to distribute load across different PODs.
	
	
	
	- ClusterIP (default service type)
	----------------------------------
		- service creates a virtual IP inside the cluster to enable communication between different services such as a set of front-end servers to a set of backend servers
		
		- You may have a number of PODs running a front-end web server, another set of PODs running a backend server, a set of PODs running a key value store like Redis, another set of PODs running a persistent database like MySQL. The requests are forwarded to one of the PODs under the service randomly.
		
		-  Each service gets an IP and name assigned to it inside the cluster and that is the name that should be used by other PODs to access the service.
		
		
service-definition.yml
----------------------
apiVersion: v1
kind: Service
metadata:
  name: back-end
spec:
  type: ClusterIP
  ports:
  - targetPort: 80
    port: 80
  selector:
	app: back-end
		

> kubectl create –f service-definition.yml


> kubectl get services
NAME 		TYPE 		CLUSTER-IP 		EXTERNAL-IP 	PORT(S) 	AGE
kubernetes 	ClusterIP 	10.96.0.1 		<none> 			443/TCP 	16d
back-end 	ClusterIP 	10.106.127.123 	<none> 			80/TCP 		2m




	-  LoadBalancer
	---------------
		- it provisions a load balancer for our service in supported cloud providers.
		
		- Using NodePort service type we expose the application to the end users on a high end port of the Nodes (30000 to 32767), but we cant share IP of kubernetes cluster nodes to end user.
		
		- For this, you will be required to setup a separate Load Balancer VM in your environment. In this case I deploy a new VM for load balancer purposes and configure it to forward requests that come to it to any of the Ips of the Kubernetes nodes. then configure my organizations DNS to point to this load balancer when a user hosts http://myapp.com
		
		-  Now setting up that load balancer by myself is a tedious task, and I might have to do that in my local or onprem environment. However, if I happen to be on a supported CloudPlatform, like Google Cloud Platform, I could leverage the native load balancing functionalities of 
the cloud platform to set this up. 

		- Kubernetes has built-in integration with supported cloud platforms.


service-definition.yml
-----------------------
apiVersion: v1
kind: Service
metadata:
  name: front-end
spec:
  type: LoadBalancer
  ports:
  - targetPort: 80
    port: 80
  selector:
    app: myapp
    type: front-end



> kubectl create –f service-definition.yml
service “front-end" created


> kubectl get services
NAME 			TYPE 		CLUSTER-IP 		EXTERNAL-IP 	PORT(S) 	AGE
kubernetes 		ClusterIP 	10.96.0.1 		<none> 			443/TCP 	16d
front-end 		LoaBalancer 10.106.127.123 	<Pending> 		80/TCP 		2m



===============================================================
Ingress Resource and Ingress controller
===============================================================
- read the info detailed in Ingress section in KodeKloud-Kubernetes+-CKAD.pdf to understand the pain points of using NodePort/LoadBalancer service types in exposing the applications running on pods via deployments.

- Ingress helps your users access your application using a single Externally accessible URL, that you can configure to route to different services within your cluster based on the URL path, at the same time terminate TLS.

-  ingress as a layer 7 load balancer built-in to the kubernetes cluster that can be configured using native kubernetes primitives

- Ingress controller: Now remember, even with Ingress you still need to expose it to make it accessible outside the cluster. So you still have to either publish it as a NodePort or with a Cloud Native LoadBalancer. But that is just a one time thing. Going forward you are going to perform all your load balancing, Auth, SSL and URL based routing configurations on the Ingress controller

	- Without ingress, we would use a reverse-proxy or a load balancing solution like NGINX or HAProxy or Traefik. I would deploy them on my 
kubernetes cluster and configure them to route traffic to other services. The configuration involves defining URL Routes, SSL certificates etc.

	- Ingress is implemented by Kubernetes in the same way. You first deploy a supported 
solution (NGINX, Contour, HAPROXY, TRAFIK and Istio PODs  deployed as deployments), and then specify a set of rules to configure Ingress. The solution you deploy is called as an Ingress Controller. 

- ress resources: the set of rules/routing you configure is called as Ingress Resources. Ingress resources are created using definition files.


Ingress controller
----------------------------------------------------------
- a kubernetes cluster does NOT come with an Ingress Controller by default. So if you simply create ingress resources and expect them to work, 
they wont.

- There are a number of solutions available for Ingress, a few of them being GCE - which is Googles Layer 7 HTTP Load Balancer. NGINX, Contour, HAPROXY, TRAFIK and Istio. Out of this, GCE and NGINX are currently being supported and maintained by the Kubernetes project. 
	
- we will use NGINX as an example. An NGINX Controller is deployed as just another deployment in Kubernetes.

- the image used is nginx-ingress-controller with the right version. This is a special build of NGINX built specifically to be used as 
an ingress controller in kubernetes. So it has its own requirements. Within the image the nginx program is stored at location /nginx-ingress-controller. So you must pass that as the command to start the nginx-service. 

- it has a set of configuration options such as the path to store the logs, keep-alive threshold, ssl settings, session timeout etc. In order to decouple these configuration data from the nginx-controller image, you must create a ConfigMap object and pass that in.

- Now remember the ConfigMap object need not have any entries at this point. A blank object will do. But creating one makes it easy for you to modify a configuration setting in the future. You will just have to add it in to this ConfigMap.


- You must also pass in two environment variables that carry the POD’s name and namespace it is deployed to. The nginx service requires these to read the configuration data from within the POD. 

- Specify the ports (http and https) used by the ingress controller. 


ingress-controller-configmap.yaml
-----------------------------------
kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration



ingress-controller-deployment.yaml
-----------------------------------
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 1
selector:
  matchLabels:
    name: nginx-ingress
template:
  metadata:
    labels:
      name: nginx-ingress
spec:
  containers:
  - name: nginx-ingress-controller
    image: quay.io/kubernetes-ingresscontroller/nginx-ingress-controller:0.21.0
    args:
    - /nginx-ingress-controller
    - --configmap=$(POD_NAMESPACE)/nginx-configuration
  env:
  - name: POD_NAME
    valueFrom:
      fieldRef:
        fieldPath: metadata.name
  - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
  ports:
  - name: http
    containerPort: 80
  - name: https
    containerPort: 443



- We then need a service to expose the ingress controller to the external world. So we create a service of type NodePort with the nginx-ingress label selector to link the service to the deployment. 


ingress-controller-service.yaml
-----------------------------------
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec:
  type: NodePort
ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  - port: 443
    targetPort: 443
    protocol: TCP
    name: https
selector:
  name: nginx-ingress


- So with these three objects (1 configMap (nginx configs), 1 deployment for ingress-controller(nginx), 1 NodePort service for exposing the ingress-controller to external world) we should be ready with an ingress controller in its simplest form. 


> kubectl create -f ingress-controller-configmap.yaml

> kubectl create -f ingress-controller-deployment.yaml


> kubectl create -f ingress-controller-service.yaml




Ingress Resources
-------------------------------------------
- An Ingress Resource is a set of rules and configurations applied on the ingress controller. 

- Ingress resources deployed in the cluster gets automatically detected by the ingress controller.

- You can configure rules to say, 

1/ simply forward all incoming traffic to a single application, 

Ingress-wear.yaml
-----------------
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear
spec:
  backend:
    serviceName: wear-service
    servicePort: 80

> kubectl create –f Ingress-wear.yaml
ingress.extensions/ingress-wear created


> kubectl get ingress
NAME 			HOSTS 	ADDRESS 	PORTS 
ingress-wear 	* 		80 			2s


here, traffic is routed to the application services and not PODs directly. The Backend section defines where the traffic will be routed to. 
So if it’s a single backend, then you don’t really have any rules.




2/ route traffic to different applications (deployments) based on the URL. 
ex: 
my-online-store.com/wear gets routed to wear app service created for wear app deployment, 
my-online-store.com/watch gets routed to watch app service created for wear app deployment.


Ingress-wear-watch.yaml
---------------------------
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - http:
      paths:
      - path: /wear
	    backend:
          serviceName: wear-service
          servicePort: 80
      - path: /watch
		backend:
		  serviceName: watch-service
		  servicePort: 80


> kubectl describe ingress ingress-wear-watch

Name: ingress-wear-watch
Namespace: default
Address:
Default backend: default-http-backend:80 (<none>)
Rules:
	Host 	Path 		Backends
	---- 	---- 		--------
	*
			/wear 		wear-service:80 (<none>)
			/watch 		watch-service:80 (<none>)
Annotations:
Events:
	Type 	Reason 	Age 	From 						Message
	---- 	------ 	---- 	---- 						-------
	Normal 	CREATE 	14s 	nginx-ingress-controller 	Ingress default/ingress-wear-watch



here, since the domain is same (my-online-store.com), we just need single rule, but need multiple paths (/wear and /watch) for each URL paths.

*** NOte, we also need to deploy an additional service: default-http-backend, in case URLs that does not match any of these rules, then the user is 
directed to the service specified as the default backend. i.e.  default backend service to display this 404 Not Found error page



3/
route user based on the different domain names itself. i.e.
wear.my-online-store.com
watch.my-online-store.com


ingress-wear-watch.yaml
------------------------
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - host: wear.my-online-store.com
    http:
    paths:
    - backend:
        serviceName: wear-service
        servicePort: 80
		
  - host: watch.my-online-store.com
    http:
    paths:
    - backend:
        serviceName: watch-service
        servicePort: 80



here,  Now that we have two domain names, we create two rules. One for each domain. To split traffic by domain name, we use the 
host field. The host field in each rule matches the specified value with the domain name used in the request URL and routes traffic to the appropriate backend.

- You can still have multiple path specifications in each of these to handle different URL paths.



Exercise:
---------------------------------

- describe service
$ kubectl -n critical-space describe service

Name:              pay-service
Namespace:         critical-space
Labels:            <none>
Annotations:       <none>
Selector:          app=webapp-pay
Type:              ClusterIP
IP Families:       <none>
IP:                10.100.108.183
IPs:               10.100.108.183
Port:              <unset>  8282/TCP
TargetPort:        8080/TCP
Endpoints:         10.244.0.9:8080
Session Affinity:  None
Events:            <none>


$ kubectl get deployments.apps --all-namespaces 
NAMESPACE       NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
app-space       default-backend            1/1     1            1           2m23s
app-space       webapp-video               1/1     1            1           2m23s
app-space       webapp-wear                1/1     1            1           2m23s
ingress-space   nginx-ingress-controller   1/1     1            1           2m24s
kube-system     coredns                    2/2     2            2           6m43s


$ kubectl get ingress -n app-space 
NAME                 CLASS    HOSTS   ADDRESS   PORTS   AGE
ingress-wear-watch   <none>   *                 80      4m35s



- describe ingress resource definition
$ kubectl -n app-space describe ingress ingress-wear-watch
Name:             ingress-wear-watch
Namespace:        app-space
Address:          
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
Rules:
  Host        Path  Backends
  ----        ----  --------
  *           
              /wear    wear-service:8080 (10.244.0.7:8080)
              /watch   video-service:8080 (10.244.0.4:8080)
Annotations:  nginx.ingress.kubernetes.io/rewrite-target: /
              nginx.ingress.kubernetes.io/ssl-redirect: false
Events:       <none>

$ kubectl -n app-space get ingress ingress-wear-watch -o yaml > ingress.yaml


$ kubectl -n app-space delete ingress ingress-wear-watch 
ingress.networking.k8s.io "ingress-wear-watch" deleted

$ kubectl apply -f ingress.yaml 
ingress.networking.k8s.io/ingress-wear-watch created

$ kubectl get ingress -n app-space 
NAME                 CLASS    HOSTS   ADDRESS   PORTS   AGE
ingress-wear-watch   <none>   *                 80      10s




- Ingress controller exercise:
-----------------------------------------------------------------------------------------------
- create a namespace:
$ kubectl create namespace ingress-space
namespace/ingress-space created

$ kubectl get namespaces

NAME              STATUS   AGE
app-space         Active   91s
default           Active   4m46s
ingress-space     Active   3s
kube-node-lease   Active   4m49s
kube-public       Active   4m49s
kube-system       Active   4m50s

- create a empty configMap
$ kubectl create configmap nginx-configmap -n ingress-space 
configmap/nginx-configmap created

- create a service account
$ kubectl -n ingress-space create serviceaccount ingress-serviceaccount
serviceaccount/ingress-serviceaccount created

- created the Roles and RoleBindings for the ServiceAccount.
$ kubectl -n ingress-space get roles.rbac.authorization.k8s.io              
NAME           CREATED AT
ingress-role   2022-01-01T18:08:19Z

$ kubectl -n ingress-space describe roles.rbac.authorization.k8s.io ingress-role 
Name:         ingress-role
Labels:       app.kubernetes.io/name=ingress-nginx
              app.kubernetes.io/part-of=ingress-nginx
Annotations:  <none>
PolicyRule:
  Resources   Non-Resource URLs  Resource Names                     Verbs
  ---------   -----------------  --------------                     -----
  configmaps  []                 []                                 [get create]
  configmaps  []                 [ingress-controller-leader-nginx]  [get update]
  endpoints   []                 []                                 [get]
  namespaces  []                 []                                 [get]
  pods        []                 []                                 [get]
  secrets     []                 []                                 [get]
  
$ kubectl -n ingress-space get rolebindings.rbac.authorization.k8s.io 
NAME                   ROLE                AGE
ingress-role-binding   Role/ingress-role   4m37s

$ kubectl -n ingress-space describe rolebindings.rbac.authorization.k8s.io ingress-role-binding 
Name:         ingress-role-binding
Labels:       app.kubernetes.io/name=ingress-nginx
              app.kubernetes.io/part-of=ingress-nginx
Annotations:  <none>
Role:
  Kind:  Role
  Name:  ingress-role
Subjects:
  Kind            Name                    Namespace
  ----            ----                    ---------
  ServiceAccount  ingress-serviceaccount
  
  
- deploy the Ingress Controller. ingress controller of type nginx image is used here.
- NOTE: ingress controller is deployed as regular deployments hence it gets, Deployment, ReplicaSet and Pod at the end.

ingress-controller.yaml
------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-controller
  namespace: ingress-space
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      labels:
        name: nginx-ingress
    spec:
      serviceAccountName: ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --default-backend-service=app-space/default-http-backend
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
			  


$ kubectl -n app-space get deployments.apps default-backend -o yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: default-backend
  namespace: app-space
spec:
  replicas: 1
  selector:
    matchLabels:
      app: default-backend
  template:
    metadata:
      labels:
        app: default-backend
    spec:
      containers:
      - image: kodekloud/ecommerce:404
        imagePullPolicy: Always
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP
			  

$ kubectl create -f ingress-controller.yaml 
deployment.apps/ingress-controller created

$ kubectl -n ingress-space get all
NAME                                     READY   STATUS    RESTARTS   AGE
pod/ingress-controller-5857685bf-jr9cb   1/1     Running   0          85s

NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ingress-controller   1/1     1            1           85s

NAME                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/ingress-controller-5857685bf   1         1         1       85s
			  




- create a service for the ingress-controller deployment to make Ingress available to external users. 
- NOTE *** the selector gets automatically taken from the depoloyment as we are exposing the deployment.
- kubectl expose --help

$ kubectl -n ingress-space expose deployment ingress-controller --name ingress --type NodePort --port 80 --target-port 80 --dry-run=client -o yaml > ingress-svc.yaml

ingress-svc.yaml
----------------
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  name: ingress
  namespace: ingress-space    // added this manually, as no option to pass using kubectl imperative command above
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    NodePort: 30080			// added this manually, as no option to pass using kubectl imperative command above
  selector:
    name: nginx-ingress
  type: NodePort


$ kubectl apply -f ingress-svc.yaml 
service/ingress created




- Create the ingress resource to make the applications available at /wear and /watch on the Ingress service.

$ kubectl -n app-space get service
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
default-http-backend   ClusterIP   10.110.140.184   <none>        80/TCP     46m
video-service          ClusterIP   10.111.81.241    <none>        8080/TCP   46m
wear-service           ClusterIP   10.110.169.135   <none>        8080/TCP   46m

ingress-resource.yaml
---------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
            name: wear-service
            port:
              number: 8080
      - path: /watch
        pathType: Prefix
        backend:
          service:
            name: video-service
            port:
              number: 8080
			  
			  
			  
$ kubectl apply -f ingress-res.yaml 
ingress.networking.k8s.io/minimal-ingress created

$ kubectl -n app-space describe ingress minimal-ingress 
Name:             minimal-ingress
Namespace:        app-space
Address:          
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
Rules:
  Host        Path  Backends
  ----        ----  --------
  *           
              /wear    wear-service:8080 (10.244.0.6:8080)
              /watch   video-service:8080 (10.244.0.4:8080)
Annotations:  nginx.ingress.kubernetes.io/rewrite-target: /
Events:       <none>








===============================================================================
Network Policy
===============================================================================

-  you have a web server serving front-end to users, an app server serving backend API’s and a database server. The user sends in a request to the web server at port 80. The web server then sends a request to the API server at port 5000 in the backend. The API server then fetches data from the 
database server at port 3306. And then sends the data back to the user.

- two types of traffic here. Ingress and Egress.

-  For example, for a web server, the incoming traffic from the users is an Ingress Traffic. And the outgoing requests to the app server is Egress traffic.

-  remember you are only looking at the direction in which the traffic originated. The response back to the user

- the backend API server receives ingress traffic from the web server on port 80 and has egress traffic to port 3306 to the database server.

- from the database servers perspective, it receives Ingress traffic on port 3306 from the API server.

hence the traffic flow and rules will be:

1/ An Ingress rule that is required to accept HTTP traffic on port 80 on the web server.
2/ An Egress rule to allow traffic from the web server to port 5000 on the API server.
3/ An ingress rule to accept traffic on port 5000 on the API server.
4/ An egress rule to allow traffic to port 3306 on the database server.
5/ An egress rule to allow traffic to port 3306 on the database server.


Network Security
----------------------------------------

- So we have a cluster with a set of nodes hosting a set of pods and services. Each node has an IP address and so does each pod as well as service. One of the pre-requisite for networking in kubernetes, is whatever solution you implement, the pods should be able to communicate with each other without having to configure any additional settings, like routes. 

- all pods are on a virtual private network that spans across the nodes in the kubernetes cluster.

- Kubernetes is configured by default with an “All Allow” rule that allows traffic from any pod to any other pod or services for an given namespace.

-  For each component in the application we deploy a POD. One for the front-end web server, for the API server and one for the database. We create services to enable communication between the PODs as well as to the end user. 

- the security teams and audits require, the front-end web server should NOT be allowed to communicate with the database server directly. Hence a Network Policy is needed to allow traffic to the db server only from the api server.

- Network Policy acts like one more layer on top of selected PODs, to allow/deny access from other objects in kubernetes.

- A Network policy is another object in the kubernetes namespace. Just like PODs, ReplicaSets or Services. You apply a network policy on selected pods. 

- You link a network policy to one or more pods. You can define rules within the network policy. In this case I would say, only allow Ingress Traffic from the API Pod on Port 3306. Once this policy is created, it blocks all other traffic to the Pod and only allows traffic that matches the specified rule. Again, this is only applicable to the Pod on which the network policy is applied. 

- Ingress/Egress rules defined on the Network Policy on selected PODs will only be ALLOWED and rest all traffic will be DENIED.

-  under the spec section, we will first move the pod selector to apply this policy to the db pod and then the network rule.

- this policy applied on DB Pods, ingress rule from api-pod on port 3306, rest all traffics will be blocked,.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db
	  
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: api-pod
		  
	ports:
    - protocol: TCP
      port: 3306
	  

- Network Policies are enforced by the Network Solution implemented on the Kubernetes Cluster. 

- Not all network solutions support network policies. A few of them that are supported are kube-router, Calico, Romana and Weave-net. 

- If you used Flannel as the networking solution, it does not support network policies as of this recording

- remember, even in a cluster configured with a solution that does not support network policies, you can still create the policies, but they will just not be enforced. You will not get an error message saying the networking solution does not support network policies.


Exercises on Network Policy
----------------------------------------------------------------------------------

- Network policy wont be visible in kubectl get all command.

$ kubectl get netpol

NAME             POD-SELECTOR   AGE
payroll-policy   name=payroll   3m49s


$ kubectl describe netpol payroll-policy 

Name:         payroll-policy
Namespace:    default
Created on:   2022-01-02 02:24:42 +0000 UTC
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     name=payroll
  Allowing ingress traffic:
    To Port: 8080/TCP
    From:
      PodSelector: name=internal
  Not affecting egress traffic
  Policy Types: Ingress
  


- exercise:
- Create a network policy to allow traffic from the Internal application only to the payroll-service and db-service.
- Note: Network policy is applicable only to PODs and Namespaces and NOT on Services
--------------------------------------------
$ kubectl get all

NAME           READY   STATUS    RESTARTS   AGE
pod/external   1/1     Running   0          2m48s
pod/internal   1/1     Running   0          2m48s
pod/mysql      1/1     Running   0          2m48s
pod/payroll    1/1     Running   0          2m48s

NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/db-service         ClusterIP   10.99.126.77    <none>        3306/TCP         2m48s
service/external-service   NodePort    10.100.216.6    <none>        8080:30080/TCP   2m48s
service/internal-service   NodePort    10.105.72.135   <none>        8080:30082/TCP   2m48s
service/kubernetes         ClusterIP   10.96.0.1       <none>        443/TCP          31m
service/payroll-service    NodePort    10.111.126.95   <none>        8080:30083/TCP   2m48s

ingress-policy.yaml -- allows egress from internal pod to mysql and payroll pods.
----------------------------------------------------------------------------------
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306
  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080
	  

$ kubectl create -f ingress-policy.yaml

networkpolicy.networking.k8s.io/internal-policy created



$ kubectl get netpol     
  
NAME              POD-SELECTOR    AGE
internal-policy   name=internal   32s
payroll-policy    name=payroll    2m22s




$ kubectl describe netpol internal-policy

Name:         internal-policy
Namespace:    default
Created on:   2022-01-02 03:29:37 +0000 UTC
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     name=internal
  Not affecting ingress traffic
  Allowing egress traffic:
    To Port: 3306/TCP
    To:
      PodSelector: name=mysql
    ----------
    To Port: 8080/TCP
    To:
      PodSelector: name=payroll
  Policy Types: Egress





==========================================================================================
Persistent volumes
==========================================================================================

persist data in Docker
--------------------------
- To persist data processed by the containers, we attach a volume to the containers when they are created. The data processed by the container is now placed in this volume, thereby retaining it permanently.


Docker Storage
--------------------------
- when we install docker on a system, it created the below folder structure at var/lib/docker:
- var/lib/docker
	- aufs
	- containers
	- image
	- volumes
- this is where docker stores all of its data (files related to images/containers etc.)
- any volumes created goes to volume folder.


Docker Volume Mount
---------------------------
$ docker volume ls      -----> list all the registered volumes.

- volume mount is to mount a location/directory of /var/lib/docker/volumes on docker host to the containres.

- when we stop the container, the entire CONTAINER layer gets purged along with the container. what if we want to persist specially the DB data files created by a DB container.

- to do this, we first need to create a volume, using "docker volume create data_volume", it creates a new folder "data_volume" under "/var/lib/docker/volumes/"

- then we run the "docker run -v data_volume:/var/lib/mysql mysql", this way we mount the external host location to the /var/lib/mysql directory of the mysql container. NOTE: "/var/lib/mysql" is the default location where mysql stores data files.

- Even we dont create the volume first, docker creates new volume if we use -v option during docker run.

$ docker run -v data_volume2:/var/lib/mysql mysql  

- -v is old syntax:

$ docker run \
--mount type=mount,source=data_volume2,target=/var/lib/mysql mysql

- source: location on docker host
- target: is location on the container

- this will create a new volume i.e. data_volume2 and then mount and start the container.

- we should see all these directories if we list /var/lib/docker/volumes/ directory on the docker host.


Docker Bind Mount (external - NAS)
--------------------------------------
- - Bind mount is to mount any location/directory on docker host to the containres.
- If we have an external NAS storage which is mounted on Docker host on a different directory than docker volueme directory (/var/lib/docker/volumes)
- in this case we need to give complete path ex: NAS mount is: /data/mysql

$ docker run -v /data/mysql:/var/lib/mysql mysql

OR

$ docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql




Volumes in Kubernetes
-------------------------------------
- Similar to Docker, the PODs created in Kubernetes are transient in nature

- When a POD is created to process data and then deleted, the data processed by it gets deleted as well. For this we attach a volume to the POD. The data generated by the POD is now stored in the volume



Volumes & Mounts in Kubernetes
-------------------------------------
- to retain the data processed by the POD, we need to create 
1/ a volume with its storage option (various options available) and 
2/ mount that volume on some path for the underlying container of the POD.

apiVersion: v1
kind: Pod
metadata:
  name: random-number-generator
spec:
  containers:
  - image: alpine
    name: alpine
    command: ["/bin/sh","-c"]
    args: ["shuf -i 0-100 -n 1 >> /opt/number.out;"]
    volumeMounts:
    - mountPath: /opt
      name: data-volume
  
  volumes:
  - name: data-volume
    hostPath:
      path: /data
	  type: Directory
	  

- a simple implementation of volumes

	- We have a single node kubernetes cluster. We create a simple POD that generates a random between 1 and 100 and writes that to a file at /data/number.out and then gets deleted along with the random number. 
	
	- To retain the number generated by the pod, we create a volume. And a Volume needs a storage.
	
	- When you create a volume you can chose to configure it’s storage in different ways. but for now we will simply configure it to use a directory on the host. In this case I specify a path /data on the host. This way any files created in the volume would be stored in the directory data on my node. 
	
	volumes:
	- name: data-volume
	  hostPath:
	    path: /data
	    type: Directory
	
	- Once the volume is created, to access it from a container we mount the volume to a directory inside the container. 
	
	-  use the volumeMounts field in each container to mount the data-volume to the directory /opt within the container. The random number will now be written to /opt mount inside the container, which happens to be on the data-volume which is in fact /data directory on the host.
	
	- When the pod gets deleted, the file with the random number still lives on the host.
	

- is not recommended for use in a multi-node cluster. This is because the PODs would use the /data directory on all the nodes, and expect all of them to be the same and have the same data. Since they are on different servers, they are in fact not the same, unless you configure some kind of external replicated clustered storage solution. 


- Kubernetes supports several types of standard storage solutions such as NFS, glusterFS, Flocker, FibreChannel, CephFS, ScaleIO or public cloud solutions like AWS EBS, Azure Disk or File or Google’s Persistent Disk. 


Volume from configMap
---------------------
- create a configMap

config-map-definition.yaml
---------------------------
apiVersion: v1
kind: ConfigMap
metadata:
	name: app-config
data:
	DB_host: mysql
	DB_user: root
	DB_password: passwd
	
$ kubectl create -f config-map-definition.yaml 
configmap/app-config created


$ kubectl get configmap

NAME               DATA   AGE
kube-root-ca.crt   1      9m3s
app-config         3      17s


$ kubectl describe configmap app-config 

Name:         app-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
DB_user:
----
root
DB_host:
----
mysql
DB_password:
----
passwd

BinaryData
====

Events:  <none>


- now create the pod using a volume of cofigMap

pod-definition.yaml
---------------------
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx:alpine
    name: nginx
    volumeMounts:
    - name: app-config-vol
      mountPath: /etc/app-config
  volumes:
  - name: app-config-vol
    configMap:
      name: app-config
	  

$ kubectl create -f pod.yaml 
pod/nginx created


$ kubectl get pod
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          7s

$ kubectl exec -it nginx -- ls -lrt /etc/app-config
total 0
lrwxrwxrwx    1 root     root            14 Jan 14 05:01 DB_user -> ..data/DB_user
lrwxrwxrwx    1 root     root            18 Jan 14 05:01 DB_password -> ..data/DB_password
lrwxrwxrwx    1 root     root            14 Jan 14 05:01 DB_host -> ..data/DB_host


$ kubectl exec -it nginx -- cat /etc/app-config/DB_user
root

$ kubectl exec -it nginx -- cat /etc/app-config/DB_password
passwd

$ kubectl exec -it nginx -- cat /etc/app-config/DB_host
mysql

- So every properties defined in the configMap ends up creating individual files inside the volume of pod.



Volume Types in Kubernetes
----------------------------------
- various storage options available. 

- hostPath option for storage will be allocated on node where the POD is running, not for production.

- to configure an AWS Elastic Block Store volume as the storage or the volume, we replace hostPath field of the volume with awsElasticBlockStore field along with the volumeID and filesystem type. The Volume storage will now be on AWS EBS


volumes:
- name: data-volume
  awsElasticBlockStore:
    volumeID: <volume-id>
    fsType: ext4
	
	


Persistent Volumes in Kubernetes
-----------------------------------------
- Manage the Storage more centrally rather adding storage to every pod definitions.

- When we created volumes in the previous section we configured volumes within the POD definition file. So every configuration information required to configure storage for the volume goes within the pod definition file.


- Now, when you have a large environment with a lot of users deploying a lot of PODs, the users would have to configure storage every time for each POD. Whatever storage solution is used, the user who deploys the PODs would have to configure that on all POD definition files in his environment. Every time a change is to be made, the user would have to make them on all of his PODs. 


- You would like it to be configured in a way that an administrator can create a large pool of storage, and then have users carve out pieces from it as required.

- A Persistent Volume is a Cluster wide pool of storage volumes configured by an Administrator, to be used by users deploying applications on the cluster. The users can now select storage from this pool using Persistent Volume Claims.


pv-definition.yaml
-------------------
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi
  awsElasticBlockStore:
	volumeID: <volume-id>
	fsType: ext4

- Under spec, 
	- accessModes
		- Access Mode defines how the Volume should be mounted on the hosts. Weather in a ReadOnly mode, or ReadWrite mode. 
		- ReadOnlyMany, ReadWriteOnce or ReadWriteMany mode.

	- capacity
		- Specify the amount of storage to be reserved for this Persistent Volume. Which is set to 1GB here.
		
	- storage volume type
		- the hostPath option that uses storage from the node’s local directory. not to be used in a production environment.
		- public cloud storage options like: awsElasticBlockStore


kubectl create –f pv-definition.yaml
pv-definition.yaml

> kubectl get persistentvolume
NAME 	CAPACITY 	ACCESS MODES 	RECLAIM POLICY 		STATUS 		CLAIM 	STORAGECLASS 	REASON 	AGE
pv-vol1 1Gi 		RWO 			Retain 				Available 									3m




Persistent Volume Claim
----------------------------------------------------------------------

- Persistent Volume Claim to make the storage available to a node

- Persistent Volumes and Persistent Volume Claims are two separate objects in the Kubernetes namespace.

- An Administrator creates a set of Persistent Volumes and a user creates Persistent Volume Claims to use the storage.

- Once the Persistent Volume Claims are created, Kubernetes binds the Persistent Volumes to Claims based on the request and properties set on the volume.

- There is a one-to-one relationship between Claims and Volumes

Binding
----------------------------------------------------------------------

- Once the Persistent Volume Claims are created, Kubernetes binds the Persistent Volumes to Claims based on the request and properties set on the volume. 

- Every Persistent Volume Claim is bound to a single Persistent volume. During the binding process, kubernetes tries to find a Persistent Volume that has sufficient Capacity as requested by the Claim, and any other requested properties such as Access Modes, Volume Modes, Storage Class etc. 

- However, if there are multiple possible matches for a single claim, and you would like to specifically use a particular Volume, you could still use labels and selectors to bind to the right volumes.

- a smaller Claim may get bound to a larger volume if all the other criteria matches and there are no better options. 

- There is a one-to-one relationship between Claims and Volumes, so no other claim can utilize the remaining capacity in the volume.

- If there are no volumes available the Persistent Volume Claim will remain in a pending state, until newer volumes are made available to the cluster. Once newer volumes are available the claim would automatically be bound to the newly available volume.

- persistentvolume-controller helps in binding PVC to pv.

pvc-definition.yaml
---------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
	  
	  
> kubectl create –f pvc-definition.yaml

> kubectl get persistentvolumeclaim
NAME 		STATUS 	VOLUME 	CAPACITY 	ACCESS MODES 
myclaim 	Pending



- When the claim is created, kubernetes looks at the volume created previously. The access Modes match. The capacity requested is 500 Megabytes but the volume is configured with 1 GB of storage. Since there are no other volumes available, the PVC is bound to the PV


- View PVCs

> kubectl get persistentvolumeclaim
NAME 		STATUS 	VOLUME 		CAPACITY 	ACCESS MODES 	STORAGECLASS 	AGE
myclaim 	Bound 	pv-vol1 	1Gi 		RWO 							43m



- Delete PVCs
- when PVC is deleted, Underlying PV is retained by default.

- 3 options for the mapped PVs in case PVC is deleted.
	- Retain (default). will remain until it is manually deleted by the administrator, BUT the PV will no longer be available for re-use by any other claims.
	
	- auto delete: This way as soon as the claim is deleted, the volume will be deleted as well. 
	
	- recycle: the data in the volume will be scrubbed before making it available to other claims

> kubectl delete persistentvolumeclaim myclaim
persistentvolumeclaim "myclaim" deleted


Claims As Volumes in Pod
-----------------------------------------
- Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod's namespace and uses it to get the PersistentVolume backing the claim.

- The volume is then mounted to the host and into the Pod.

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
		

Exercise:
-----------------------------------
1. Configure a volume to store these logs at /var/log/webapp on the host.

spec:
  containers:
    image: kodekloud/event-simulator
    imagePullPolicy: Always
	volumeMounts:
	- mountPath: /log
	  name: log-vol

volumes:
  - name: log-vol
    hostPath:
      path: /var/log/webapp
      type: DirectoryOrCreate


$ kubectl exec -it webapp -- ls -lrt /log
total 12
-rw-r--r--    1 root     root         11160 Jan 13 06:05 app.log

- file system lookup on same node.
$ ls -ltr /var/log/webapp/
total 12
-rw-r--r-- 1 root root 11675 Jan 13 06:05 app.log



2. Create a Persistent Volume with the given specification.
Volume Name: pv-log
Storage: 100Mi
Access Modes: ReadWriteMany
Host Path: /pv/log
Reclaim Policy: Retain

- https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume

- Kubernetes supports hostPath for development and testing on a single-node cluster. A hostPath PersistentVolume uses a file or directory on the Node to emulate network-attached storage.

- In a production cluster, you would not use hostPath. Instead a cluster administrator would provision a network resource like a Google Compute Engine persistent disk, an NFS share, or an Amazon Elastic Block Store volume. Cluster administrators can also use StorageClasses to set up dynamic provisioning.

pv-volume.yaml
--------------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
  labels:
    type: local
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /pv/log
	
$ kubectl create -f pv.yaml 
persistentvolume/pv-log created


$ kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv-log   100Mi      RWX            Retain           Available                                   11m


3. Let us claim some of that storage for our application. Create a Persistent Volume Claim with the given specification.

Volume Name: claim-log-1
Storage Request: 50Mi
Access Modes: ReadWriteOnce

pvc.yaml
----------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Mi
	  
$ kubectl get pvc
NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim-log-1   Pending                                                     6s


- since the access mode is dofferent between pv vs pvc, the pvc will stay in pending state.
$ kubectl get pvc
NAME          STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim-log-1   Bound    pv-log   100Mi      RWX                           6s



- Incase we try to delete a pvc while the same pvc is being used in a pod, the "kubectl delete pvc" command will be in terminating state.
$ kubectl delete pvc claim-log-1
persistentvolumeclaim "claim-log-1" deleted

$ kubectl get pvc
NAME          STATUS        VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim-log-1   Terminating   pv-log   100Mi      RWX                           10m

- deleting the pod will result in deleting the pvc
$ kubectl delete pod webapp 
pod "webapp" deleted

$ kubectl get pvc
No resources found in default namespace.

$ kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                 STORAGECLASS   REASON   AGE
pv-log   100Mi      RWX            Retain           Released   default/claim-log-1                           33m


=================================================================
Storage Classes
=================================================================
before this, we know, how to create PV and then claim that PV using PVC (gets bound based on requested size and accessMode) and finally use the PVC in POD as volumes.

in this case the problem is before creating the PV, we must have to create the disk on the cloud providers (ex: gcp)

> gcloud beta compute disk create --size 1GB --region us-east-1 pd-disk

pv-volume1.yaml
----------------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
spec:
  capacity:
    storage: 500Mi
  accessModes:
    - ReadWriteOnce
  gcePresistentDisk:
    pdName: pd-disk
	fsType: ext4
	

- Static Provisioning: everytime an application requires storage, we need to first manually provision storage on google cloud then manually create a PV using the same name as that of the disk we created.

- Dynamic Provisioning: Nice if the PV get automatically created as and when an application requires it. With Storage Class object, we can define a provisioner such as Google storage (kubernetes.io/gce-pd), AWS-EBS (kubernetes.io/aws-ebs) that can automatically provision new storage on google cloud and attach that to the POD when a claim is made.

- create a Storage class object, multiple available parameters based on the selected provisioner.

sc-definition.yaml
--------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard // or pd-ssd
  replication-type: none // or regional-pd
	
	
- we no longer need a PV definition as the PV and any associated storage is going to be created automatically when the storage class is created. 
	
- for the PVC to use the Storage Class we defined, we use the storage class name. thats how the PVC knows which storage class to use.
	
pvc.yaml
----------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim-sc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: google-storage
  resources:
    requests:
      storage: 500Mi

- With PVC created, the Storage Class associated with it used the defined provisioner to provision a new disk with the required size on GCP, and creates a persistent volume automatically and then binds the PVC to the Volume defined in POD definition.

- It still creates a PV but we dont manually create anymore. its created by the Storage Class.


- The usage of Storage class is to categorize different types of storage and make them available for the app user to consume based on the need.
- ex: silver storage class with standard HDD, gold storage class with SSD and a platinum class with SSD and replication


sc-silver-definition.yaml
-------------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: silver
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
  replication-type: none


sc-gold-definition.yaml
-------------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gold
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
  replication-type: none
  
  
sc-platinum-definition.yaml
----------------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: silver
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
  replication-type: regional-pd
  

- Next time when we create PVC, we can define the class of storage we want to satisfy the need.

$ kubectl get sc
NAME                        PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path (default)        rancher.io/local-path           Delete          WaitForFirstConsumer   false                  14m
local-storage               kubernetes.io/no-provisioner    Delete          WaitForFirstConsumer   false                  2m14s
portworx-io-priority-high   kubernetes.io/portworx-volume   Delete          Immediate              false                  2m14s

- here, VOLUMEBINDINGMODE can be Immediate (provisoning of storage to pvc will occur as soon as the pvc is created) / WaitForFirstConsumer (pvc created but in pending status and waiting for the first POD use the same pvc)

- The Storage Class makes use of VolumeBindingMode set to WaitForFirstConsumer. This delays the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.

local-storage.yaml
-------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{},"name":"local-storage"},"provisioner":"kubernetes.io/no-provisioner","volumeBindingMode":"WaitForFirstConsumer"}

  name: local-storage
provisioner: kubernetes.io/no-provisioner
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer


$ kubectl get pvc
NAME        STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
local-pvc   Pending

$ kubectl create -f pod.yaml 
pod/nginx created


$ kubectl get pvc
NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
local-pvc   Bound    local-pv   500Mi      RWO            local-storage   37m


- on creating a new pod with the same pvc, the pvc status shows 'Bound'

- example of:
Create a new Storage Class called delayed-volume-sc that makes use of the below specs:
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer


pv.yaml
-------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: delayed-volume-sc
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer






=============================================================================
Stateful Sets
=============================================================================

- why we need stateful states and why cant we leave with deployments.


- lets understand a deployment pattern of mysql without k8s i.e. on bear metal boxes.

- we installed mysql server on one host and user are writing to it. 
- to withstand failures, we installed mysql on additional servers, these server are now empty, Now, how do we replicate data from original server to the newly installed servers.
- there are different toplogies available, the most easy way is a single master and multi slave topology, where writes is always to master and read can be served either by master or slave.
- So,
	- master server should be setup first and then the slaves.
	- once the slave is deployed, perform the initial data load from the master to slave-1.
	- enable continous replication from master to slave-1, so the db on slave-1 is in sync with master.
	- we then do the same for other slave servers. but load from master will impact the resource on master, soecially the network interface. since the data is available on slave-1, its better to copy from slave-1
	- so wait for slave-1 to be ready.
	- clone data from slave-1 to slave-2
	- enable continous replication from master to slave-2, so the db on slave-2 is in sync with master.
	- master address is to be configured on both the slaves.
	
- Now in kubernetes world, each of these instances of master and slave are POD part of a deployment hence can easily scale it up/down as required.

with deployment we have 2 issues:
- but with deployment we dont have a order i.e. all pod come up part of the deployment at the same time. 
- master pod should have a dedicated constant identifier or address that does not change which these slave nodes will use. can rely on IP address in k8s world, but deployment create pod with random names that can't help.

stateful sets:
- similar to deployments as it create pod based on template, scale up/down, perfom rolling updates or rollback. 
- PODs are created  seqetial order, after the 1st pod is deployed, it must be in a running/ready state before the next pod is deployed. helps in deploying master first and other slaves in order.
- stateful sets assigns unique identifier to each pod, a numver starting from 0 for the first pod and increment by 1, each pod gets a unique name derived from this index combined with stateful sets name. ex: mysql-0, mysql-1, mysql-3, no more random names hence ensure pod names is always same.
- can use the master pod name to all the slaves. i.e mysql-0, even if any pod fails and on restart, the name of the pod remains same. stateful sets maintain a sticky identity to each of its pods.




Stateful Sets Introduction
-------------------------------------------------------------------------
- we might not need stateful set always as it depends on application type, if the instance need to comeup in particualr order or it need a constant name to its pods.
- stateful set manifest is exactly same as deployment.

statefulset-definition.yaml
-----------------------------
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql

	serviceName: mysql-h
	
	podManagementPolicy: parallel  // this attribute to override the default nature and deploy all pods in parallel.
	
	
- serviceName needs to be specified. i.e. the headless service name.

$ kubectl create -f statefulset-definition.yaml

- it creates each replica of this pod one after another. i.e ordered, graceful deployment.
- each pod gets a stable unique DNS record on the network that any other application can use to access a pod.

- when we scaled stateful set, each pod comesup becomes ready and only then the next one comes up.
$ kubectl scale statefulset mysql --replicas=5


- it works in reverse order when we scale it down, i.e. the last pod is removed first followed by the 2nd last one.
$ kubectl scale statefulset mysql --replicas=4


- the same is true during pod termination. when we delete stateful state, the pods are deleted in reverse order.
$ kubectl delete statefulset mysql

- these are the default behavior of statefulsets but we can override to not follow the ordered launch but still have the other benefits sch as stable and unique network id. 

podManagementPolicy: parallel to deploy all pods in parallel.




Headless Services
---------------------------------------------------------------
- when we crate a StatefulSet, it deploys one pod at a time, each pod gets an ordinal index and each pod has a stable unique name (ex: mysql-0, mysql-1 and mysql-2), so we can point slaves to reach the master as mysql-0

- Regular service: the way we point an application to another application withing the cluster is through a service. ex: if we have a webserver then to make a db server acsessible, we create a service for the mysql db. this service will now acts like a load balancer, the trafic in to the service is balanced across all the pods (mysql-0, mysql-1, mysql-2) in the deployment.
	- service has a clusterIP and a DNS name (mysql.default.svc.cluster.local i.e. <service_name>.<ns_name>.<svc>.cluster.local)
	- webserver running in same env, use this dns name to reach mysql db.
	- BUT since this a master-slave topology. the reads could be serverd by both master or slaves BUT, writes has to be by master only. hence read request can still use the mysql clusterIP service but the write requests can't.
	
- ways to reach a single pod of a deployment:
	- pod IP (ex: 10.40.2.8) but IP address are dynamic can change if the pod gets recreated.
	- each pod can be reached to its DNS address but the POD's dns is create using its IP address like: 10-40-2-8.default.pod.cluster.local
	
- Headless Service:
	- does not load balance the request but creates individual DNS entries for each pod the service is pointing to, using the pod name and the sub domain.
	- its created like a normal service but it does not have an IP address on its own like a clusterIP for normal service.
	- on creating a headless service named: mysql-h, each pod gets an DNS name in the format of: <podname>.<headless-servicename>.<namespace>.svc.<cluster-domain>
	
	ex:
	mysql-0.mysql-h.default.svc.cluster.local
	mysql-1.mysql-h.default.svc.cluster.local
	mysql-2.mysql-h.default.svc.cluster.local
	
	- the web app can now use the dns entry for the master pod (mysql-0.mysql-h.default.svc.cluster.local), this dns entry will always point to the master pod of mysql deployment.
	
	- create headless service by explicitly specifying "None" for the cluster IP (.spec.clusterIP).
	
headless-service.yaml
---------------------
apiVersion: v1
kind: Service
metadata:
  name: mysql-h
spec:
  type: None
  
  ports:
      # By default the `targetPort` is set to the same value as the `port` field.
    - port: 3306
	
  selector:
    app: mysql
	

- Now the DNS entries are created for the pod only if the 2 conditions are met. under the spec section, 2 optional field, subdomain and hostname
	- on adding the subdomain value to the headless service name, it creates a a DNS record (mysql-h.default.svc.cluster.local) for the name of the service to point to the pod.
	- how its still does not create A record for invidual pods. giving a name to hostname field in pod manifest, it creates DNS record with the Pod name (mysql-pod.mysql-h.default.svc.cluster.local)
	
pod-definition.yaml
----------------------
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: mysql
spec:
  containers:
  - image: mysql
    name: mysql
	
  subdomain: mysql-h  // MUST be the same name of headless service
  hostname: mysql-pod // any name, gets prefixed on pod DNS name created by the headless service.
  

- Now when we deploy pod using deployment, by default it does not add a hostname/subdomain to the pod if hostname or subdomain is not defined, so the headless service too does not create A record for the pods.

- in we specify like below, then it assigns the same subdomain and hostna e to all the pods as deployment simply duplicates all the pod properties for the same pod.

deployment-definition.yaml
--------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deployment
  labels:
    app: mysql
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql
        ports:
        - containerPort: 3306
	  
	  hostname: mysql-pod
	  subdomain: mysql-h
	 

- the above deploymet will create the below same DNS A-records for 3 of its pods. so this does not help in addressing each pod individually. that is where StatefulSet differs from a Deployment.

mysql-pod.mysql-h.default.svc.cluster.local
mysql-pod.mysql-h.default.svc.cluster.local
mysql-pod.mysql-h.default.svc.cluster.local

  
- in StatefulSet, we dont need to mention subdomain/hostname, instead add the headless service name in StatefulSet definition (under spec section, ex: serviceName: mysql-h) and it automatically assigns the right hostname for each pod based on the pod name and also assigns the subdomain name based on the headless service name.
	
  
statefulset-definition.yaml
----------------------------
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-statefulset
  labels:
    app: mysql
spec:

  serviceName: mysql-h

  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql
        ports:
        - containerPort: 3306


- all pods now get a separate DNS record.

mysql-pod.mysql-h.default.svc.cluster.local
mysql-pod.mysql-h.default.svc.cluster.local
mysql-pod.mysql-h.default.svc.cluster.local	




Storage in StatefulSets
----------------------------------------
- Static Provisoning: with Persistem Volumes, we create volume objects in kubernetes which are then claimed by PVC and finally used in pod definition file to use the storage in pod. this is single PV mapped to an single PVC to a single POD

- Dynamic provisioning: With StorageClass definition, we take out the manula creation of PV, and use the Storage provisioners to automatically provision volume on cloud providers, now the PV is created automatically, will still create a PVC manually and associate to the POD.


- How does these above behaviors work with StatefulSets, when we specify the same PVC under the pod spec on StatefulSet manifest, all pods created by that StatefulSet, tries to use the same volume, thats okay if the application design is fine with it, as like we want multiple pod to share and access the same storage and that also depends on the ind of volume created and the provisioner used, Note: NOT all storage types support these type of operation of RW access by multiple instance at the same time.

- If we want separate volume for each pod of StatefulSet as in case of mysql replication usecase, the PODs dont want to share the data instead each POD needs its own local storage, each instance has its own DB and replication of data between the Dbs is done at mysql level, so then each POD needs its own PVC bound to a individual PV. and ofcourse these PVs can be created from single StorageClass or different StorageClass.

- How do we automatically create separate PVC in Statedulset.
	- VolumeClaimTemplate: is nothing but a PVC manifest. i.e. instead of creating the PVC manually then specifying it inside StatefulSet manifest, we move the PVC manifest into a section called "volumeClaimTemplates", its an array, we can specify multiple templates.
	

sc-definition.yaml
-------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd


pvc-definition.yaml (dont create this - but add inside statefulset as a template)
----------------------------------------------------------------------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-volume
spec:
  accessMode: 
  - ReadWriteOnce
  storageClassName: google-storage
  resource:
    requests:
	  storage: 500Mi

statefulset-definition.yaml
-----------------------------
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql
	serviceName: mysql-h	
	
	volumeClaimTemplates:
	-  metadata:
		  name: data-volume
	   spec:
	     accessMode: 
		 - ReadWriteOnce
		 storageClassName: google-storage
		   resource:
			  requests:
			    storage: 500Mi




- it creates the first POD, during creation, a PVC is created, PVC is associated to StorageClass, so the StorageClass provision a volume on GCP, creates a PV automatically which gets bound to PVC
	- then the 2nd POD gets created, it create a PVC, the StorageClass provisions a new volume, associates that to a PV and bind the PV to PVC
	
- Incase any of these POD gets recreated or reschedules on a separate node, StatedulSet DO NOT automatically delete the PVC or the associated volume, instead ensures, the POD is re-attached to the same PVC that it was attached to before. thus StatefulSets ensures, stable storage for PODs.










===============================================================================================
Updates for Sep 2021 Changes
===============================================================================================


Docker Image Builds
===============================================================================================
docker images
-------------
- list all the images and its details pulled on local docker host.

$ docker images
REPOSITORY                      TAG                 IMAGE ID            CREATED             SIZE
redis                           latest              ccee4cdf984f        8 months ago        105MB
ubuntu                          latest              7e0aa2d69a15        9 months ago        72.7MB
mysql                           latest              0627ec6901db        9 months ago        556MB
nginx                           alpine              a64a6e03b055        9 months ago        22.6MB
alpine                          latest              6dbb9cc54074        9 months ago        5.61MB
nginx                           latest              62d49f9bab67        9 months ago        133MB
postgres                        latest              26c8bcd8b719        9 months ago        314MB
kodekloud/simple-webapp-mysql   latest              129dd9f67367        3 years ago         96.6MB
kodekloud/simple-webapp         latest              c6e3cd9aae36        3 years ago         84.8MB



sample Docker file:
---------------------------
FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]


symtax to build the image locally:

docker build -t <image_name>:<tag_name>   // tag_name is optional. if not given, the value will be latest

$ docker build -t webapp-color .

Sending build context to Docker daemon  121.3kB
Step 1/6 : FROM python:3.6
3.6: Pulling from library/python
0e29546d541c: Pull complete 
9b829c73b52b: Pull complete 
cb5b7ae36172: Pull complete 
6494e4811622: Pull complete 
6f9f74896dfa: Pull complete 
5e3b1213efc5: Pull complete 
9fddfdc56334: Pull complete 
404f02044bac: Pull complete 
c4f42be2be53: Pull complete 
Digest: sha256:f8652afaf88c25f0d22354d547d892591067aa4026a7fa9a6819df9f300af6fc
Status: Downloaded newer image for python:3.6
 ---> 54260638d07c
Step 2/6 : RUN pip install flask
 ---> Running in 37ce7302e20d
Collecting flask
  Downloading Flask-2.0.2-py3-none-any.whl (95 kB)
Collecting Jinja2>=3.0
  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)
Collecting itsdangerous>=2.0
  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)
Collecting Werkzeug>=2.0
  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)
Collecting click>=7.1.2
  Downloading click-8.0.3-py3-none-any.whl (97 kB)
Collecting importlib-metadata
  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)
Collecting MarkupSafe>=2.0
  Downloading MarkupSafe-2.0.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)
Collecting dataclasses
  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)
Collecting typing-extensions>=3.6.4
  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)
Collecting zipp>=0.5
  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)
Installing collected packages: zipp, typing-extensions, MarkupSafe, importlib-metadata, dataclasses, Werkzeug, Jinja2, itsdangerous, click, flask
Successfully installed Jinja2-3.0.3 MarkupSafe-2.0.1 Werkzeug-2.0.2 click-8.0.3 dataclasses-0.8 flask-2.0.2 importlib-metadata-4.8.3 itsdangerous-2.0.1 typing-extensions-4.0.1 zipp-3.6.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.
You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.
Removing intermediate container 37ce7302e20d
 ---> 5b842274fd45
Step 3/6 : COPY . /opt/
 ---> 49a72fadf9be
Step 4/6 : EXPOSE 8080
 ---> Running in 4f75c7639e27
Removing intermediate container 4f75c7639e27
 ---> 1c427e70bea9
Step 5/6 : WORKDIR /opt
 ---> Running in 7879a13507d9
Removing intermediate container 7879a13507d9
 ---> 9df238646257
Step 6/6 : ENTRYPOINT ["python", "app.py"]
 ---> Running in 368ac8315aec
Removing intermediate container 368ac8315aec
 ---> daa92f4c3b7c
Successfully built daa92f4c3b7c
Successfully tagged webapp-color:latest



- Run an instance of the image webapp-color and publish port 8080 on the container to 8282 on the host.
$ docker run -p 8282:8080 webapp-color

 This is a sample web application that displays a colored background. 
 A color can be specified in two ways. 

 1. As a command line argument with --color as the argument. Accepts one of red,green,blue,blue2,pink,darkblue 
 2. As an Environment variable APP_COLOR. Accepts one of red,green,blue,blue2,pink,darkblue 
 3. If none of the above then a random color is picked from the above list. 
 Note: Command line argument precedes over environment variable.


No command line argument or environment variable. Picking a Random Color =red
 * Serving Flask app 'app' (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
 * Running on http://172.12.0.2:8080/ (Press CTRL+C to quit)



- What is the base Operating System used by the python:3.6 image?

$ docker run python:3.6 cat /etc/*release*

PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"


- shorten the size of image:

FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]


$ docker build -t webapp-color:lite .

Sending build context to Docker daemon  122.4kB
Step 1/6 : FROM python:3.6-alpine
3.6-alpine: Pulling from library/python
59bf1c3509f3: Pull complete 
8786870f2876: Pull complete 
acb0e804800e: Pull complete 
52bedcb3e853: Pull complete 
b064415ed3d7: Pull complete 
Digest: sha256:579978dec4602646fe1262f02b96371779bfb0294e92c91392707fa999c0c989
Status: Downloaded newer image for python:3.6-alpine
 ---> 3a9e80fa4606
Step 2/6 : RUN pip install flask
 ---> Running in bbd914fe5835
Collecting flask
  Downloading Flask-2.0.2-py3-none-any.whl (95 kB)
Collecting click>=7.1.2
  Downloading click-8.0.3-py3-none-any.whl (97 kB)
Collecting Jinja2>=3.0
  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)
Collecting Werkzeug>=2.0
  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)
Collecting itsdangerous>=2.0
  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)
Collecting importlib-metadata
  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)
Collecting MarkupSafe>=2.0
  Downloading MarkupSafe-2.0.1-cp36-cp36m-musllinux_1_1_x86_64.whl (29 kB)
Collecting dataclasses
  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)
Collecting typing-extensions>=3.6.4
  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)
Collecting zipp>=0.5
  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)
Installing collected packages: zipp, typing-extensions, MarkupSafe, importlib-metadata, dataclasses, Werkzeug, Jinja2, itsdangerous, click, flask
Successfully installed Jinja2-3.0.3 MarkupSafe-2.0.1 Werkzeug-2.0.2 click-8.0.3 dataclasses-0.8 flask-2.0.2 importlib-metadata-4.8.3 itsdangerous-2.0.1 typing-extensions-4.0.1 zipp-3.6.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.
You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.
Removing intermediate container bbd914fe5835
 ---> 49b436b124ee
Step 3/6 : COPY . /opt/
 ---> 9c9c5105ae3f
Step 4/6 : EXPOSE 8080
 ---> Running in 15e065ec3c8b
Removing intermediate container 15e065ec3c8b
 ---> de4c06e91938
Step 5/6 : WORKDIR /opt
 ---> Running in c51f6bbdede9
Removing intermediate container c51f6bbdede9
 ---> ec70be770823
Step 6/6 : ENTRYPOINT ["python", "app.py"]
 ---> Running in 4c99bfc638a3
Removing intermediate container 4c99bfc638a3
 ---> 2ce50c0feb06
Successfully built 2ce50c0feb06
Successfully tagged webapp-color:lite



$ docker images

REPOSITORY                      TAG                 IMAGE ID            CREATED             SIZE
webapp-color                    lite                2ce50c0feb06        2 minutes ago       51.8MB
webapp-color                    latest              daa92f4c3b7c        20 minutes ago      913MB
python                          3.6                 54260638d07c        4 weeks ago         902MB
python                          3.6-alpine          3a9e80fa4606        7 weeks ago         40.7MB
redis                           latest              ccee4cdf984f        8 months ago        105MB
ubuntu                          latest              7e0aa2d69a15        9 months ago        72.7MB
mysql                           latest              0627ec6901db        9 months ago        556MB
nginx                           alpine              a64a6e03b055        9 months ago        22.6MB
alpine                          latest              6dbb9cc54074        9 months ago        5.61MB
nginx                           latest              62d49f9bab67        9 months ago        133MB
postgres                        latest              26c8bcd8b719        9 months ago        314MB
nginx                           1.14-alpine         8a2fb25a19f5        2 years ago         16MB
kodekloud/simple-webapp-mysql   latest              129dd9f67367        3 years ago         96.6MB
kodekloud/simple-webapp         latest              c6e3cd9aae36        3 years ago         84.8MB



- Run an instance of the new image webapp-color:lite and publish port 8080 on the container to 8383 on the host.

$ docker run -d -p 8383:8080 webapp-color:lite
abb5590f3e6246bddf0a920a68ac9074a19a0940249356dcc1ca74edf8ce8ca3


$ docker ps

CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                    NAMES
abb5590f3e62        webapp-color:lite   "python app.py"     27 seconds ago      Up 25 seconds       0.0.0.0:8383->8080/tcp   interesting_turing





Authentication, Authorization and Admission Control
=======================================================================================================

- Kubernetes becoming the goto platform for hosting production grade application, securing the kubernetes cluster is very important.

- There various components in the cluster, where the security should be implemented.

1/ Securing the hosts/nodes of the cluster.
	- access to these hosts must be secured.
	- root access should be disabled.
	- password based authN disabled and only SSH based authN is allowed.
	- all other measure to secure the physical or virtual infra that host kubernetes.
	

2/ Secure Kubernetes (kube apiserver)
	- what are the risks and what measures need to take to secure the cluster.
	- kube-apiserver is the center of all operation in kubernetes cluster. we interratct with it bu kubectl or accessing the API directly and we can perform almost any operations on the cluster.
	- controlling access to the kube-apiserver will be the 1st line of defence.
	

Authentication:
- who can access the cluster. different ways to authN to apiserver.
	- username and passwords stored in static files.
	- username and tokens stored in static files.
	- certificates
	- integration with external auth providers like LDAP
	- service accounts for machines.

Authorization:
- once a user get access to the cluster, AuthZ defines what can the user do?
	- RBAC - Role Based Access Control
	- ABAC - Attribute Based Access Control
	- Node Authz
	- Webhook Mode
	
- all communication between the cluster components such as etcd storage, agent runing on worker nodes i.e. kubelet, kube proxy, kube scheduler, kube controller manager, Kube apiserver is secured using TLS encryption certificates.

3/ secure communication between applications running within cluster
	- by default all pods can access others within the cluster. can restrict access between them using network policies.
	



Authentication in Kubernetes cluster
===================================================================================
- There are 4 types of users who access the kubernetes cluster.
	- Admins: access clsuter to perform admin tasks.
	- Developers: to test or deploy application
	- End User: access apps running on the cluster by accessing the hosts. security of end user access is controlled by the apps logic. 
	- Bots (system user): 3rd party apps accessing cluster for integration purpose.
	
- But its only 2 categories of users.
	- Human (Admins and Developers)
	- System (processess, 3rd part apps, bots etc)
	

- Kubernetes does not manage users or user accounts natively. it relies on external source like a static file with user details or certificates, or 3rd part identity service like LDAP to manage these users.

- So we can not create users like "kubectl create user user_1" or view the users "kubectl list users". it does not work.

- But Kubernets manage service account user. "kubectl create serviceaccount sa1" or "kubectl get serviceaccount"

- all users access to cluster via kubectl or direct API access (curl https://<kuber-master>/6443/api) is managed by the kube-apiserver. The kube-apiserver authenticates the request before processing it.



Authentication mechanism:
---------------------------------------
- list of username and password in a static password file: ex: user_pass.csv, last column is optional to specify the group of the user
	
user_pass.csv
password123,username1,userid1,group1
password123,username1,userid1,group1
password123,username1,userid1,group2
password123,username1,userid1,group2

then pass this file name while setting up the kube-apiserver (--basic-auth-file=user_pass.csv)	
	

- list of usernames and tokens in a static token file: ex: user_token.csv, last column is optional to specify the group.
	- pass the file during starting kube-apiserver (--token-auth-file=user_token.csv)	
	
user_token.csv
hkda98d7nd2898du2kd0jd2d2md9d2hfjf,username1,userid1,group1
hkda98d7nd2898du2kd0jd2d2md9d2hfjf,username1,userid1,group1
hkda98d7nd2898du2kd0jd2d2md9d2hfjf,username1,userid1,group1
	
- certificates
	
- Identity service: connect 3rd part service like LDAP and authN user.
	


Authenticate User
-------------------
- to authenticate using the basic credentials (i.e. username and its password) while accessing the api server, specify the user and password in a curl command (-u "username1:password1")

curl -v -k https://<kube-master-node-ip>:6443/api/v1/pods -u "username1:password1"


- to authenticate using tokens:

curl -v -k https://<kube-master-node-ip>:6443/api/v1/pods --header "Authorization: Bearer hkda98d7nd2898du2kd0jd2d2md9d2hfjf"

- authN using certificates:

curl https://<kube-master-node-ip>:6443/api/v1/pods \
	--key admin.key
	--cert admin.crt
	--cacert ca.crt




Security - KubeConfig
==============================================================================
- so far we have seen, user access kubernets using direct kube API via certificate authN:

ex: 
curl command to list all pods using REST API:

$ curl https://<kube-master>:6443/api/v1/pods \
	--key admin.key
	--cert admin.crt
	--cacert ca.crt
	
- this request is then validated by kube-apiserver to authenticate the user.

- How authN works while accessing cluster via kubectl command.

> kubectl get pods
	--server kube-master:6443
	--client-key admin.key
	--client-certificate admin.crt
	--certificate-authority ca.crt
	
- but typing these info/creds everytime is tedious job. so we move these creds into a config file called KubeConfig ($HOME/.kube/config) and then specify that file as --kubeconfig if the name of file is other than "config"

> kubectl get pods --kubeconfig <custom_config_file_name>   // just the name of the file if the file is inside ($HOME/.kube/config) else the full path.

ex:
kubectl get pods --kubeconfig /root/my-kube-config

Or

kubectl get pods --kubeconfig my-kube-config     // if my-kube-config is kept in $HOME/.kube/



- kubectl tool look for a file config under user's $HOME/.kube/config, bydefault it refers to this config file hence we dont have to specify --kubeconfig option explicitly in kubectl command.



KubeConfig file structure ($HOME/.kube/config)
------------------------------------------------
- the file has 3 sections and its an yaml file. each of these are accept array to specify multiple clusters/users/contexts
	
	- Clusters: various k8s cluster that we need access to. i.e. "--server kube-master:6443" goes this section.
	
	- Users: various user (admin user, dev/prd user) accounts with which we have access to these various clusters. these users have different privileges on different clusters. i.e. "--client-key admin.key", "--client-certoficate admin.crt" goes in this section.
		
	- Contexts: context maps Clusters to its Users. i.e. which user account to be used to access which cluster. ex: create a context named admin@Production to map admin user accessing Production cluster.
	i.e.
	
	
sample config file:
-------------------
apiVersion: v1
kind: Config

current-context: dev-user@gcp   // optional. this get set using kubectl use-context command.

clusters:

- name: my-kube-master
  cluster: 
    certificate-authority: ca.crt
	server:  https://my-kube-master:6443

- name: development
- name: production
- name: gcp
..
..

contexts:

- name: my-kube-admin@my-kube-master
  context:
    cluster: my-kube-master    // name of the cluster from cluster section
	user: my-kube-admin		  // name of the user from users section

- name: dev-user@gcp
- name: prd-user@production
- name: admin-user@development

users:

- name: my-kube-admin
  user:
    client-certificate: admin.crt
	client-key: admin.key

- name: admin-user
- name: dev-user
- name: prd-user
- name: gcp-user
..
..


*** NOTE: we dont create object using this file with kubectl tool. the file is read by kubectl automatically.


- Now how kubectl know which context to use out of many, add the field: "current-context: dev-user@gcp" to config file.


kubectl command to view/edit KubeConfig file
--------------------------------------------------

- to view file, bydefault it looks in $HOME/.kube/config file.

> kubectl config view


- to specify a different location

> kubectl config view --kubeconfig=my-custom-config


- to add cluster details to your configuration file
> kubectl config set-cluster development --server=https://1.2.3.4 --certificate-authority=fake-ca-file
> kubectl config --kubeconfig=config-demo set-cluster scratch --server=https://5.6.7.8 --insecure-skip-tls-verify


- Add user details to your configuration file:

> kubectl config set-credentials developer --client-certificate=fake-cert-file --client-key=fake-key-seefile
> kubectl config --kubeconfig=config-demo set-credentials experimenter --username=exp --password=some-password


- Add context details to your configuration file:

> kubectl config set-context development@frontend --cluster=development --namespace=frontend --user=developer
> kubectl config set-context development@storage --cluster=development --namespace=storage --user=developer
> kubectl config --kubeconfig=config-demo set-context experimenter@scratch --cluster=scratch --namespace=default --user=experimenter


- Use a specific context

>  kubectl config use-context development@frontend


- In a particular context there could be multiple namaspaces, it is possible to add any specific namespace along with the context so that when we use that context, all kubectl command will query on that specified namespace only.

contexts:
- name: my-kube-admin@my-kube-master
  context:
    cluster: my-kube-master    // name of the cluster from cluster section
	user: my-kube-admin		  // name of the user from users section
	namespace: finance        // all kubectl command will query on this namespace only.
	

- to know the current context:
$ kubectl config --kubeconfig=my-kube-config current-context 
dev-user@test-cluster-1

$ kubectl config --kubeconfig=/root/my-kube-config current-context  // if the custom-config file is different location.



Certificates in KubeConfig
-------------------------------
- we add the certificate for a given cluster like below:

clusters:
- name: my-kube-master
  cluster: 
    certificate-authority: ca.crt    // name of the certificate. default path: /etc/kubernetes/pki/
	server:  https://my-kube-master:6443


- we should give the complete path:

clusters:
- name: my-kube-master
  cluster: 
    certificate-authority: /etc/kubernetes/pki/ca.crt


- Now the usual content of any certificate will be something like below:

------ BEGIN CERTIFICATE --------
kkdwkjkljw982klf93lkd9ldncs9ffwhj8910373bdjagfri
nbfgjd729nkdhka8203fkf82092u9dkfaewtflf00275n0nd
nbfgjd729nkdhka8203fkf82092u9dkfaewtflf00275n0nd
nbfgjd729nkdhka8203fkf82092u9dkfaewtflf00275n0nd
nbfgjd729nkdhka8203fkf82092u9dkfaewtflf00275n0nd
nbfgjd729nkdhka8203fkf8==
------ END CERTIFICATE --------

- another way to add certificate is directly add the certificate content instead of path, but first convert the certificate content to base64 format.

> cat ca.crt | base64
LHGUFJJ&88992LGHFFHKK
LHGUFJJ&88992LGHFFHKK
LHGUFJJ&88992LGHFFHKK
LHGUFJJ&88992LGHFFHKK


clusters:
- name: my-kube-master
  cluster: 
    certificate-authority-data: LHGUFJJ&88992LGHFFHKK
								LHGUFJJ&88992LGHFFHKK
								LHGUFJJ&88992LGHFFHKK
								LHGUFJJ&88992LGHFFHKK
								

Practice KubeConfig
-----------------------------
- sample config file: /root/.kube/config

apiVersion: v1
kind: Config
preferences: {}
current-context: kubernetes-admin@kubernetes
clusters:
- cluster:
    server: https://controlplane:6443
    certificate-authority-data:		LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQ
						URBTkJnaNkSG05K2RLdXdWVlRiSW1RQ2pWNjB3WUd6bklzT0VuQ1lKZ0t0Rjc5bEVYaFIwCjF2VnF
  name: kubernetes
  
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes

users:
- name: kubernetes-admin
  user:
    client-certificate-data: 	LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURFekNDQWZ1Z0F3SUJBZ0lJUjZO
				UZ3MHlNekF4TWpBeE1EUTFNalZhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd
    client-key-data:	LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcEFJQkFBS0NBUUVBNWRmREZreEVl
				ldZbWVYN2x3TkExcEpjbVhBVWV3NjM0TGtJRXRkClJ5dnBVWE96K1dhOVg4TG9TQnh0RXdnMUE2NWhXcElte




- sample custom kubeconfig:

$ pwd
/root/.kube
$ ls -ltr
total 16
-rw------- 1 root root 5568 Jan 20 10:45 config
drwxr-x--- 4 root root 4096 Jan 20 10:49 cache
-rw-r--r-- 1 root root 1456 Jan 20 11:04 my-kube-config

my-kube-config
--------------
apiVersion: v1
kind: Config
current-context: test-user@development
preferences: {}

clusters:
- name: production
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: development
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: kubernetes-on-aws
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: test-cluster-1
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

contexts:
- name: test-user@development
  context:
    cluster: development
    user: test-user

- name: aws-user@kubernetes-on-aws
  context:
    cluster: kubernetes-on-aws
    user: aws-user

- name: test-user@production
  context:
    cluster: production
    user: test-user

- name: research
  context:
    cluster: test-cluster-1
    user: dev-user

users:
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key
- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key
- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key


-- use the dev-user to access test-cluster-1. Set the current context to the right one

$ kubectl config --kubeconfig=my-kube-config set-context dev-user@test-cluster-1 --cluster=test-cluster-1 --user=dev-user
Context "dev-user@test-cluster-1" created.

- context:
    cluster: test-cluster-1
    user: dev-user
  name: dev-user@test-cluster-1


$ kubectl config --kubeconfig=my-kube-config use-context dev-user@test-cluster-1 
Switched to context "dev-user@test-cluster-1".


- to know the current context
$ kubectl config --kubeconfig=my-kube-config current-context 
dev-user@test-cluster-1




Authorization
==================================================================
- once a user human (admins/developers) or system (bots via service accounts) users gain access to the kubernetes cluster, controlling what they can do is managed by AuthZ.

- Admin user have super access to the cluster like view objcets, create objects and even adding/delete nodes to the cluster. but there are other users like developres, testers, monitorring 3rd party apps.

- But the developer user should not have rights for modifying the clusters like adding/deleting nodes etc, or storage or networking configs. we an allow them to view but not modify.

- similary, for service account, very minimal level of access to perform specific operation like viewing certain type of cluster metrics or healths stats and not viewing any other objects like pods,services,deployments etc.

- AuthZ cal also help in applying restriction rules for certain user to have access to only their namepsace and not any other namespace used by other teams in a shared cluster.


AuthZ Mechanism
---------------------------------------
- Node AuthZ:

	- kube-apiserver is accessed by users (admins/devs) as well as by kubelets agents running on worker nodes within the cluster. kubelets access the api-server to read/write info like:
	read: Services, Endpoints, Nodes, Pods
	write: Node status, Pod status, events
	
	- these access request are handled by a special authrozer knwon as "Node Authorizer".
	
	- kubelets are part of system node group and have names prefixed with system-node*, so any access request coming from a user with a name starting with system-node and part of system-node group is authorized by this type of authZ and granted specific privileges required by the kubelets.
	

- ABAC (Attribute Based Access Control)

	- associate a user or a group of users with a set of permissions. ex: we say a dev-user can view/create/delete pod. we do this by creating a policy file with a set of policies defined in JSON and pass this file to apiserver.
	
	{ "kind": "Policy", "spec": { "user": "dev-user", "namespace": "", "resource": "pods", "apiGroup": "*" } }
	
	- similary we can create this type of policy definition file for each group or user.
	
	- everytime we need to add or make a change in the policy, we must edit these policy file manually and restart kube-apiserver which is very tedious.


- RBAC (Role Based Access Control)
	- with RBAC, instead of directly associating the users or the group to the set of permissions, we define a role i.e. create a role ex: developer with a set of privileges (view/create/delete pod) required for developers, then provision that role to all the developer type users in the cluster. Same for security user, Security Role (view/approve CSR).
	
	- any change needed on the privileges, is done at Role level and same reflects to all its associated users immediately.


- Webhook
	- to integrate with 3rd party identity providers. If we want to out-source these whole authZ mechanism to any external service and not use any of these above built-in k8s solution. ex: Open Policy Agent is a 3rd part tool that helps in admission control and authZ, we can have kube-apiserver make an external api call to this agent to know if a given user access/operation can be granted. based on agent's response, kube-apiserver allow the operation.
	


AuthZ Mode
---------------------------------------
- in addition to the above 4 authZ types (Node, ABAC, RBAC, Webhook) there are 2 additional authZ modes/types.

- AlwaysAllow: allows all request without performing any authZ checks.
- AlwaysDeny: Denies all request.

- The type of AuthZ mechanism is set using the "--authorization-mode" config field to kube-apiserver. Its "AlwaysAllow" bydefault if we dont pass this config field (--authorization-mode=AlwaysAllow)

ExecStart=/usr/local/bin/kube-apiserver \\
--advertise-address=${INTERNAL_IP} \\
--allow-privileged=true \\
--apiserver-count=3 \\
--authorization-mode=Node,RBAC,Webhook \\

- we may provide multiple types comma separated, that we wish to use.

- when we have multiple authZ mode/type configured, any access request is passed through each of these modes in the order it is specified. 

USER --->	NODE Authz	--->	RBAC Authz	--->	Webhook Authz

ex: when a dev user send a access request, in this case it gets first handled by Node AuthZ which handles only node request, hence it denies. whenevr a module denies a request, the same request is forwared to the next module in the chain, here RBAC performs its checks and grants the access to dev-user. authZ is complete and the user is given access to the requtes operation. 

- ** every time a module denies a request, it goes to the next one in the chain and as soon as any module approves the request, no more checks are done and the user is granted permission.


- Check the authorization modes configured on the cluster. (look for --authorization-mode=Node,RBAC)

$ kubectl describe pod kube-apiserver-controlplane -n kube-system

Name:                 kube-apiserver-controlplane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 controlplane/10.51.138.6
Start Time:           Thu, 20 Jan 2022 18:43:53 +0000
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.51.138.6:6443
                      kubernetes.io/config.hash: c89ea5e7cca5b55e748e8e4ddd432c41
                      kubernetes.io/config.mirror: c89ea5e7cca5b55e748e8e4ddd432c41
                      kubernetes.io/config.seen: 2022-01-20T18:43:50.959500112Z
                      kubernetes.io/config.source: file
Status:               Running
IP:                   10.51.138.6
IPs:
  IP:           10.51.138.6
Controlled By:  Node/controlplane
Containers:
  kube-apiserver:
    Container ID:  docker://0c3d7f5f50f173190708eec67252945f88f0ee4d52e30946e51bcd170691b26c
    Image:         k8s.gcr.io/kube-apiserver:v1.20.0
    Image ID:      docker-pullable://k8s.gcr.io/kube-apiserver@sha256:8b8125d7a6e4225b08f04f65ca947b27d0cc86380bf09fab890cc80408230114
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=10.51.138.6
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
	  --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --insecure-port=0
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/12
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Thu, 20 Jan 2022 18:43:35 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://10.51.138.6:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://10.51.138.6:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://10.51.138.6:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>



Role Based Access Controls
=========================================================================
https://kubernetes.io/docs/reference/access-authn-authz/rbac/

Role and ClusterRole object
----------------------------
- An RBAC Role or ClusterRole contains rules that represent a set of permissions.
- Role: A Role always sets permissions within a particular namespace; when you create a Role, you have to specify the namespace it belongs in.
- ClusterRole: ClusterRole, by contrast, is a non-namespaced resource. The resources have different names (Role and ClusterRole) because a Kubernetes object always has to be either namespaced or not namespaced; it can't be both.

- Role and RoleBinding (to asscociate user to Role) are namespaced i.e. they are created on a specific namespace. if namespace is not provided it will created in default namespace. Hence it controls access within that namespace alone.

- Role and RoleBinding (to asscociate user to Role) are used to authorize namespaced resources.

- various resources/objects in kubernetes are categorized either to Namespaced (pod, service, deployment, role, rolebinding etc) OR to Cluster scoped (ex: node can not belong to a namespace)

- namespaced resources are always created in a given namespace, if not provided, gets created in default namespace.

- to list all the namespaces resources:

$ kubectl api-resources --namespaced=true



- cluster scoped resources are those where we dont specify namespace like nodes, PV, clusterrole, clusterrolebindings, namespaces etc.

- to list all the cluster scoped resources:

$ kubectl api-resources --namespaced=false


STEP-1
- a Role is created using Role object.
- can add multiple rule for each type of resources under a single role.

developer-role.yaml
--------------------
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: developer-role
rules:
- apiGroups: [""] 				// "" indicates the core API group, any other group, specify the group name.
  resources: ["pods"]
  verbs: ["get", "watch", "list", "create", "delete", "update"]
- apiGroups: [""] 				// "" indicates the core API group, any other group, specify the group name.
  resources: ["configmaps"]
  verbs: ["create"] 


- same can also be created like: 
$ kubectl create role developer-role --namespace=default --verb=list,create,delete --resource=pods

** NOTE: within a given a resource, if we want to restrict to only some specific pod by its name, add "resourceNames" field.
ex:

rules:
- apiGroups: [""] 				// "" indicates the core API group, any other group, specify the group name.
  resources: ["pods"]
  verbs: ["create", "delete"]
  resourceNames: ["pob_name_blue", "pod_name_green"]  // give access for create/delete on 2 pods only named: pob_name_blue and pod_name_green


$ kubectl create -f developer-role.yaml
role.rbac.authorization.k8s.io/developer created

STEP-2
- associate the user to tis role. hence create RoleBinding object.
- multiple user can be bound to single role.

devuser-developer-rolebinding.yaml
----------------------------------
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-rolebinding
  namespace: finance   // specifying namespace, will limit this mapping of devuser to developer-role on specific namespace. if not provided, its default namespace.
subjects:
- kind: User    // it can also be Group for group of user
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer-role # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
  
 
$ kubectl create -f  devuser-developer-rolebinding.yaml
rolebinding.rbac.authorization.k8s.io/dev-user-binding created


- the same can also created like:
$ kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user


View RBAC
-------------
- to view the created roles.

$ kubectl get roles


NAME			AGE
developer-role	4s


$ kubectl get role --all-namespaces

NAMESPACE     NAME                                             CREATED AT
blue          developer                                        2022-01-20T18:48:15Z
kube-public   kubeadm:bootstrap-signer-clusterinfo             2022-01-20T18:43:48Z
kube-public   system:controller:bootstrap-signer               2022-01-20T18:43:47Z
kube-system   extension-apiserver-authentication-reader        2022-01-20T18:43:47Z
kube-system   kube-proxy                                       2022-01-20T18:43:49Z
kube-system   kubeadm:kubelet-config-1.20                      2022-01-20T18:43:47Z
kube-system   kubeadm:nodes-kubeadm-config                     2022-01-20T18:43:47Z
kube-system   system::leader-locking-kube-controller-manager   2022-01-20T18:43:47Z
kube-system   system::leader-locking-kube-scheduler            2022-01-20T18:43:47Z
kube-system   system:controller:bootstrap-signer               2022-01-20T18:43:47Z
kube-system   system:controller:cloud-provider                 2022-01-20T18:43:47Z
kube-system   system:controller:token-cleaner                  2022-01-20T18:43:47Z


- to list rolebindings:

$ kubectl get rolebindings

NAME							AGE
devuser-developer-rolebinding	24s

ex:

$ kubectl get rolebinding -n kube-system 
NAME                                                ROLE                                                  AGE
kube-proxy                                          Role/kube-proxy                                       16m
kubeadm:kubelet-config-1.20                         Role/kubeadm:kubelet-config-1.20                      16m
kubeadm:nodes-kubeadm-config                        Role/kubeadm:nodes-kubeadm-config                     16m
system::extension-apiserver-authentication-reader   Role/extension-apiserver-authentication-reader        16m
system::leader-locking-kube-controller-manager      Role/system::leader-locking-kube-controller-manager   16m
system::leader-locking-kube-scheduler               Role/system::leader-locking-kube-scheduler            16m
system:controller:bootstrap-signer                  Role/system:controller:bootstrap-signer               16m
system:controller:cloud-provider                    Role/system:controller:cloud-provider                 16m
system:controller:token-cleaner                     Role/system:controller:token-cleaner                  16m



- to see the details.

$ kubectl describe role <role_name>

ex:
$ kubectl describe role kube-proxy -n kube-system
Name:         kube-proxy
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources   Non-Resource URLs  Resource Names  Verbs
  ---------   -----------------  --------------  -----
  configmaps  []                 [kube-proxy]    [get]


- to see details about role bindings.

$ kubectl describe rolebinding <role_binding_name>
ex:
kubectl describe rolebinding devuser-developer-rolebinding


$ kubectl describe rolebinding kube-proxy -n kube-system 

Name:         kube-proxy
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  Role
  Name:  kube-proxy
Subjects:
  Kind   Name                                             Namespace
  ----   ----                                             ---------
  Group  system:bootstrappers:kubeadm:default-node-token 



Edit RBAC
-----------
$ kubectl edit role developer -n blue
role.rbac.authorization.k8s.io/developer edited



- Grant the dev-user permissions to create deployments in the blue namespace.
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: blue
  name: deploy-role
rules:
- apiGroups: ["apps", "extensions"]
  resources: ["deployments"]
  verbs: ["create"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-deploy-binding
  namespace: blue
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: deploy-role
  apiGroup: rbac.authorization.k8s.io




Check Access for a given user (auth can-i command)
---------------------------------------------------
- to just check access for the current user for an operaion on given resource.

$ kubectl auth can-i <operation verbs i.e. get/create/delete> <resource_name>

ex:
$ kubectl auth can-i create deployments
yes


$ kubectl auth can-i delete node
no


- in order to test access previleges for any other user other than current logged-in user.
$ kubectl auth can-i create deployments --as dev-user
no

$ kubectl auth can-i create pods --as dev-user
yes


- we can also test the same for a given namespace.

$ kubectl auth can-i create pods --as dev-user --namespace test  // dev-user is not setup to have privileges on test namespace.
no




Cluster Roles
======================================================================================
- we now know about Role and RoleBinding.

- cluster scoped resources are those where we dont specify namespace like nodes, PV, clusterrole,clusterrolebindings, namespaces etc.

- to list all the cluster scoped resources:

$ kubectl api-resources --namespaced=false


- ClusterRole and ClusterRoleBinding (to asscociate user to ClusterRole) are used to authorize cluster scoped resources (like nodes, PV etc.)

- ClusterRole are just like Role except they are for controlling access to Cluster scoped resources.
ex:  
1/ "ClusterAdmin" role can be created to provision a cluster admin user and give permission to view/create/delete node in the cluster across namespace.

2/ "StorageAdmin" role can be created to provision a cluster storage admin user and give permission to view/create/delete PV and PVC in the cluster across namespace.


*** NOTE: ClusterRole can also be used to control access for namespaced scoped resources. BUT then that ClusterRole will be for entire cluster acrosss namespaces.

STEP-1
- create a ClusterRole object.

cluster-admin-role.yaml
-----------------------
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  namespace: default
  name: cluster-administrator
rules:
- apiGroups: [""] 				// "" indicates the core API group, any other group, specify the group name.
  resources: ["nodes"]
  verbs: ["get", "list", "create", "delete"]
  
$ kubectl create -f cluster-admin-role.yaml



STEP-2
- link the cluster admin user to this ClusterRole.

cluster-admin-role-binding.yaml
----------------------------------
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-bind
  namespace: finance   // specifying namespace, will limit this mapping of devuser to developer-role on specific namespace. if not provided, its default namespace.
subjects:
- kind: User
  name: cluster-admin-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-administrator # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io


$ kubectl create -f cluster-admin-role-binding.yaml




Admission Controllers
===================================================================================
- everytime we run a command using kubectl utility, it sends the request (say create a pod), it goes to kube-apiserver and then the pod gets created, and this info is finally persisted in etcd database.

- when the request hits the kube-apiserver, it goes through an Authentication process and this usually done through certificates. if the request is submitted via kubectl utility, it uses the creds/certs defined in $HOME/.kube/config file. The AuthN process is responsible to indentify the request, and makes sure the user is valid.

- then the request goes through an Authorization process, where it checks if the authenticated user has right permission to perform the operation. this is done through Role Based Access Control.


USER	-->		Authentication		--> 		Authorization	-->		Admission Controllers		-->		create pod. 


- as we can see, most of thes rules that we can create using RBAC is at the kubernetes API level, i.e. user is allowed what operations on what API i.e. create/view/list on pods

- Admission Controllers comes after Authorization stage, to apply more granular rules in how any object will be created. 
ex:
when a pod creation request comes in, we would like to review the config file and 
1/ look at the image name and say dont allow images from public registry. i.e. only permit images from the internal registry, 
OR enforce, we must never use the latest tag for images.
2/ do not permit runAs root user.
3/ only permit certain capabilities only. in this case adding "MAC_ADMIN" capabilities should not be allowed.
4/ ensure metadat section always contains labels.

web-pod.yaml
-------------
apiVersion: v1
kind: Pod
metadata: 
  name: web-pod
spec:
  containers:
  - name: ubuntu
    image: ubuntu:latest
	command: ["sleep", "3600"]
	securityContext: 
	  runAsUser: 0
	capabilities:
	  add: ["MAC_ADMIN"]
	  
	  
- So these are some of fine grained rules that cant be achieved using RBAC, hence Admission Controllers helps implementing better security measures to enforce how a cluster is used.

- apart from simply validating the requested definition, it can also change the request itself or perform additional operations before the pod gets created. 

- There are no. of pre built-in Admission controllers. ex: 
	- AlwaysPullImages: ensure everytime a pod is created the image is always pulled afresh
	- DefaultStorageClass: observes creation of PVC and automatically adds a default storage class to them if one is not specified. 
	- EventRateLimit: set a limit that the kube-apiserver can handle at a time and prevent apiserver from flooding multiple request.
	- NamespaceAutoProvision: disabled by default. it automatically creates the namespace if it does not exists. 
	- NamespaceExists: rejects request to create on namespace that do not exists. enabled bydefault.


NamespaceExists Admission Controller
-------------------------------------------
- say we have a namespace "blue" that does not exists.

- running the below command will throw error: <Error from server (NotFound): namespaces "blue" not found>

> kubectl run nginx --image nginx --namespace blue

- here this request gets authenticated and then authorized and it then goes though the built-in admission controllers. the "NamespaceExists" admission controllers handles the request and checks if the blue namespace is available, if it is not then the request gets rejected. 

- the "NamespaceExists" is built-in admission controller that is enabled by default.


NamespaceAutoProvision Admission Controller
-------------------------------------------	
- NamespaceAutoProvision is another built-in admission controllers that is not enabled by default, it automatically creates the namespace if it foes not exists. 


NamespaceLifecycle Admission Controller
-------------------------------------------	
** NOTE: Note that the NamespaceExists and NamespaceAutoProvision admission controllers are deprecated and now replaced by NamespaceLifecycle admission controller.

- The NamespaceLifecycle admission controller will make sure that requests to a non-existent namespace is rejected and that the default namespaces such as default, kube-system and kube-public cannot be deleted.


View all by-default enabled Admission Controllers
--------------------------------------------------
$ kube-apiserver -h | grep enable-admission-plugins


- Since the kube-apiserver is running as pod you can check the process to see enabled and disabled plugins.

$ ps -ef | grep kube-apiserver | grep admission-plugins

root     18825 18808  0 20:35 ?        00:00:05 kube-apiserver --advertise-address=10.55.41.9 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision --disable-admission-plugins=DefaultStorageClass --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key


- to add a new admission controller, update the --enable-admission-plugins flag on the kube-apiserver

- to disable any of the controllers, use the --disable-admission-plugins flag on the kube-apiserver



- Which admission controller is enabled in this cluster which is normally disabled?
Check enable-admission-plugins in /etc/kubernetes/manifests/kube-apiserver.yaml

$ cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep enable-admission-plugins
    - --enable-admission-plugins=NodeRestriction
	

- enable the NamespaceAutoProvision admission controller

Add NamespaceAutoProvision admission controller to --enable-admission-plugins list to /etc/kubernetes/manifests/kube-apiserver.yaml
API server will automatically restart and pickup this configuration.


- Disable DefaultStorageClass admission controller

Update /etc/kubernetes/manifests/kube-apiserver.yaml as below

   - --disable-admission-plugins=DefaultStorageClass

Note: Once you update kube-apiserver yaml then please wait few mins for the kube-apiserver to restart completely.




Validating and Mutating Admission Controllers
===================================================================================================

- 2 types of admission controller. Validating Admission Controllers and Mutating Admission Controllers
	- Validating Admission Controllers: those that can validate the request and then allow/deny.
	- Mutating Admission Controllers: change the incoming request itself before its created.

- the "NamespaceExists" is built-in admission controller, helps validating if the namespace exists and rejects the request f it does not exists. this is known as a validating admission controller. enabeld bydefault.

- the "DefaultStorageClass" built-in admission controller, intercepts any PVC creation request, checks if it has any StorageClass defined, if not, it will modify the definition and add a default Storage class. this is known as a Mutating admission controller. it can change the object itself before its created.

pvc-definition.yaml
--------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  myclaim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
  storageClassName: default
  
  
- there can be an admission controller that can do both validate as well as mutate.

- generally mutating admission controller are invoked first followed by Validating admission controllers, this is so that any changes made by Mutating controller can be validated by Validating admission controller.
ex: "NamespaceAutoProvision" Mutating admission controller is invoked firts followed by "NamespaceExists" Validating admission controller. the reverse order would cause always rejecting the request if the namespace does not exists by "NamespaceExists", and "NamespaceAutoProvision" would never get invoked.

- when a request goes thorugh these admission controllers, if any admission controller rejects the request, the request is REJECTED and an error message is shown to the user.




Custom Admission Controller (Dynamic Admission Control)
====================================================================
- How we can configure our own admission controller with our own validation or mutation logic.

- admission plugins can be developed as extensions and run as webhooks configured at runtime.


admission webhooks
--------------------------
- Admission webhooks are HTTP callbacks that receive admission requests and do something with them. two types of admission webhooks, validating admission webhook and mutating admission webhook. 

- with the help of Admission webhooks, we can write our own admission controller.

- we can configure these webhooks to point to a Server, thats hosted either within the kubernetes cluster or outside, and that server will have its 
Admission webhook service running with its own custom logic. 

- after any request to kube-apiserver goes through to all built-in admission controllers, it hits the Webhook if configured. once it hits the webhook, it makes a call to the Admission Webhook server by passing the admission reveiw object in JSON format, this oJSON object has all the details about the request such as the user that made the request, type of operation on what object and details about the object etc.

- on receiving the admission object, the admission webhook server running our custom logic should respond an admission review object with a result of whether the request is allowed or not. if the "allowed" field is set to "true" then the request is allowed else denied.



- How do we set this up:

STEP-1

- Develop our own custom admission webhook server is a service that we deploy contains the logic or the code to permit/reject a request and it must be able to receive and respond with the appropriate response that the admission webhook and/or mutating admission webhook expects.


- there will be 2 calls, a/ validate and b/ mutate

- sample sudo code in python: 

/* validates the request, rejects request if the username and objectname to be created are equal else accept */

@app.route("/validate", methods=["POST"])
dev validate():
	object_name = request.json["request"]["object"]["metadata"]["name"]
	user_name = request.json["request"]["userInfo"]["name"]
	status = True
	if object_name == user_name:
		message = "You can not create object with your own name"
		status = False
	
	return jsonify (
		{
			"response": {
				"allowed": status,
				"uid": request.json["request"]["uid"],
				"status": {"message": message}
			}
		}
	)
	
	

- the mutate function gets the object and responds with a patch object wrapped in it. a Patch json object is a list of patch operation being add/remove/replace etc, then specify the path within the json object that needs to targetted for change, and then the value that needs to be added.

- this patch then gets encoded as base64 and then sent as part of the response.

/* adds the username as a label inside metadata */

@app.route("/mutate", methods=["POST"])
dev mutate():
	user_name = request.json["request"]["userInfo"]["name"]
	patch = [{ "op": "add", "path": "/metadata/labels/users", "value": user_name }]
	return jsonify (
		{
			"response": {
				"allowed": True,
				"uid": request.json["request"]["uid"],
				"patch": base64.b64encode(patch),
				"patchtype": "JSONPatch"
			}
		}
	)
	
	
	

STEP-2
- once our webhook server is ready, the next step is to host it.
- we either run it as a server somewhere or containerize and deploy within kubernetes cluster as deployment, if deployed as a deployment (webhook-deployment) in a kubernets cluster, then it needs a service (webhook-service) to be accessed.


STEP-3
- we then configure this webhook in kubernetes by creating the respective Webhook configuration object (ValidatingWebhookConfiguration/MutatingWebhookConfiguration).

Prerequisites:

- Ensure that the Kubernetes cluster is at least as new as v1.16 (to use admissionregistration.k8s.io/v1), or v1.9 (to use admissionregistration.k8s.io/v1beta1).

- Ensure that MutatingAdmissionWebhook and ValidatingAdmissionWebhook admission controllers are enabled. 

- Ensure that the admissionregistration.k8s.io/v1 or admissionregistration.k8s.io/v1beta1 API is enabled.


custom-validate-webhook-config.yaml
---------------------------------------
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: "pod-policy.example.com"
webhooks:
- name: "pod-policy.example.com"
  clientConfig:
    service:
      namespace: "webhook-namespace"
      name: "webhook-service"
    caBundle: "Ci0tLS0tQk...haha"
  rules:
  - apiGroups:   [""]
    apiVersions: ["v1"]
    operations:  ["CREATE"]
    resources:   ["pods"]
    scope:       "Namespaced"


- Here, in clientConfig, we configure the location of our admission webhook server, if deployed externally. the provide the url like below:

clientConfig:
  url: https://external-server.example.com
  
if deployed on kubernets cluster, give the name of the service and the namespace where its deployed.

- we also need to add the certificate as caBundle used for communication between kube-apiserver to our custom admission webhook service.


- Here, in rules we specify set of rules to config exactly when to call our webhook server like which particular object and which operation. ex: we only wanted to be called during creating of a new pod request.

Exercise:
----------
- Create TLS secret named webhook-server-tls

$ kubectl -n webhook-demo create secret tls webhook-server-tls \
	--cert "/root/keys/webhook-server-tls.crt" \
	--key "/root/keys/webhook-server-tls.key"
	
secret/webhook-server-tls created


- Create webhook deployment 

webhook-deployment.yaml
-----------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webhook-server
  namespace: webhook-demo
  labels:
    app: webhook-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webhook-server
  template:
    metadata:
      labels:
        app: webhook-server
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1234
      containers:
      - name: server
        image: stackrox/admission-controller-webhook-demo:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8443
          name: webhook-api
        volumeMounts:
        - name: webhook-tls-certs
          mountPath: /run/secrets/tls
          readOnly: true
      volumes:
      - name: webhook-tls-certs
        secret:
          secretName: webhook-server-tls
		  

$ kubectl create -f webhook-deployment.yaml 
deployment.apps/webhook-server created


- Create webhook service now so that admission controller can communicate with webhook

webhook-service.yaml
--------------------
apiVersion: v1
kind: Service
metadata:
  name: webhook-server
  namespace: webhook-demo
spec:
  selector:
    app: webhook-server
  ports:
    - port: 443
      targetPort: webhook-api


$ kubectl create -f webhook-service.yaml 
service/webhook-server created

$ kubectl get all -n webhook-demo 
NAME                                  READY   STATUS    RESTARTS   AGE
pod/webhook-server-7c8b68dccc-57stp   1/1     Running   0          3m57s

NAME                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/webhook-server   ClusterIP   10.107.105.186   <none>        443/TCP   2m20s

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/webhook-server   1/1     1            1           3m57s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/webhook-server-7c8b68dccc   1         1         1       3m57s


-  added MutatingWebhookConfiguration under /root/webhook-configuration.yaml
webhook-configuration.yaml
--------------------------
apiVersion: admissionregistration.k8s.io/v1beta1
kind: MutatingWebhookConfiguration
metadata:
  name: demo-webhook
webhooks:
  - name: webhook-server.webhook-demo.svc
    clientConfig:
      service:
        name: webhook-server
        namespace: webhook-demo
        path: "/mutate"
      caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURQekN
	  RWUpLb1pJaHZjTkFRRUwKQlFBd0x6RXRNQ3NHQT
	  heUJFWlcxdgpJRU5CTUI0WERUSXlNREV5TVRFer
    rules:
      - operations: [ "CREATE" ]
        apiGroups: [""]
        apiVersions: ["v1"]
        resources: ["pods"]
		

$ kubectl create -f webhook-configuration.yaml
mutatingwebhookconfiguration.admissionregistration.k8s.io/demo-webhook created



- In previous steps we have deployed demo webhook which does below
- Denies all request for pod to run as root in container if no securityContext is provided.
- If no value is set for runAsNonRoot, a default of true is applied, and the user ID defaults to 1234
- Allow to run containers as root if runAsNonRoot set explicitly to false in the securityContext


- Deploy a pod with no securityContext specified.
pod-with-defaults.yaml	  
----------------------
# A pod with no securityContext specified.
# Without the webhook, it would run as user root (0). The webhook mutates it
# to run as the non-root user with uid 1234.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-defaults
  labels:
    app: pod-with-defaults
spec:
  restartPolicy: OnFailure
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]
	  

$ kubectl create -f pod-with-defaults.yaml 
pod/pod-with-defaults created

below gets added after pod is created.

securityContext:
    runAsNonRoot: true
    runAsUser: 1234	  



- Deploy pod with a securityContext explicitly allowing it to run as root
pod-with-override.yaml
----------------------
 A pod with a securityContext explicitly allowing it to run as root.
# The effect of deploying this with and without the webhook is the same. The
# explicit setting however prevents the webhook from applying more secure
# defaults.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-override
  labels:
    app: pod-with-override
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: false
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]
	  

$ kubectl create -f pod-with-override.yaml 
pod/pod-with-override created



- Deploy a pod with a conflicting securityContext i.e. pod running with a user id of 0 (root).
- Mutating webhook should reject the request as its asking to run as root user without setting runAsNonRoot: false
pod-with-conflict.yaml
----------------------
# A pod with a conflicting securityContext setting: it has to run as a non-root
# user, but we explicitly request a user id of 0 (root).
# Without the webhook, the pod could be created, but would be unable to launch
# due to an unenforceable security context leading to it being stuck in a
# 'CreateContainerConfigError' status. With the webhook, the creation of
# the pod is outright rejected.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-conflict
  labels:
    app: pod-with-conflict
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: true
    runAsUser: 0
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]
	  
# A pod with a conflicting securityContext setting: it has to run as a non-root
# user, but we explicitly request a user id of 0 (root).
# Without the webhook, the pod could be created, but would be unable to launch
# due to an unenforceable security context leading to it being stuck in a
# 'CreateContainerConfigError' status. With the webhook, the creation of
# the pod is outright rejected.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-conflict
  labels:
    app: pod-with-conflict
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: true
    runAsUser: 0
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]
	  

$ kubectl create -f pod-with-conflict.yaml 

Error from server: error when creating "pod-with-conflict.yaml": admission webhook "webhook-server.webhook-demo.svc" denied the request: runAsNonRoot specified, but runAsUser set to 0 (the root user)









API Groups
===================================================================================
- all operation performed on the kubernetes cluster is done by kube-apiserver via kubectl utility or directly via REST

- The API group is specified in a REST path and in the apiVersion field of a serialized object.

- to check the version of cluster:

- There are several API groups in Kubernetes:
	- The core (also called legacy) group is found at REST path /api/v1. The core group is not specified as part of the apiVersion field, for example, apiVersion: v1.
	
	- The named groups are at REST path /apis/$GROUP_NAME/$VERSION and use apiVersion: $GROUP_NAME/$VERSION (for example, apiVersion: batch/v1, apps/vi).
	
- Enabling or disabling API groups
	- Certain resources and API groups are enabled by default.
	- You can enable or disable them by setting --runtime-config on the API server. The --runtime-config flag accepts comma separated <key>[=<value>] pairs describing the runtime configuration of the API server. 
	- If the =<value> part is omitted, it is treated as if =true is specified.
	ex:
	- to disable batch/v1, set --runtime-config=batch/v1=false
	- to enable batch/v2alpha1, set --runtime-config=batch/v2alpha1 OR --runtime-config=batch/v2alpha1=true

$ curl https://kube-master:6443/version 

{
	"major": "1",
	"minor": "13",
	"gitVersion": "v1.13.0",
	"gitCommit": "ddf47ac13c1a9483ea035a79cd7c10005ff21a6d",
	"gitTreeState": "clean",
	"buildDate": "2018-12-03T20:56:12Z",
	"goVersion": "go1.11.2",
	"compiler": "gc",
	"platform": "linux/amd64"
}


- to get the list of pods

$ curl https://kube-master:6443/api/v1/pods 
{
"kind": "PodList",
"apiVersion": "v1",
"metadata": {
"selfLink": "/api/v1/pods",
"resourceVersion": "153068"
},
"items": [
{
	"metadata": {
	"name": "nginx-5c7588df-ghsbd",
	"generateName": "nginx-5c7588df-",
	"namespace": "default",
	"creationTimestamp": "2019-03-20T10:57:48Z",
	"labels": {
	"app": "nginx",
	"pod-template-hash": "5c7588df"
},
"ownerReferences": [
{
	"apiVersion": "apps/v1",
	"kind": "ReplicaSet",
	"name": "nginx-5c7588df",
	"uid": "398ce179-4af9-11e9-beb6-020d3114c7a7",
	"controller": true,
	"blockOwnerDeletion": true
}
]
},
..
..



- all kubernets APIs are grouped together under the group name URI such as:
/metrics  	: monitoring
/healthz	: monitoring
/version	
/api		: core api for cluster functionalities
/apis		: named api for cluster functionalities
/logs		: for integrating with 3rd party logging application


ex:
$ curl http://localhost:6443 -k
{
"paths": [
"/api",
"/api/v1",
"/apis",
"/apis/",
"/healthz",
"/logs",
"/metrics",
"/openapi/v2",
"/swagger-2.0.0.json",
..
..
}



- core APIs:
/api (core) -> /v1 ->
/namaspaces
/pods
/rc
/events
/endpoints
/nodes
/bindings
/PV
/PVC
/configmaps
/secrets
/services



- named group APIs (/apis) are more organized, going forward, all the new features will come under this.

ex:
$ curl http://localhost:6443/apis -k | grep "name"    // lists all supported resources under apis group.
"name": "extensions",
"name": "apps",
"name": "events.k8s.io",
"name": "authentication.k8s.io",
"name": "authorization.k8s.io",
"name": "autoscaling",
"name": "batch",
"name": "certificates.k8s.io",
"name": "networking.k8s.io",
"name": "policy",
"name": "rbac.authorization.k8s.io",
"name": "storage.k8s.io",
"name": "admissionregistration.k8s.io",
"name": "apiextensions.k8s.io",
"name": "scheduling.k8s.io",


/apis/apps
	/v1 
		/deployments (resources)  --> list/get/create/delete/update/watch  (verbs)
		/replicasets
		/statefulsets

/extensions

/networking.k8s.io
	/v1
		/networkpolicies
		
/storage.k8s.io
/authentication.k8s.io
/certificates.k8s.io
	/v1
		/certificatesingingrequests
		

NOTE: 
/v1alpha1 is for Alpha
/v1beta1 is for Beta 
/v1 resource group is for GA/stable version.

- If we login to any cluster and try to curl the APIs like below:
$ curl https://<kube-master>:6443 -k8s

{
	"kind": "Status",
	"apiVersion": "v1",
	"metadata": {},
	"status": "Failure",
	"message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
	"reason": "Forbidden",
	"details": {},
	"code": 403
}

- we see 403 authentication error, we will not be allowed access except for certain API like https://<kube-master>:6443/version, as we have not specified any authN params in the request.

- we need to pass like: (<kube-master> can be localhost in case single node cluster)

$ curl http://<kube-master>:6443 –k
--key admin.key
--cert admin.crt 
--cacert ca.crt


{
	"paths": [
		"/api",
		"/api/v1",
		"/apis",
		"/apis/",
		"/healthz",
		"/logs",
		.
		..
	]
}



kubectl proxy
---------------
- kubectl proxy command launches a proxy service locally on port 8001 (default) and use credentials and certificates present in your kube config file ($HOME/.kube/config) to access the cluster, that way we dont have to specy the certs in curl command. so this proxy stamps the creds from kubeconfig and forwards the request to kube-apiserver.


$ kubectl proxy

Starting to serve on 127.0.0.1:8001



$ curl http://localhost:8001 -k

{
	"paths": [
		"/api",
		"/api/v1",
		"/apis",
		"/apis/",
		"/healthz",
		"/logs",
		"/metrics",
		"/openapi/v2",
		"/swagger-2.0.0.json",
		..
		.
	]
}



kube proxy vs kubectl proxy
---------------------------------------------
- they are not same.
- kube proxy: used to enable connectivity between pods and services acorss different nodes in the cluster.
- kubectl proxy: is a HTTP proxy service created by kubectl utility to access the kube-apiserver via REST



API Versions
================================================================================
- Each api group have 3 types of versioning. Different API versions indicate different levels of stability and support.

- Alpha

	- The version names contain alpha (for example, v1alpha1, v2alpha2 etc).
	- this feature is disabled by default
	- may contain bugs.
	- support for a feature may be dropped at any time without notice.
	
- Beta

	- The version names contain beta (for example, v2beta3).
	- well tested. enabled by default.
	- support for a feature will not be dropped, though the details may change.
	- not recommended for production uses
	- available for test users


- v1 (GA i.e. Generally Available) i.e Stable

	- The version name is vX where X is an integer.
	- avalable for all
	

ex:
/apis/apps/v1alpha1/deployments/<verbs>
/apis/apps/v1beta1/deployments/<verbs>
/apis/apps/v1/deployments/<verbs>
	

/apis/betworking.k8s.io/v1alpha1/<resource>/<verbs>


API Groups versions:
--------------------
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#-strong-api-groups-strong-

- An API Group can support multiple versions at the same time. ex: the apps API group can have v1alpha1, v1beta1 or v1 at the same time, means we will be to create the same object using any of these versions in yaml file.
ex: creating deployment in all 3 versions.

- Even through there are multiple versions supported at the same time, only 1 can be the PREFERRED or STORAGE version.

nginx-deployment.yaml
---------------------
apiVersion: apps/v1alpha1
kind: Deployment
metedata:
  name: nginx
spec:
 ..


nginx-deployment.yaml
---------------------
apiVersion: apps/v1beta1
kind: Deployment
metedata:
  name: nginx
spec:
 ..
 
 
nginx-deployment.yaml
---------------------
apiVersion: apps/v1
kind: Deployment
metedata:
  name: nginx
spec:
 ..



Preferred Version
------------------
- is the default version set to Kubernetes apiserver used when we retrieve information through k8s API using kubectl get command etc. 

ex: kubectl get deployment ----> which version this command going to query. thats defined by the preferred version set to kube-apiserver. in this case if v1 is set to preffred version then that version of API will be queried.

- kubectl explain deployment  ---> shows the preferred api version.

- preferred version is listed when we list the API 
ex: https://<kube-master>:6443/apis/batch
{
	kind: APIGroup
	
}


Storage Version
------------------	
- is the default version set to Kubernetes apiserver used when an object is finally created and stored in etcd database, its irrespective of the version used in definition yaml file.

- if storage version set in kubernetes is different than the version used in yaml file, then the definition object will be converted to the same storage version and the the object will be created and stored in etcd.

- if multiple versions are enabled i.e. v1alpha1, v1beta1, v1, at any point, only any one version can be set as Storage version.

- as of now, its not possible to see which is the storage version of particular API though an api or a command
	
	- way to find. query etcd database.
	
	ex: 
	$ etcdctl get "registry/deployments/default/blue" --print-value-only
	k8s
	
	apps/v1
	Deployment
	

NOTE: one one version for preferred or Storage version, usually both of these are same but they can be different.
 




API Deprecations
===============================================================
- along with kubernets own release version cycles every API groups also followes certain versioning.

- a single API group can support multiple versions at a time, why do we need multiple versions and how many old versions we should support, when can we remove an older version that is no longer required -- all this is answered by API deprication policy.

ex: a new API being developed:
/apis/kodekloud.com
	/v1alpha1
		/course
		/webinar
		
	/v1alpha2
		/course
		/webinar X  (dropped on this version.)


at this stage, if we can set the preferred/storage version to v1alpha2, so any old yaml creating an object of type course of v1alpha1 ver., will get converted to the preferred storage version v1slpha2 and then created.

Rules:
- an API featire can only be removed by incrementing the version of the API group. ex: v1alpha1 ver. has /webinar, to remove this feature, we need v1alpha2 ver. created and keep only /course

- other than the most recent api versions in each track, older api versions must be supported for durations:
	- GA: 12 months or 3 release whichever is longer.
	- Beta: 9 months or 3 release whichever is longer.
	- Alpha: 0 releases.
	
	
kubectl convert command
--------------------------
- whwn a kubernetes cluster being upgraded, we have new API being added and old ones are deprecated and removed. when old apis are removed, we need to uupgrade existing old manifest files to new version.

- ex: a yaml file with Deployment version of v1beta1, now, when kubernetes is upgraded, the beta1 ver. of Deployment is removed and we need to use the v1 ver. going forward. however we may have a lot of yaml files in the old manifest which old v1beta1 in them.

$ kubectl convert -f <old_file> --output-version <new_api_ver>

ex:
$ kubectl convert -f nginx-deployment.yaml --output-version apps/v1

NOTE: the kubectl convert is a separate plugin and may not be readily installed on your k8s cluster. need to install.
$ curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   154  100   154    0     0   8105      0 --:--:-- --:--:-- --:--:--  8105
100 51.8M  100 51.8M    0     0   150M      0 --:--:-- --:--:-- --:--:--  150M

$ chmod +x kubectl-convert 

$ mv kubectl-convert /usr/local/bin/kubectl-convert

$ cat ingress-old.yaml 
---
# Deprecated API version
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: ingress-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /video-service
        pathType: Prefix
        backend:
          serviceName: ingress-svc
          servicePort: 80


$ kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
..
..

$ kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1 > ingress-new.yaml


Exercise:
---------
1/ Enable the v1alpha1 version for rbac.authorization.k8s.io API group on the controlplane node.

- As a good practice, take a backup of that apiserver manifest file before going to make any changes. In case, if anything happens due to misconfiguration you can replace it with the backup file.

root@controlplane:~# cp -v /etc/kubernetes/manifests/kube-apiserver.yaml /root/kube-apiserver.yaml.backup

- Now, open up the kube-apiserver manifest file and add the --runtime-config flag in the command field as follows :-

 - command:
    - kube-apiserver
    - --advertise-address=10.18.17.8
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --runtime-config=rbac.authoriza

- After that kubelet will detect the new changes and will recreate the apiserver pod.

root@controlplane:~# kubectl get po -n kube-system




Custom Resource Definition (CRD)
=============================================================================
- Resource:
	- when we create a resource say Deployment in yaml file, kubernetes creates a deployment object and stores its information in etcd datastore.
	- we can list the deployment
	- we can run the delete command to delete the deployment object.
	- all of these going to add a create, list and delete info for the deployment object in etcd datastore. 
	- Now, when a deployment object is created, ther Deployment Controller is responsible to create corresponding ReplicaSet objcet which in turn create the no. of pods defined.
	- Deployment controller is built-in that comes along with kubernetes.
	
- Resource Controller:
	- runs in the background and continously monitor the status of the resources that it supposed to manage. when we add create/update/delete commands to etcd datastore, it makes necessary changes in the cluster to match the state.
	- written in Go language.. is part of deployment src code of kubernetes.
	
- every resource types in kubernetes has its corresponding controllers.
- ReplicaSet, Deployment, Job, CronJob, StatefulSet, Namespace these resources has its controllers responsible to watch its status and makes needed changes to match the state as expected.

example of creating a custom resource:

flightticket.yaml
------------------
apiVersion: flights.com/v1
kind: FlightTicket
metadata:
  name: my-flight-ticket
spec:
  from: Mumbai
  to: London
  number: 2
  
- to create a flightticket resource in kubernetes:
$ kubectl create -f flightticket.yaml

- to list all the flightticket objects:
$ kubectl get flightticket

- to delete:
$ kubetcl delete -f flightticket.yaml


- all these commands will create/delete the object in etcd datastore, BUT its not actually booked the flight ticket yet. ex: we have an api to book flights at https://book-flight.com/api 
- now we are going to need a controller (flightticket_controller.go), will watch for creating or updation of flight ticket resource, and on creation/deletetion of it, the controller contacts the https://book-flight.com/api to book or make necessary changes.

	
- Now, before we able to run the kubectl create command for this new custom resource, we need to regsuter that resource in kubernetes using Cistom Resource Definition.

- Custom resources can appear and disappear in a running cluster through dynamic registration, and cluster admins can update custom resources independently of the cluster itself. 

- Once a custom resource is installed, users can create and access its objects using kubectl

- When you create a new CustomResourceDefinition (CRD), the Kubernetes API Server creates a new RESTful resource path for each version you specify.

- The CRD can be either namespaced or cluster-scoped, as specified in the CRD's scope field.

- CustomResourceDefinitions themselves are non-namespaced and are available to all namespaces.


flightticker-crd-definition.yaml
---------------------------------
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  # name must match the spec fields below, and be in the form: <plural>.<group>
  name: flighttickets.flights.com
spec:
  # group name to use for REST API: /apis/<group>/<version>
  group: flights.com
  
  # either Namespaced or Cluster
  scope: Namespaced
  names:
    # plural name to be used in the URL: /apis/<group>/<version>/<plural>
    plural: flighttickets
    # singular name to be used as an alias on the CLI and for display
    singular: flightticket
    # kind is normally the CamelCased singular type. Your resource manifests use this.
    kind: FlightTicket
    # shortNames allow shorter string to match your resource on the CLI ex: kubectl get ft
    shortNames:
    - ft
	
  # list of versions supported by this CustomResourceDefinition
  versions:
    - name: v1
      # Each version can be enabled/disabled by Served flag.
      served: true
      # One and only one version must be marked as the storage version.
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                from:
                  type: string
                to:
                  type: string
                number:
                  type: integer
				  minimum: 1    // optional
				  maximum: 10   // optional
				  

$ kubectl create -f flightticker-crd-definition.yaml

- once the CRD is created, we can now create the flightticket object.

- use CRD to create any kind of resource type and specify a schema and add validations. BUT its only going to create these objcet in etcd datastore, its not actually going to do anything about these resource objectives, as we have not created the corresponding custom controller.




Custom Controllers
==========================================================================================
- A custom controller is needed to keep monitoring the cluster for creation/updation of its custom resource type objects and take action accordingly. ex: book or cancel flight ticket using the booking api website.

- a control loop is a non-terminating loop that regulates the state of a system.

- In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.


- any process or code that runs in a loop, continously monitoring the kubernetes cluster and listening to events of specific objects being changed (ex: flightticket object), can write in python or Go.

- developing custom controllers in python may be challenging as the internal calls to the k8s api may become expensive also, need to create our own queuing and caching mechanism, developing in go with kubernetes Go client, provides support for other libraries like shared-informer that provides caching and queuing mechanism and helps build controllers easicu in right way.

- kubernets has sample controller github repo, clone and use.

- install Go on machine.

- git clone https://github.com/kubernetes/sample-controller.git

> cd sample-controller

> customize controller.go as per your own logic.

> go build -o sample-controller .

> ./sample-controller -kubeconfig=$HOME/.kube/config
- this authenticated using kube config with the cluster and start watching locally and once any object is created of same type, it makes the necessary actions.

- once the controller code is ready, we package the custom controller in docker image and then run it inside kubernetes cluster as POD or Deployment.




Deployment Strategy - Blue Green
============================================================================

Deployment Strategy
-------------------
- 2 basic deployment strategies, kubernetes supports via its config 
	- Recreate strategy: 
		- Upgrade to newer version all at once. NOT the default.
		- the application will be down in between.
		- .spec.strategy.type==Recreate
		
	- RollingUpdate 
		- we do not destroy all of them at once. Instead we take down the older version and bring up a newer version one by one. 
		- RollingUpdate is the default Deployment Strategy.
		- .spec.strategy.type==RollingUpdate
		
Other than these 2, there are few additional deployment strategies that we can not specify as configurations in kubernetes yaml file but can be achieved.

- Blue/Green is a deployment strategy where we have the new version deployed along side the old version. old version is called BLUE and new ver, is GREEN. 100% of the traffic is still routed to the old version, tests are run on new version, once all tests are passed, we switch traffic all at once from BLUE to GREEN.

- best implemented using Service Mesh using Istio.

- can be implemented using native kubernetes deployments and services.

- 2 deployment yamal, blue and green, blue has old ver. v1 and green has new ver. v2
- 1 service, first pointing to old ver. blue (image: myapp-image:1.0) using pod selector as v1, once the green is tested, point to green (image: myapp-image:2.0) using pod selector as v2

myapp-deployment-blue.yaml
---------------------------
apiVresion: v1
kind: Deployment
metadata:
  name: myapp-blue
  labels:
    app: myapp
	type: front-end
spec:
  template:
    metadata:
	  name: myapp-pod
	  labels:
	    version: v1
	spec:
	  containers:
	  - name: app-container
	    image: myapp-image:1.0
	replicas: 5
	selector:
	  matchLabels:
	    type: front-end
		

myapp-service.yaml
------------------
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    version: v1

myapp-deployment-green.yaml
---------------------------
apiVresion: v1
kind: Deployment
metadata:
  name: myapp-green
  labels:
    app: myapp
	type: front-end
spec:
  template:
    metadata:
	  name: myapp-pod
	  labels:
	    version: v2
	spec:
	  containers:
	  - name: app-container
	    image: myapp-image:2.0
	replicas: 5
	selector:
	  matchLabels:
	    type: front-end
		
myapp-service.yaml
------------------
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    version: v2
	




Deployment Strategy - Canary
==========================================================
- In this strategy we deploy a new version and route only a small percentage to it. the majority of traffic is being routed to the older version. at this we on test and if everything looks good, we upgrade the original deployment with the new version, we may use RollingUpdate to upgrade. then we get rid of the canary deployment.


- this can be achieved using native kubernetes deployments and services, BUT best implemented using Servuce Mesh using Istio.

- 1st deployment named: deployment-primary, where the pod label: version = v1 and a common label: app = front-end with replicas set to 5.
- 2nd deployment named: deployment-canary, where the pod label: version = v2 and a common label: app = front-end with replicas set to 1.
- 1 service pointing to primary initially using pod selector: version = v1
- then after the canary deployment is deployed, point the service to both using pod selector: app = front-end

- Now, Service equally distributes the traffic among the total no. of target pods, hence in this case, total no. pod = 6, hence 83% of traffic gets routed to primary ver. and only 17% of traffic routed to canary deployment.

- implementing canary deployment strategy using depoyment and service, we have limited control over traffic between each deployment, traffic split is always going to get governed by the no. of pods present in each deployment. ex: we can not say just route 1% of traffic to canary deployment, to achieve this we need to have atleast 100 pods all together. thats why Service Mesh like Istio comes with better control with Istio, we can define the exact no. of traffic to be routed between each deployment and it does not depends on no. of pods etc.











 













