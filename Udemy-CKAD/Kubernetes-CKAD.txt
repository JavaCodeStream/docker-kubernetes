Resources:
https://github.com/lucassha/CKAD-resources
https://github.com/dgkanatsios/CKAD-exercises


PODS
===============================
- kubernetes ultimate aim is to deploy applications in the form of containers on a set of machines that are configured as worker nodes in a cluster. however k8s does not deploy containers directly on node, containers are encapsulated in kubernets object known as PODs.

run an image into pod. image name: nginx and pod name: nginx. pulls the image from docker hub and runs within pod.
$ kubectl run nginx --image nginx 

$ kubectl run mosquito --image nginx
pod/mosquito created

$ kubectl run bee --image=nginx --restart=Never --dry-run=client -o yaml > bee-pod.yaml


- to know the Pod configs help. 
$ kubectl explain pod --recursive | less

$ kubectl explain pod --recursive | grep -A5 tolerations
tolerations       <[]Object>
	 effect <string>
	 key    <string>
	 operator       <string>
	 tolerationSeconds      <integer>
	 value  <string>


- list of pods available
$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
bee        1/1     Running   0          3m
mosquito   0/1     Pending   0          15m

- shows more details:
$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE    IP           NODE     NOMINATED NODE   READINESS GATES
bee        1/1     Running   0          3m5s   10.244.1.2   node01   <none>           <none>
mosquito   0/1     Pending   0          15m    <none>       <none>   <none>           <none>

$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
bee        1/1     Running   0          8m34s   10.244.1.2   node01         <none>           <none>
mosquito   1/1     Running   0          20m     10.244.0.4   controlplane   <none>           <none>


create pod
- kubectl create -f <pod-definition yml file name>
- kubectl run <pod_name> --image <image_name>  // pod name and container name will be the given <pod_name>

describe pod and look at its containers
- kubectl describe pod <pod_name>


create a manifest file using dryrun flag, dry-run=client allows not create the pod instead redirect the definition to yaml file.
- kubectl run redis --image=redis123 --dry-run=client -o yaml > redis-pod-definition.yaml
- kubectl apply -f <redis-pod-definition.yaml>  // create the pod using apply

edit and rerun pod creation
- kubectl apply -f <pod-definition yml file name>
or
- kubectl edit pod <pod_name>  // it opens the manifest in vi editor, (i) change the file and save (escape + :wq!)
kubernetes will immediately trigger a re-deploy of the pod, check the state and events using 'kubectl get pod <pod_name>' and 'kubectl describe pod <pod_name>'


If you are not given a pod definition file, you may extract the definition to a file using the below command, Then edit the file to make the necessary changes, delete and re-create the pod.
- kubectl get pod <pod-name> -o yaml > pod-definition.yaml


- Opening a shell when a Pod has more than one container
	- --container or -c to specify a container, If omitted, the first container in the pod will be chosen
	- -i, --stdin=false: Pass stdin to the container
	- -t, --tty=false: Stdin is a TTY
	- $ kubectl exec --help

$ kubectl exec -i -t my-pod --container main-app -- /bin/bash


- viwing logs
$ kubectl exec -i -t webapp -- cat /log/app.log

- list the containers under a pod.
$ kubectl get pods <pod_name> -o jsonpath='{.spec.containers[*].name}'
simple-webapp db


- list pods withour the header column in output
$ kubectl get pods --no-headers

- select specific pods by it's labels.
$ kubectl get pod --no-headers --selector bu=finance | wc -l


- get pods on all namespaces
$ kubectl get pods --all-namespaces


- show pods by its label
$ kubectl get pods -l <label-key>=<label-value>
ex:
$ kubectl get pods -l name=internal


- shows pods along with its labels
$ kubectl get pods --show-labels

NAME       READY   STATUS              RESTARTS   AGE   LABELS
external   0/1     ContainerCreating   0          22s   name=external
internal   0/1     ContainerCreating   0          21s   name=internal
mysql      0/1     ContainerCreating   0          22s   name=mysql
payroll    0/1     ContainerCreating   0          21s   name=payroll


Manifest file
------------------
- every k8s manifest files have 4 sections
- apiVersion, kind, metadata, spec.
	- apiVersion is the K8s API version using which we create any object of given kind.
	- kind is various type of objects available in kubernetes.
	- metadata is like a dictionary, has kubernetes defined set of key values pairs, name, labels (label can have user defined key-value pair)
	- spec is unique to each type of kind. it defines whats we are creating inside the object if the given kind.



=====================================================================================================
ReplicaSets
=====================================================================================================
- Controllers are the brain behind kubernetes, these are processes that monitor kubernetes objects and responds accordingly.
- one of such controller is 'Replication Controller'

- we need more than one pod instance to increase availability of our application  in case some pod fails or node crash.
- Replication Controller helps in running multiple instances of a single pod in kubernetes cluster thus provinding high availability.
- even we have a single pod the replication controller can help by automatically bringing up a new pod when the existing one fails, ensures the specified no. of pods are running all times.

- Replication controller used to create mutiple pod to share the application load balancing.

- Replication controller and Replica Set both have the same purpose but they are not same. Replication controller is older technology that is been replaced by Replica Set. Replica Set is new recommended way to setup replication.


Replication controller:
------------------------

rc-definition.yml
-------------------
apiVersion: v1
kind: ReplicationController
metedata: 
	name: myapp-rc
	labels: 
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-pod
			labels:
				app: myapp
				type: front-end
		spec:
			containers:
			-	name: nginx-container
				image: nginx			
	replicas:3
	
in above line 67-75 is talen from the below pod-definition.yaml (line 87-95)

pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
	labels:
		app: myapp
		type: front-end
spec:
	containers:
	-	name: nginx-container
		image: nginx
		

- kubectl create -f rc-definition.yml //this creates replication controller by first creating the pod.

- kubectl get replicationcontroller // shows desired/current/ready no. of pod replicas.

- kubectl get pods // shows 3 pod running. names for these pod starts with replication controller name i.e. myapp-rc-* indicating they are created via replication controller


Replica Set:
------------------------
- very similar to replication controller. the spec has template section to provide the pod definition.

- it requires a selector section to indentify whats pods fall under it, but why need to add the selector even if we provided the pod definition under its spec, its because, replica set can also manage pods that are not created as part of replica set creation. pods created before replica set, matched with the labels specified in selector, replica set include those pods for replication.

	- we still need to add the pod spec even we must add selector, as in case any of these pod dies, replica set creates a new one by using the pod spec given under template section.

- selector section is mandatory for replica set but optional for replication controller (it makes the selector same as given pod definition)

replicaset-definition.yml
--------------------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
	name: myapp-replicaset
	labels:
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-pod
			labels:
				app: myapp
				type: front-end
		spec:
			containers:
			-	name: nginx-container
				image: nginx
	replicas: 3
	selector: 
		matchLabels:
			type: front-end
			
- kubectl create -f replicaset-definition.yml

- kubectl get replicasets // shows desired/current/ready no. of pod replicas.
NAME              DESIRED   CURRENT   READY   AGE
new-replica-set   4         4         0       16m
replicaset-1      2         2         2       3m21s
replicaset-2      2         2         2       58s

- kubectl describe replicasets new-replica-set // to the image used to create the pods using replicaset

- kubectl get pods

- kubectl delete replicaset <replicaset_name>
replicaset.apps "replicaset-1" deleted

steps to edit any existing replicaset:
- kubectl edit replicaset new-replica-set 
replicaset.apps/new-replica-set edited

- kubectl get pods
NAME                    READY   STATUS             RESTARTS   AGE
new-replica-set-rpfr7   0/1     ImagePullBackOff   0          20m
new-replica-set-gvvlv   0/1     ImagePullBackOff   0          20m
new-replica-set-h9t8f   0/1     ImagePullBackOff   0          20m
new-replica-set-49d4f   0/1     ImagePullBackOff   0          12m

delete all pods.
- kubectl delete pod new-replica-set-rpfr7
pod "new-replica-set-rpfr7" deleted

- kubectl delete pod new-replica-set-gvvlv
pod "new-replica-set-gvvlv" deleted

- kubectl delete pod new-replica-set-h9t8f
pod "new-replica-set-h9t8f" deleted

- kubectl delete pod new-replica-set-49d4f
pod "new-replica-set-49d4f" deleted

- kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
new-replica-set-z9xf6   1/1     Running   0          46s
new-replica-set-jgwtr   1/1     Running   0          35s
new-replica-set-kknnd   1/1     Running   0          21s
new-replica-set-q4lm8   1/1     Running   0          9s

- kubectl get replicasets.apps 
NAME              DESIRED   CURRENT   READY   AGE
new-replica-set   4         4         4       23m



- ways to sclae the replicas.
	1/
	- update the replica set manifest file to scale the replicas. ex: from 3 to 6
	- kubectl replace -f replicaset-definition.yml
	
	2/
	- kubectl scale --replicas=6 -f replicaset-definition.yml
	
	3/
	- kubectl scale replicaset myapp-replicaset --replicas=6  // type (replicaset) and value (name of replicaset)
	
	4/
	- kubectl edit replicaset new-replica-set // vi editor and change the replicas and save.
	
	
	
Deployments
=====================================
- Pods which deploy single instance of our application such as webapp, each container is encapsulated withing pods, multiple such pods are deployed through replicaset or replication controller.
- then comes deployment which is kubernetes object that comes higher in hierarchy, deployment provides us with the capability to upgrade the underlying instances seamlessly with rolling updates, roll backs.
- deployment manifest looks similar to replicaset.

deployment-definition.yaml
---------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
	name: myapp-deployment
	labels:
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-pod
			labels:
				app: myapp
				type: front-end
		spec:
			containers:
			-	name: nginx-container
				image: nginx
	replicas: 3
	selector: 
		matchLabels:
			type: front-end
			

kubectl create/apply creates the deployment which automatically creates a replicaset in the name of depoloyment (ex: myapp-deployment-56d8ff5458), the replicaset ultimately create pods and the pods names are in the name of replicaset (ex: myapp-deployment-56d8ff5458-5hjhr, myapp-deployment-56d8ff5458-5gb8q)
- kubectl create -f deployment-definition.yaml
- kubectl get deployments
- kubectl get replicasets
- kubectl get pods

- kubetcl get all // to see all the objects in the given namespace.

- kubectl describe deployments.apps frontend-deployment
Name:                   frontend-deployment
Namespace:              default
CreationTimestamp:      Tue, 21 Dec 2021 17:18:10 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=busybox-pod
Replicas:               4 desired | 4 updated | 4 total | 0 available | 4 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=busybox-pod
  Containers:
   busybox-container:
    Image:      busybox888
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      echo Hello Kubernetes! && sleep 3600
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   frontend-deployment-56d8ff5458 (4/4 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  5m30s  deployment-controller  Scaled up replica set frontend-deployment-56d8ff5458 to 4
  
  
- kubectl describe deployments.apps frontend-deployment | grep -i image // to know image used in deployment.


deployment-definition-1.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  replicas: 2
  selector:
    matchLabels:
      name: busybox-pod
  template:
    metadata:
      labels:
        name: busybox-pod
    spec:
      containers:
      - name: busybox-container
        image: busybox888
        command:
		- sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600	
		

- kubectl [command] [TYPE] [NAME] -o <output_format>
Here are some of the commonly used formats:

-o json Output a JSON formatted API object.
-o name Print only the resource name and nothing else.
-o wide Output in the plain-text format with any additional information.
-o yaml Output a YAML formatted API object.

- kubectl create namespace test-123 --dry-run -o yaml
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: test-123
spec: {}
status: {}

Output with wide (additional details):
Probably the most common format used to print additional details about the object:
- kubectl get pods -o wide
NAME      READY   STATUS    RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES
busybox   1/1     Running   0          3m39s   10.36.0.2   node01   <none>           <none>
ningx     1/1     Running   0          7m32s   10.44.0.1   node03   <none>           <none>
redis     1/1     Running   0          3m59s   10.36.0.1   node01   <none>           <none>




==============================================================================

::Namespace::

==============================================================================

- kubernetes automatically creates a namespace during the cluster setup, known 'default' and all the objects gets created in it if not specified explicitly.

- kubernetes also creates a set of pods and services for its internal purpose such as those required by its network solutions, DNS service etc, kubernets creates these interla objects in its own namespace (kube-system) to prevent from users accidentally deleting or modifying.

- A 3rd namespace created by kubernetes: kube-public where such resources that should be made available to all users are created.

- we can create our own namespace to isolate resources/objects among them.

- each of these namespaces can have its own set of policies that define who can do what

- can also define quota to each namespaces to limit the certain amount of resource usage.

- just like members within a house can refer them by simply calling their name, objects within namespace can refer them by their name. ex: webapp-pod can use the db-service name to establish jdbc connection. i.e. mysql.connect("db-service")

	- object can also refer another object cross namespace by using the namespace_name.object_name
	ex: mysql.connect("db-service.dev-namespace.svc.cluster.local"), here: dev-namespace - the other namespace, svc - sub-domain for service, cluster.local is the default domain name, db-service is the name of service.

- kubectl get pods // shows pods from default namespace.
- kubectl get pods --namespace=<namespace_name>	
- kubectl create -f pod-definition.yaml -n dev-namespace

- can also move the namespace into the manifest file.

pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
	namespace: dev
	name: myapp-pod
	labels:
		app: myapp
		type: front-end
spec:
	containers:
	-	name: nginx-container
		image: nginx


namespace-definition.yaml
-------------------------
apiVersion: v1
kind: Namespace
metadata:
	name: dev
	
- kubectl create -f namespace-definition.yaml
or
- kubectl create namespace <namespace_name>



$ kubectl get ns
NAME                   STATUS   AGE
default                Active   302d
kube-node-lease        Active   302d
kube-public            Active   302d
kube-system            Active   302d
kubernetes-dashboard   Active   302d


- default: namespace is used when you schedule pods without declaring a specific namespace.

- kube-system: namespace is where pods essential to the operation of the Kubernetes cluster live.

- kubernetes-dashboard: contains the resources for a monitoring and graphical interface for managing the cluster.


- List all of the pods in the kube-system namespace:

$ kubectl get pod -n=kube-system 

NAME                                                               READY   STATUS    RESTARTS   AGE
calico-kube-controllers-69496d8b75-hcctr                           1/1     Running   1          302d
calico-node-77h65                                                  1/1     Running   0          17m
calico-node-lm56l                                                  1/1     Running   1          302d
calico-node-wrhnf                                                  1/1     Running   0          17m
coredns-74ff55c5b-h7f7t                                            1/1     Running   1          302d
coredns-74ff55c5b-kk4cw                                            1/1     Running   1          302d
etcd-ip-10-0-0-100.us-west-2.compute.internal                      1/1     Running   1          302d
kube-apiserver-ip-10-0-0-100.us-west-2.compute.internal            1/1     Running   1          302d
kube-controller-manager-ip-10-0-0-100.us-west-2.compute.internal   1/1     Running   1          302d
kube-proxy-bzbcn                                                   1/1     Running   1          302d
kube-proxy-hmz67                                                   1/1     Running   0          17m
kube-proxy-rw46v                                                   1/1     Running   0          17m
kube-scheduler-ip-10-0-0-100.us-west-2.compute.internal            1/1     Running   1          302d
metrics-server-5986c4b4d6-7lg48                                    1/1     Running   1          302d


- All of the components of the cluster are running in pods in the kube-system namespace:

calico: The container network used to connect each node to every other node in the cluster. Calico also supports network policy. Calico is one of many possible container networks that can be used by Kubernetes.
coredns: Provides DNS services to nodes in the cluster
etcd: The primary data store of all cluster state
kube-apiserver: The REST API server for managing the Kubernetes cluster
kube-controller-manager: Manager of all of the controllers in the cluster that monitor and change the cluster state when necessary
kube-proxy: Network proxy that runs on each node
kube-scheduler: Control plane process which assigns Pods to Nodes
metrics-server: Not an essential component of a Kubernetes cluster but it is used in this lab to provide metrics for viewing in the Kubernetes dashboard.




======================================================================================== 

::Resource Quota::

========================================================================================
https://kubernetes.io/docs/concepts/policy/resource-quotas/

- to limit resources in a given namespace, we need to create a resource quota.

- When several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its fair share of resources.

- Resource quotas are a tool for administrators to address this concern.

-Different teams work in different namespaces. 

- The administrator creates one ResourceQuota for each namespace.

- Users create resources (pods, services, etc.) in the namespace, and the quota system tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.

- If creating or updating a resource violates a quota constraint, the request will fail with HTTP status code 403 FORBIDDEN with a message explaining the constraint that would have been violated.


resource-quota-definition.yaml
------------------------------
apiVersion: v1
kind: ResourceQuota
metadata:
	name: compute-quota
	namespace: dev
spec:
	hard: 
		requests.cpu: "4"
		requests.memory: 5Gi
		limits.cpu: "10"
		limits.memory: 10Gi
		


object-counts.yaml
--------------------
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-counts
spec:
  hard:
    configmaps: "10"
    persistentvolumeclaims: "4"
    pods: "4"
    replicationcontrollers: "20"
    secrets: "10"
    services: "10"
    services.loadbalancers: "2"
	

- kubectl create -f resource-quota-definition.yaml
	

KUBECTL imperative command
======================================
- kubectl create namespace <namespace_name>

- kubectl config current-context // Displays the current-context
- kubectl config set-context $(kubectl config current-context) --namespace=dev  // gets the current context and set the namespace to it.

- kubectl get pods --all-namespace  // lists all pods from all namepaces

- kubectl get namespaces

- kubectl run redis --image redis --namespace finance // creates a pod in a given namespace
- kubectl run redis --image redis --dry-run=client -o yaml > pod-redis.yaml  // now open the yaml file add the namespace and create.

- kubectl get ns --no-headers | wc -l // counst all namaspaces
- kubectl -n <namespace_name> get pods --no-headers

- kubectl get pods --all-namespaces | grep blue  // Which namespace has the blue pod in it?
marketing       blue                                     1/1     Running            0               9m38s
 
- kubectl run nginx --image=nginx --dry-run=client -o yaml  // Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

- kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml  // Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

- kubectl create deployment nginx --image=nginx --replicas=4  // Generate Deployment with 4 Replicas

- kubectl scale deployment nginx --replicas=5  //  scale a deployment using the kubectl scale command.
 
Deploy a redis pod using the redis:alpine image with the labels set to tier=db, multiple labels? 
- kubectl run redis --image redis:alpine --labels tier=db

Create a service redis-service to expose the redis application within the cluster on port 6379
- kubectl expose pod redis --name redis-service --port 6379 --target-port 6379
- kubectl describe svc redis-service 
Name:              redis-service
Namespace:         defaut
Labels:            tier=db
Annotations:       <none>
Selector:          tier=db
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.43.154.70
IPs:               10.43.154.70
Port:              <unset>  6379/TCP
TargetPort:        6379/TCP
Endpoints:         10.42.0.10:6379
Session Affinity:  None
Events:            <none>


Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas, webapp is deployment name.
- kubectl create deployment webapp --image=kodekloud/webapp-color
deployment.apps/webapp created

- kubectl get deployment webapp
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
webapp   1/1     1            1           62s

- kubectl scale deployment webapp --replicas=3 
deployment.apps/webapp scaled

- kubectl get deployment webapp
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
webapp   3/3     3            3           108s


Create a new pod called custom-nginx using the nginx image and expose it on container port 8080. if we dont specify --port 8080, which port it runs?
- kubectl run custom-nginx --image nginx --port 8080
pod/custom-nginx created

Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2 replicas.
- kubectl --namespace dev-ns create deployment redis-deploy --image redis --replicas 2
deployment.apps/redis-deploy created


- kubectl -n dev-ns get deployments.apps redis-deploy 
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
redis-deploy   2/2     2            2           16s


Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80.
--expose: creates a service for the pod.

- kubectl run httpd --image httpd:alpine --port 80 --expose --dry-run=client -o yaml

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  name: httpd
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: httpd
status:
  loadBalancer: {}
---
---
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: httpd
  name: httpd
spec:
  containers:
  - image: httpd:alpine
    name: httpd
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}




===========================================================================================
Image and Docker vs Kubernetes
===========================================================================================
- lets say we have an image: ubuntu-sleeper

FROM ubuntu
ENTRYPOINT["sleep"]
CMD["5"]

docker
---------
$ docker run --name ubuntu-sleeper --entrypoint sleep2.0 ubuntu-sleeper 10

kubernetes
-----------
pod-definition.yaml

apiVersion: v1
kind: Pod
metadata: 
	name: ubuntu-sleeper-pod
spec:
	containers:
	-	name: ubuntu-sleeper
		image: ubuntu-sleeper
		command: ["sleep2.0"]   // this is corresponding to --entrypoint sleep2.0 in docker world i.e. overriding the command used in ENTRYPOINT instruction in docker file.
		args: ["10"]  // this to override the args mentioned in CMD instruction in docker file.


------------------------
example1:

Dockerfile2
------------
FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]


webapp-color-pod-2.yaml
-----------------------
apiVersion: v1
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]
	

pod-green.yaml // Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green
-----------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: webapp-green
  name: webapp-green
spec:
  containers:
  - image: kodekloud/webapp-color
    name: webapp-green
    args: [--color=green]
  dnsPolicy: ClusterFirst
  restartPolicy: Always



======================================================================================
Editing PODs and Deployments
======================================================================================
Edit a POD
------------
Remember, you CANNOT edit specifications of an existing POD other than the below.

spec.containers[*].image
spec.initContainers[*].image
spec.activeDeadlineSeconds
spec.tolerations

For example you cannot edit the environment variables, service accounts, resource limits
But if you really want to, you have 2 options:

1/
Run the kubectl edit pod <pod name> command.  This will open the pod specification in an editor (vi editor). Then edit the required properties. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable.

A copy of your changes has been stored to "/tmp/kubectl-edit-1554090010.yaml"
error: Edit cancelled, no valid changes were saved.

Delete the existing pod by running the command:
- kubectl delete pod webapp

Then create a new pod with your changes using the temporary file
- kubectl create -f /tmp/kubectl-edit-ccvrq.yaml


2/
The second option is to extract the pod definition in YAML format to a file using the command
- kubectl get pod webapp -o yaml > my-new-pod.yaml

Then make the changes to the exported file using an editor (vi editor). Save the changes
- vi my-new-pod.yaml

Then delete the existing pod
- kubectl delete pod webapp

Then create a new pod with the edited file
- kubectl create -f my-new-pod.yaml



Edit Deployments
-----------------
With Deployments you can easily edit any field/property of the POD template. Since the pod template is a child of the deployment specification,  with every change the deployment will automatically delete and create a new pod with the new changes. So if you are asked to edit a property of a POD part of a deployment you may do that simply by running the command:

- kubectl edit deployment <deployment_name>



ENV variables in Kubernetes
=============================
- docker run -e APP_COLOR=red simple-webapp-color

- set the environment variables under env section, its an array and each item has name and value

pod-definition.yaml
--------------------
apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    env:
    - name: APP_COLOR
      value: pink
	  

1/ plain key-value types:
env:
 -	name: APP_COLOR
	value:
	
2/ configMap
env:
 -	name: APP_COLOR
	valueFrom:
		configMapKeyRef: 
	
3/ Secrets
env:
 -	name: APP_COLOR
	valueFrom:
		secretKeyRef:
		



==============================================================
ConfigMap
==============================================================
ConfigMap
-----------
APP_COLOR: blue
APP_MODE: prod


Create configMap using command inline:
---------------------------------------
- kubectl create configmap <configmap_name> --from-literal=<key>=<value>
ex:
- kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod

- from-literal is uded to specify the key/value pair in the command itself.
- adding multiple key/value pair using from-literal 


- empty configMap
$ kubectl create configmap <configmap_name> -n <namespace_name>

ex:
$ kubectl create configmap nginx-configmap -n ingress-space 
configmap/nginx-configmap created



Create configMap using command from file:
------------------------------------------
- kubectl create configmap <configmap_name> --from-file=<file_path>
- when you create a ConfigMap using --from-file, the filename becomes a key stored in the data section of the ConfigMap. The file contents become the key's value.

ex:
- kubectl create cm app-config --from-file=app.properties 
configmap/app-config created


$ kubectl get cm

NAME               DATA   AGE
kube-root-ca.crt   1      5m58s
app-config         1      8s



$ kubectl describe cm app-config 

Name:         app-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
app.properties:
----
APP_COLOR=blue
APP_MODE=prod


BinaryData
====

Events:  <none>




$ kubectl create cm app-cm --from-literal=APP=front_end --dry-run=client -o yaml

apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: app-cm
data:
  APP: front_end


$ kubectl create cm app-cm --from-file=app.properties --dry-run=client -o yaml
apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: app-cm
data:
  app.properties: "APP_COLOR=blue\r\nAPP_MODE=prod"



config-map.yaml (data instead of spec)
----------------
apiVersion: v1
kind: ConfigMap
metadata: 
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod

- kubectl create -f config-map.yaml

- use these configMap names while associating to pods

- kubectl get configmaps | kubectl get cm
NAME               DATA   AGE
kube-root-ca.crt   1      16m
db-config          3      10s

- kubectl describe configmaps <name_of_configmap> | kubectl describe cm <name_of_configmap>
Name:         db-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
DB_PORT:
----
3306
DB_HOST:
----
SQL01.example.com
DB_NAME:
----
SQL01

BinaryData
====

Events:  <none>




configMap in Pods
------------------
pod-definition.yaml
--------------------
apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp-color
spec:
	containers:
	- name: simple-webapp-color
	  image: simple-webapp-color
	  ports:
	  - containerPort: 8080
	  envFrom:
	  - configMapRef:
		  name: webapp-config-map // inject a specific configMap as env variables into the pod created earlier. 
			
- we can inject specific env values from the earlier created configMap object:
env:
	- name: APP_COLOR
	  valueFrom:
		configMapKeyRef:
			name: app-config
			key: APP_COLOR
			

- we cal also inject the configMap data as a file in volume:
volumes:
	- name: app-config-volume
	  configMap: 
		name: app-config
		


- Populate a Volume with data stored in a ConfigMap
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: k8s.gcr.io/busybox
      command: [ "/bin/sh", "-c", "ls /etc/config/" ]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        # Provide the name of the ConfigMap containing the files you want
        # to add to the container
        name: special-config
  restartPolicy: Never





Exercise:
--------------
# Create namespace
kubectl create namespace configmaps
# Set namespace as the default for the current context
kubectl config set-context $(kubectl config current-context) --namespace=configmaps

1. Create a ConfigMap from two literal key-value pairs:

kubectl create configmap app-config --from-literal=DB_NAME=testdb \
  --from-literal=COLLECTION_NAME=messages
  
  
2. Display the ConfigMap:

$ kubectl get configmaps app-config -o yaml
apiVersion: v1
data:
  COLLECTION_NAME: messages
  DB_NAME: testdb
kind: ConfigMap

3. Create a Pod that mounts the ConfigMap using a volume:

cat << 'EOF' > pod-configmap.yaml
apiVersion: v1
kind: Pod
metadata:
  name: db 
spec:
  containers:
  - image: mongo:4.0.6
    name: mongodb
    # Mount as volume 
    volumeMounts:
    - name: config
      mountPath: /config
    ports:
    - containerPort: 27017
      protocol: TCP
  volumes:
  - name: config
    # Declare the configMap to use for the volume
    configMap:
      name: app-config
EOF
kubectl create -f pod-configmap.yaml


4. List the /config directory, where the ConfigMap volume is mounted, in the container:
- The && echo is added simply to put the shell prompt onto a new line.

$ kubectl exec db -it -- ls /config
COLLECTION_NAME  DB_NAME

$ kubectl exec db -it -- cat /config/DB_NAME && echo
testdb

$ kubectl exec db -it -- cat /config/COLLECTION_NAME && echo
messages






=============================================================================

Secrets

=============================================================================
- Kubernetes provides Secrets as a way of storing sensitive data, such as passwords or API keys. Secrets reduce the risk of accidental exposure compared to if they were stored in an image or put in a Pod specification. Kubernetes supports storing Secrets as generic key-value pairs and also provides specialized support for TLS certificate secrets and Docker registry secrets used for pulling images from registries that require authentication. This lab step focuses on the generic variety of Secrets.

- Secrets are very similar to ConfigMaps with the main difference being their intent, i.e. Secrets store sensitive information, and ConfigMaps should store configuration data. Secrets are not encrypted at rest by default and are instead only base-64 encoded. However, Kubernetes can separately control access to ConfigMaps and Secrets. So by following the pattern of storing sensitive data in Secrets, users of the cluster can be denied access to Secrets but granted access to ConfigMaps using Kubernetes access control mechanisms.



mysql.connector.connect(host="mysql", database="mysql", user="root", password="passwd")

- the code above is hard coding all the db connection details. we can move them to configMap but configMap store the key/value in plain text which is not good for storing passwords.

config-map-definition.yaml
---------------------------
apiVersion: v1
kind: ConfigMap
metadata:
	name: app-config
data:
	DB_host: mysql
	DB_user: root
	DB_password: passwd
	

- this where secret is useful, store password like sensitive information. similar to configMap except they ate store in encoded or hashed format.
- similar to configMaps, create secret and inject into pod

Create secret using command inline:
---------------------------------------
- kubectl create secret generic <secret_name> --from-literal=<key>=<value>
ex:
- kubectl create secret generic app-secret --from-literal=DB_password=passwd --from-literal=DB_host=mysql --from-literal=DB_user=root


Create secret using command from file:
------------------------------------------
- kubectl create secret generic <secret_name> --from-file=<file_path>

ex:
- kubectl create secret generic app-secret --from-file=/opt/app_secret.properties

- using from-file, app_secret.properties will be the name of the key and value is file content.

- You can optionally set the key name using --from-file=[key=]source

$ kubectl create secret generic db-user-pass \
  --from-file=username=./username.txt \
  --from-file=password=./password.txt

- do not need to escape special characters ($, \, *, =, and !) in password strings that you include in a file. BUT require escaping while using --from-literal=<key>=<value> and surround it with single quotes (') i.e. \ before * char

$ kubectl create secret generic db-user-pass \
  --from-literal=username=devuser \
  --from-literal=password='S!B\*d$zDsb='


- Also, when using imperative kubectl way, the strings gets automatically encoded with base64, the value does not get displayed during kubectl describe but kubectl get secrets <secret_name> -o yaml shows the secret in base64 encoded. while used inside a pod as env and if we print the env value, it will be dsplayed in plain text.

$ kubectl create secret generic test-sec --from-literal=username=abc --from-literal=password=xyz
secret/test-sec created

$ kubectl describe secrets test-sec 
Name:         test-sec
Namespace:    default
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
password:  3 bytes
username:  3 bytes

$ kubectl get secrets test-sec -o yaml
apiVersion: v1
data:
  password: eHl6
  username: YWJj
kind: Secret
metadata:


pod.yaml
--------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-test
  name: nginx-test
spec:
  containers:
  - image: nginx:alpine
    name: nginx-test
    ports:
    - containerPort: 8080
    env:
	  - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: test-sec
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: test-sec
            key: password
  restartPolicy: Always


$ kubectl create -f pod.yaml 
pod/nginx-test created

$ kubectl exec -it nginx-test -- /bin/sh

/ # echo $SECRET_USERNAME
abc
/ # echo $SECRET_PASSWORD
xyz



$ kubectl exec -it nginx-test -- /bin/sh
/ # /bin/sh -c env



secret.yaml (data instead of spec)
----------------
apiVersion: v1
kind: Secret
metadata: 
	name: app-secret
data:
	DB_host: mysql
	DB_user: root
	DB_password: passwd
	
- kubectl create -f secret.yaml

- now these secrets in manifest file are in plantext, hence we should hash these secrets from plan text to an encoded format and then add into the manifest file like below:

$ echo -n "mysql" | base64
bC1hsjk=

$ echo -n "root" | base64
cm9vD=

$ echo -n "passwd" | base64
cGFzagh

secret-hashed.yaml (data instead of spec)
----------------
apiVersion: v1
kind: Secret
metadata: 
	name: app-secret
data:
	DB_host: bC1hsjk=
	DB_user: cm9vD=
	DB_password: cGFzagh

- use these configMap names while associating to pods


- keubectl get secrets 

- kubectl describe secrets  // shows the name of the secrets but hides the values.

- kubectl get secret <secret_name> -o yaml // this shows the manifest file along with the secret values (hashed if provided in hashed format else in plain text)

$ echo -n "bC1hsjk=" | base64 --decode
mysql


- Create TLS secret named webhook-server-tls for ssl certificates.

$ kubectl -n webhook-demo create secret tls webhook-server-tls \
	--cert "/root/keys/webhook-server-tls.crt" \
	--key "/root/keys/webhook-server-tls.key"
	
secret/webhook-server-tls created


Secrets in PODs
---------------------------------
secret-hashed.yaml (data instead of spec)
----------------
apiVersion: v1
kind: Secret
metadata: 
	name: app-secret
data:
	DB_host: bC1hsjk=
	DB_user: cm9vD=
	DB_password: cGFzagh
	
	
pod-definition.yaml
--------------------
apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp-color
spec:
	containers:
	- name: simple-webapp-color
	  image: simple-webapp-color
	  ports:
	  - containerPort: 8080
	  envFrom:
		- secretRef:
			name: app-secret // inject a specific secret as env variables into the pod. 


- we can inject specific secret values as env variables from the earlier created configMap object:
env:
	- name: DB_password
	  valueFrom:
		secretKeyRef:
			name: app-secret
			key: DB_password
			

- we can also inject the secret data as a file in volume:
volumes:
	- name: app-secret-volume
	  secret: 
		secretName: app-secret
		
- if you want to mount the secret as a volume in the pod, each attribute in the secret is create as a file with value of the secret as its content.

- ls /opt/app-secret-volumes  // 3 files are created for each secret keys.
DB_Host		DB_password		DB_user

- cat /opt/app-secret-volumes/DB_password   // content of these files has the secret value.
passwd

IMP ***** note of Secret
--------------------------
- Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered as not very safe.
- Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:
	- Not checking-in secret object definition files to source code repositories.
	- Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD. 
	
- Also the way kubernetes handles secrets. Such as:
	- A secret is only sent to a node if a pod on that node requires it.
	- Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.
	- Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.
	
	

CoudAcademy Exercise:
-----------------------
$ kubectl create ns secrets
$ kubectl config set-context --current --namespace=secrets


$ kubectl create secret generic app-secret --from-literal=password=123457

$ kubectl get secrets app-secret 
NAME         TYPE     DATA   AGE
app-secret   Opaque   1      16s


$ kubectl get secrets app-secret -o yaml
apiVersion: v1
data:
  password: MTIzNDU3
kind: Secret
..
..

- The data field holds all of the key-value pairs. In this case, there is only one. The key password appears as expected, but the value (MTIzNDU3) is far from "123457". That is because secret values are base-64 encoded. 


- *** Note: When you use kubectl create secret, the value is automatically encoded. If you use kubectl create -f, and specify a resource file, you need to encode the value yourself when setting the data: mapping. See the next instruction for how to achieve this. Alternatively, you can set a stringData: mapping instead which will perform the encoding for you. 


- Confirm the secret value is base-64 encoded by decoding it:


- Create a Pod that uses the Secret through an environment variable:
- When using a secret through an environment variable, you must include valueFrom.secretKeyRef to specify the source of the environment variable.

cat << EOF > pod-secret.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod-secret
spec:
  containers:
  - image: busybox:1.30.1
    name: busybox
    args:
    - sleep
    - "3600"
    env:
    - name: PASSWORD      # Name of environment variable
      valueFrom:
        secretKeyRef:
          name: app-secret  # Name of secret
          key: password     # Name of secret key
EOF

kubectl create -f pod-secret.yaml



- Print the value of the environment variable in the Pod's container:
- *** Notice that the value is base-64 decoded automatically, so there is no need to use base64 --decode inside the container.

$ kubectl exec -it pod-secret busybox -- /bin/sh -c 'echo $PASSWORD' 

123457










=============================================================================

Seurity Context

=============================================================================

Seurity Context in Docker
--------------------------
- We have learned that unlike virtual machines, containers are not completely isolated  from their host. Containers and the hosts share the same kernel. Containers are isolated using namespaces in Linux. The host has a namespace and the containers have their own namespace. All the processes run by the containers are in fact run on the host itself, but in their own namespaces.

$ docker run ubuntu sleep 3600

- So when you list the processes from within the docker container you see the sleep process with a process ID of 1. 

$ ps aux
USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND
root 1 0.0 0.0 4528 828 ? Ss 03:06 0:00 sleep 3600

- For the docker host, all processes of its own as well as those in the child namespaces are visible as just another process in the system. So wen you list the processes on the host you see a list of processes including the sleep command, but with a different process ID. This is because the processes can have different process IDs in different namespaces and that’s how Docker isolates containers within a system. So that’s process isolation.

$ ps aux
USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND
project 3720 0.1 0.1 95500 4916 ? R 06:06 0:00 sshd: project@pts/0
project 3725 0.0 0.1 95196 4132 ? S 06:06 0:00 sshd: project@notty
project 3727 0.2 0.1 21352 5340 pts/0 Ss 06:06 0:00 -bash
root 3802 0.0 0.0 8924 3616 ? Sl 06:06 0:00 docker-containershim -namespace m
root 3816 1.0 0.0 4528 828 ? Ss 06:06 0:00 sleep 3600


Security - Users
--------------------
- The docker host has a set of users, a root user as well as a number of non-root users. By default docker runs processes 
within containers as the root user. This can be seen in the output of the commands we ran earlier. Both within the container and outside the container on the host, the process is run as the root user. 
	- Now if you do not want the process within the container to run as the root user, you may set the user using the user option with the docker run command and specify the new user ID. 

$ docker run --user=1001 ubuntu sleep 3600

$ ps aux
USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND
1001 1 0.0 0.0 4528 828 ? Ss 03:06 0:00 sleep 3600


- Another way to enforce user security is to have this defined in the Docker image itself at the time of creation.

Docker file:
FROM ubuntu
USER 1001

$ docker build –t my-ubuntu-image 

$ docker run my-ubuntu-image sleep 3600

$ ps aux
USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND
1001 1 0.0 0.0 4528 828 ? Ss 03:06 0:00 sleep 3600


- Now What happens when you run containers as the root user? Is the root user within the container the same as the root user on the host? Can the process inside the container do anything that the root user can do on the system? If so isn’t that dangerous? Well, docker implements a set of security features that limits the abilities of the root user within the container. So the root user within the container isn’t really like the root user on the host. 

	- Docker uses Linux Capabilities to implement this. ex of linux capabilities are like: CHOWN, DAC, KILL SETFCAP, SETPCAP, SETGID, SETUID, NET_BIND, NEW_RAW, MAC_ADMIN, BROADCAST, NET_ADMIN, SYS_ADMIN, SYS_CHROOT, etc..
	
	- the root user is the most powerful user on a system. The root user can literally do anything. And so does 
a process run by the root user. It has unrestricted access to the system. From modifying files and permissions on files, Access Control, creating or killing processes, setting group id or user ID, performing network related operations such as binding to network ports, broadcasting on a network, controlling network ports; system related operations like rebooting the host, manipulating system clock and many more. All of these are the different capabilities on a Linux system and you can see a full list at this location. You can now control and limit what capabilities are made available to a process.

	- By default Docker runs a container with a limited set of capabilities. And so the processes running within the container do not have the privileges to say, reboot the host or perform operations that can disrupt the host or other containers running on 
the same host.
		- override this behavior and provide additional privileges than what is available use the cap-add option in the docker run command. 
		$ docker run --cap-add MAC_ADMIN ubuntu
		
		- Similarly you can drop privileges as well using the cap drop option.
		$ docker run --cap-drop KILL ubuntu
		
		- to run the container with all privileges enabled, use the privileged flag.
		$ docker run --privileged ubuntu
		
		
	
Seurity Context in Kubernetes
-------------------------------
$ docker run --user=1001 ubuntu sleep 3600
$ docker run --cap-add MAC_ADMIN ubuntu

-  in Kubernetes containers are encapsulated in PODs. You may chose to configure the security settings at a container level or at a POD level.

- If you configure it at a POD level, the settings will carry over to all the containers within the POD. If you configure it at both the POD and the Container, the settings on the container will override the settings on the POD. 

sc-pod-definition.yaml
------------------
apiVersion: v1
kind: Pod
metadata:
	name: web-pod
spec:
	securityContext:
		runAsUser: 1000
	containers:
	- name: ubuntu
	  image: ubuntu
	  command: ["sleep", "3600"]


sc-container-definition.yaml
----------------------------
apiVersion: v1
kind: Pod
metadata:
	name: web-pod
spec:
	securityContext:
	  runAsUser: 1000
	containers:
	- name: ubuntu
	  image: ubuntu
	  command: ["sleep", "3600"]
	  securityContext:
		capabilities:
			add: ["MAC_ADMIN"]
		
- to add more: 	 add: ["MAC_ADMIN", "SYS_TIME"]




Exercise:
-----------

1. Create the following Pod manifest file:

cat << EOF > pod-no-security-context.yaml
apiVersion: v1
kind: Pod
metadata:
  name: security-context-test-1
spec:
  containers:
  - image: busybox:1.30.1
    name: busybox
    args:
    - sleep
    - "3600"
EOF


$ kubectl create -f pod-no-security-context.yaml


- There are only a minimal number of devices available in the container and none that can do any harm. 

$ kubectl exec security-context-test-1 -it -- ls /dev

core             null             shm              termination-log
fd               ptmx             stderr           tty
full             pts              stdin            urandom
mqueue           random           stdout           zero



2. Delete the previous Pod and create a similar Pod that has a privileged container:

$ kubectl delete -f pod-no-security-context.yaml

$ cat > pod-privileged.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: security-context-test-2
spec:
  containers:
  - image: busybox:1.30.1
    name: busybox
    args:
    - sleep
    - "3600"
    securityContext:
      privileged: true
EOF

$ kubectl create -f pod-privileged.yaml


- All of the host devices are available including the host file system disk nvme0n1p1. This could be a major security breach and shows the importance of carefully considering if you should ever use a privileged container.

$ kubectl exec security-context-test-2 -it -- ls /dev

autofs           pts              tty32            tty9
btrfs-control    random           tty33            ttyS0
core             rfkill           tty34            ttyS1
cpu_dma_latency  rtc0             tty35            ttyS2
cuse             shm              tty36            ttyS3
ecryptfs         snapshot         tty37            ttyprintk
fd               stderr           tty38            udmabuf
full             stdin            tty39            uinput
fuse             stdout           tty4             urandom
hpet             termination-log  tty40            vcs
..
..
..
..



3. Create another pod that includes a Pod security context as well as a container security context:

- The Pod security context enforces that container processes do not run as root (runAsNonRoot) and sets the user ID of the container process to 1000. The container securityContext sets the container process' user ID to 2000 and sets the root file system to read-only.

cat << EOF > pod-runas.yaml
apiVersion: v1
kind: Pod
metadata:
  name: security-context-test-3
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
  containers:
  - image: busybox:1.30.1
    name: busybox
    args:
    - sleep
    - "3600"
    securityContext:
      runAsUser: 2000
      readOnlyRootFilesystem: true
EOF

$ kubectl create -f pod-runas.yaml



- Open a shell in the container:

$ kubectl exec security-context-test-3 -it -- /bin/sh
/ $ 


- *** Notice that the shell prompt is $ and not # indicating that you are not the root user.


- List the running processes in the container:
$ ps
PID   USER     TIME  COMMAND
    1 2000      0:00 sleep 3600
    7 2000      0:00 /bin/sh
   14 2000      0:00 ps


- The USER ID is 2000 illustrating that it is not root and that the container security context overrides the setting in the Pod security context when both security contexts include the same field. Whenever possible you should not run as root.



- Attempt to create a file in the /tmp directory:

$ touch /tmp/test-file

touch: /tmp/test-file: Read-only file system



- The attempt fails due to the Read-only file system. When possible, it is best to use read-only root file systems to harden your container environments. A best practice is to use volumes to mount any files that require modification, allowing the root file system to be read-only.





====================================================================

Service Accounts

====================================================================

- Kubernetes uses ServiceAccounts as a mechanism for providing Pods with an identity in the cluster. Pods can authenticate using ServiceAccounts and gain access to APIs that the ServiceAccount has been granted. Your cluster administrator can create specific roles that grant access to APIs and bind the roles to ServiceAccounts. This is referred to as role-based access control (RBAC). Pods can then declare a ServiceAccount in their specification to gain the access associated with the ServiceAccount's role. As an example, you could use a ServiceAccount to grant a Pod access to the GET Pod API to allow the Pod to get the details of other Pods. This lab step focuses on ServiceAccounts and not the roles that are used to grant access to Kubernetes APIs that would be configured by a Kubernetes administrator.


- The concept of service accounts is linked to other security related concepts in kubernetes such as Authentication, Authorization, Role based access controls etc. 

- two types of accounts in Kubernetes. A user account and a service account. 
	- A user account could be for an administrator accessing the cluster to perform administrative tasks, a developer accessing the 
	cluster to deploy applications etc.
	- A service account, could be an account used by an application to interact with the kubernetes cluster.  
	For ex:, 1/ a monitoring application like Prometheus uses a service account to poll the kubernetes API for performance metrics, and 2/ An automated build tool like Jenkins uses service accounts to deploy applications on the kubernetes cluster. 

$ kubectl create serviceaccount dashboard-sa
serviceaccount “dashboard-sa” created

$ kubectl get serviceaccount
NAME SECRETS AGE
default 1 218d
dashboard-sa 1 4d

$ kubectl describe serviceaccount dashboard-sa
Name: dashboard-sa
Namespace: default
Labels: <none>
Annotations: <none>
Image pull secrets: <none>
Mountable secrets: dashboard-sa-token-kbbdm
Tokens: dashboard-sa-token-kbbdm
Events: <none>

- When the service account is created, it also creates a token automatically. The service account token is what must be used by the external application while authenticating to the Kubernetes API. The token, however, is stored as a secret object. In this case its named dashboard-sa-token-kbbdm.

- So when a service account is created, it first creates the service account object and then generates a token for the service account. It then creates a secret object and stores that token inside the secret object. The secret object is then linked to the service account. To view the token, view the secret object by running the command kubectl describe secret. 

$ kubectl describe serviceaccount dashboard-sa
Name: dashboard-sa
Namespace: default
Labels: <none>
Annotations: <none>
Image pull secrets: <none>
Mountable secrets: dashboard-sa-token-kbbdm
Tokens: dashboard-sa-token-kbbdm
Events: <none>


$ kubectl describe secret dashboard-sa-token-kbbdm
Name: dashboard-sa-token-kbbdm
Namespace: default
Labels: <none>
Type: kubernetes.io/service-account-token
Data
====
ca.crt: 1025 bytes
namespace: 7 bytes
token: 
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL
3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3V
udC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3

- This token can then be used as an authentication bearer token while making a rest call to the kubernetes API.
$ curl https://192.168.56.70:6443/api --header "Authorization: Bearer <paste the above token>"

- You can create a service account, assign the right permissions using Role based access control mechanisms 
(which is out of scope for this course) and export your service account tokens and use it to configure your third party application to authenticate to the kubernetes API.

	- if your third party application is hosted on the kubernetes cluster itself. For example, we can have our custom-kubernetes-dashboard or the Prometheus application used to monitor kubernetes, deployed on the kubernetes cluster itself. i.e. mounting the service token secret as a volume inside the POD hosting the third party application. the token to access the kubernetes API is already placed inside the POD and can be easily read by the application.

$ kubectl get serviceaccount
NAME SECRETS AGE
default 1 218d
dashboard-sa 1 4d

$ kubectl describe serviceaccount default
Name:                default
Namespace:           default
Labels:              <none>
Annotations:         <none>
Image pull secrets:  <none>
Mountable secrets:   default-token-rspjd
Tokens:              default-token-rspjd
Events:              <none>

pod-definition.yml
-------------------
apiVersion: vi
kind: Pod
metadata: 
	name: my-kubernetes-dashboard
spec:
	containers:
		- name: my-kubernetes-dashboard
		  image: my-kubernetes-dashboard
	  
	  
$ kubectl describe pod my-kubernetes-dashboard
Name: my-kubernetes-dashboard
Namespace: default
Annotations: <none>
Status: Running
IP: 10.244.0.15
Containers:
	nginx:
		Image: my-kubernetes-dashboard
	Mounts:
		/var/run/secrets/kubernetes.io/serviceaccount from default-token-j4hkv (ro)
Conditions:
Type Status
Volumes:
default-token-j4hkv:
Type: Secret (a volume populated by a Secret)
SecretName: default-token-j4hkv     // default service account that exists already.
Optional: false

	  
- For every namespace in kubernetes a service account named default is automatically created. Each namespace has its own 
default service account. Whenever a POD is created the default service account and its token are automatically mounted to that POD as a volume mount.  a volume is automatically created from the secret named default-token-j4hkv, which is in fact the secret containing the token for the default service account. The secret token is mounted at location /var/run/secrets/kubernetes.io/serviceaccount inside the pod. 

- the secret mounted as 3 separate files. The one with the actual token is the file named token.
$ kubectl exec -it my-kubernetes-dashboard ls /var/run/secrets/kubernetes.io/serviceaccount
ca.crt namespace token

- Now remember that the default service account is very much restricted. It only has permission to run basic kubernetes API queries.

$ kubectl exec -it my-kubernetes-dashboard cat /var/run/secrets/kubernetes.io/serviceaccount/token
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3V
udC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tajRoa3Y
iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWF
jY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjcxZGM4YWExLTU2MGMtMTFlOC04YmI0LTA4MDAyNzkzMTA3MiIsInN1YiI6InN5c3RlbTpzZXJ2


- Remember, you cannot edit the service account of an existing pod, so you must delete and re-create the pod. However in case of a 
deployment, you will be able to edit the serviceAccount, as any changes to the pod definition will automatically trigger a new roll-out for the deployment.

pod-definition.yml
-------------------
apiVersion: v1
kind: Pod
metadata:
	name: my-kubernetes-dashboard
spec:
	serviceAccount: dashboard-sa
	containers:
		- name: my-kubernetes-dashboard
		  image: my-kubernetes-dashboard
	

$ kubectl describe pod my-kubernetes-dashboard
Name: my-kubernetes-dashboard
Namespace: default
Annotations: <none>
Status: Running
IP: 10.244.0.15
Containers:
	nginx:
		Image: my-kubernetes-dashboard
Mounts: /var/run/secrets/kubernetes.io/serviceaccount from dashboard-sa-token-kbbdm (ro)


- So remember, kubernetes automatically mounts the default service account if you haven’t explicitly specified any. You may choose not to mount a service account automatically by setting the automountServiceAccountToken: false in the POD spec section. 



pod-definition.yml
-------------------
apiVersion: v1
kind: Pod
metadata:
	name: my-kubernetes-dashboard
spec:
	automountServiceAccountToken: false
	containers:
		- name: my-kubernetes-dashboard
		  image: my-kubernetes-dashboard
	
	
deployment-def.yaml
--------------------
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-deployment
spec:
  template:
    # Below is the podSpec.
    metadata:
      name: ...
    spec:
      serviceAccountName: dashboard-sa
      automountServiceAccountToken: false
      ...
	  


CoundAcademy Exercise:
-----------------------
# Create namespace
kubectl create namespace serviceaccounts
# Set namespace as the default for the current context
kubectl config set-context $(kubectl config current-context) --namespace=serviceaccounts



- Each Namespace has a default ServiceAccount. The default ServiceAccount grants minimal access to APIs and cannot be used to get any cluster state information. Therefore, you should use custom ServiceAccounts when your application requires access to cluster state.

$ kubectl get serviceaccount
NAME      SECRETS   AGE
default   1         29s



kubectl run default-pod --image=mongo:4.0.6
kubectl get pod default-pod -o yaml | more

serviceAccount: default
serviceAccountName: default


- It is a best practice to create a ServiceAccount for each of your applications to use the least amount of access necessary (principle of least privilege) to improve security. The created ServiceAccount will not have any specific role bound to it so there are no additional permissions associated with it. 
- In practice, your Kubernetes administrator would create a role and bind it to the ServiceAccount. 

$ kubectl create serviceaccount app-sa



- Create a new Pod that uses the custom ServiceAccount:

$ kubectl run default-pod --image=mongo:4.0.6 --serviceaccount=app-sa --dry-run=client -o yaml

Or

cat << 'EOF' > pod-custom-sa.yaml
apiVersion: v1
kind: Pod
metadata:
  name: custom-sa-pod 
spec:
  containers:
  - image: mongo:4.0.6
    name: mongodb
  serviceAccount: app-sa
EOF
kubectl create -f pod-custom-sa.yaml



$ kubectl get pod custom-sa-pod -o yaml | more

..
..
serviceAccount: app-sa
serviceAccountName: app-sa
..
..


- Every ServiceAccount has a corresponding token secret (see them with kubectl get secrets) that can be used to authenticate requests from inside the container. The ServiceAccount's token secret is automatically mounted as a volume. That is what you see in the volumeMounts configuration.


volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: app-sa-token-q5n4w
      readOnly: true

volumes:
  - name: app-sa-token-q5n4w
    secret:
      defaultMode: 420
      secretName: app-sa-token-q5n4w
	  












==============================================================

Resources

==============================================================

https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/


- It is a best practice to include some information about the compute resource requirements for a Pod in its specification. The compute resources that Kubernetes includes built-in support for are CPU, measured in cores, and memory, measured in bytes. Kubernetes allows you to include a requested amount (minimum) and a limit (maximum) for each compute resource. Although the compute resource requests and limits are optional, the Kubernetes scheduler can make better decisions about which node to schedule a Pod on if it knows the Pod's resource requirement. The scheduler will only schedule a Pod on a node if the node has enough resources available to satisfy the Pod's total request, which is the sum of the Pod's containers' requests. Limits also reduce resource contention on a node and can make performance more predictable.

- Each node has a set of CPU, Memory and Disk resources available. Every POD consumes a set of resources
- Whenever a POD is placed on a Node, it consumes resources available to that node.

- If the node has no sufficient resources, the scheduler avoids placing the POD on that node, Instead places the POD on one were sufficient resources are available.

- If there is no sufficient resources available on any of the nodes, Kubernetes holds back scheduling the POD, and you will see the POD in a pending state. If you look at the events, you will see the reason – insufficient cpu.

- By default, kubernetes assumes that a POD or a container within a POD requires .5 CPU & 256 Mebibyte of memory. The minimum amount of CPU or Memory requested by the container. 
- For the POD to pick up those defaults you must have first set those as default values for request and limit by creating a LimitRange in that namespace.



mem-limit-range-definition.yaml
-------------------------------
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container

cpu-limit-range-definition.yaml
-------------------------------
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container


	- When the scheduler tries to place the POD on a Node, it uses these numbers to identify a Node which has sufficient amount of resources available. Now, if you know that your application will need more than these, you can modify these 
values, by specifying them in your POD or deployment definition files. 
	- You can specify any value as low as 0.1. 0.1 CPU can also be expressed as 100m were m stands for milli. 
	- 1 count of CPU is equivalent to 1 vCPU. That’s 1 vCPU in AWS, or 1 Core in GCP or Azure or 1 Hyperthread.
	
1 G (Gigabyte) = 1,000,000,000 bytes
1 M (Megabyte) = 1,000,000 bytes
1 K (Kilobyte) = 1,000 bytes

1 Gi (Gibibyte) = 1,073,741,824 bytes
1 Mi (Mebibyte) = 1,048,576 bytes
1 Ki (Kibibyte) = 1,024 bytes


- G is Gigabyte and it refers to a 1000 Megabytes, whereas Gi refers to Gibibyte and refers to 1024 Mebibyte. 

-  In the Docker world, a docker container has no limit to the resources it can consume on a Node. Say a container 
starts with 1 vCPU on a Node, it can go up and consume as much resource as it requires, suffocating the native processes on the node or other containers of resources. 
	- However, you can set a limit for the resource usage on these PODs. By default Kubernetes sets a limit of 1vCPU to containers. So if you do not specify explicitly, a container will be limited to consume only 1 vCPU from the Node.
	-  By default, kubernetes sets a limit of 512 Mebibyte on containers.
	
pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp-color
labels:
	name: simple-webapp-color
	spec:
	containers:
		- name: simple-webapp-color
		  image: simple-webapp-color
		  ports:
			- containerPort: 8080
		resources:
			requests:
				memory: "1Gi"
				cpu: 1
			limits:
				memory: "2Gi"
				cpu: 2

- the limits and requests are set for each container. 

- So what happens when a pod tries to exceed resources beyond its specified limit. In case of the CPU, kubernetes throttles the CPU so that it does not go beyond the specified limit. A container cannot use more CPU resources than its limit. However, this is not the case with memory. A container CAN use more memory resources that its limit. So if a pod tries to consume more memory than its limit constantly, the POD will be terminated.


https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/

$ kubectl create namespace constraints-cpu-example


cpu-constraints.yaml
--------------------
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-min-max-demo-lr
spec:
  limits:
  - max:
      cpu: "800m"
    min:
      cpu: "200m"
    type: Container

$ kubectl apply -f cpu-constraints.yaml --namespace=constraints-cpu-example


$ kubectl get limitrange cpu-min-max-demo-lr -o yaml -n constraints-cpu-example

limits:
- default:
    cpu: 800m
  defaultRequest:
    cpu: 800m
  max:
    cpu: 800m
  min:
    cpu: 200m
  type: Container
  
  
Now whenever you create a Pod in the constraints-cpu-example namespace (or some other client of the Kubernetes API creates an equivalent Pod), Kubernetes performs these steps:
- If any container in that Pod does not specify its own CPU request and limit, the control plane (LimitRanger admission controller) assigns the default CPU request and limit to that container.
- Verify that every container in that Pod specifies a CPU request that is greater than or equal to 200 millicpu.
- Verify that every container in that Pod specifies a CPU limit that is less than or equal to 800 millicpu.



admin-resource-cpu-constraints-pod-yaml
----------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: constraints-cpu-demo
spec:
  containers:
  - name: constraints-cpu-demo-ctr
    image: nginx
    resources:
      limits:
        cpu: "800m"
      requests:
        cpu: "500m"



Exercise:
--------------
1. Create a Pod manifest for a Pod that will consume a lot of CPU resources:

load.yaml
---------
apiVersion: v1
kind: Pod
metadata:
  name: load
spec:
  containers:
  - name: cpu-load
    image: cloudacademydevops/stress
    args:
    - -cpus
    - "2"


- The args instruct stress to attempt to consume two CPU cores, which is how many cores each node in the cluster has.

$ kubectl apply -f load.yaml


- The top command is similar to the native Linux top command for measuring CPU and memory usage of processes. You can monitor at the node or pod level.

- List the resource consumption of pods:
- The load pod is using nearly two full cores (1930milli-cores). Having such an unrestricted pod in your cluster could wreak havoc.

$ kubectl top pods

NAME   CPU(cores)   MEMORY(bytes)   
load   1952m        1Mi


$ kubectl get po -o wide

NAME   READY   STATUS    RESTARTS   AGE     IP               NODE                                      NOMINATED NODE   READINESS GATES
load   1/1     Running   0          2m25s   192.168.23.129   ip-10-0-0-11.us-west-2.compute.internal   <none>           <none>


$ kubectl top node

NAME                                       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-10-0-0-10.us-west-2.compute.internal    61m          3%     527Mi           13%       
ip-10-0-0-100.us-west-2.compute.internal   180m         9%     1080Mi          28%       
ip-10-0-0-11.us-west-2.compute.internal    1999m        99%    619Mi           16%


2. Create a similar Pod specification except with CPU and memory resource limits and requests:

load-limited.yaml
-----------------
apiVersion: v1
kind: Pod
metadata:
  name: load-limited
spec:
  containers:
  - name: cpu-load-limited
    image: cloudacademydevops/stress
    args:
    - -cpus
    - "2"
    resources:
      limits:
        cpu: "0.5" # half a core
        memory: "20Mi" # 20 mebibytes 
      requests:
        cpu: "0.35" # 35% of a core
        memory: "10Mi" # 10 mebibytes


- The Pod will only be scheduled on a Node with 0.35 CPU cores and 10MiB of memory available. 

- **** It's important to note that the scheduler doesn't consider the actual resource utilization of the node. Rather, it bases its decision upon the sum of container resource requests on the node. For example, if a container requests all the CPU of a node but is actually 0% CPU, the scheduler would treat the node as not having any CPU available. In the context of this lab, the load Pod is consuming 2 CPUs on a Node but because it didn't make any request for the CPU, its usage doesn't impact following scheduling requests.


$ kubectl apply -f load-limited.yaml


- In this case the load-limited Pod is scheduled on the Node (NODE) that isn't running the load Pod. However, this is only by chance since the load Pod did not make any request for the CPU it is using.

$ kubectl get pod -o wide

NAME           READY   STATUS    RESTARTS   AGE   IP               NODE                                      NOMINATED NODE   READINESS GATES
load           1/1     Running   0          16m   192.168.23.129   ip-10-0-0-11.us-west-2.compute.internal   <none>           <none>
load-limited   1/1     Running   0          24s   192.168.23.130   ip-10-0-0-11.us-west-2.compute.internal   <none>           <none>



- In this case, the load-limited Pod is using near half a CPU as specified by its limit. However, the load Pod is now using roughly half a CPU less because it had to give up CPU resources for the load-limited Pod.

$ kubectl top pods

NAME           CPU(cores)   MEMORY(bytes)   
load           1319m        1Mi             
load-limited   503m         1Mi



3. Update the load Pod manifest to include a request for 1.7 CPU:

- The value of 1.7 is chosen because there may not be a Node with a full 2 CPUs available considering some system Pods also make requests in addition to the Pods you create.

cat << 'EOF' > load.yaml
apiVersion: v1
kind: Pod
metadata:
  name: load
spec:
  containers:
  - name: cpu-load
    image: cloudacademydevops/stress
    args:
    - -cpus
    - "2"
    resources:
      requests:
        cpu: "1.7"
EOF


- Apply the new Pod manifest:
- The apply command cannot be used without the --force option case because updates are not allowed to update container resources. The --force option first deletes the Pod and then creates a new one.

$ kubectl apply --force -f load.yaml



- Confirm the Pods are running on separate Nodes:
- It is now guaranteed that the Pods won't be on the same Node because their combined CPU request is 2.1 and the Nodes only have 2 CPUs.

$ kubectl get pods -o wide

NAME           READY   STATUS    RESTARTS   AGE     IP                NODE                                      NOMINATED NODE   READINESS GATES
load           1/1     Running   0          24s     192.168.203.129   ip-10-0-0-10.us-west-2.compute.internal   <none>           <none>
load-limited   1/1     Running   0          3m54s   192.168.23.130    ip-10-0-0-11.us-west-2.compute.internal   <none>           <none>










===================================================================================
	
Taints and Tolerations

===================================================================================

- This is about the Pod to Node relationships and how to restrict what pod to get scheduled on what nodes. this is used to restrict on what pods can be scheduled on specific nodes.

- when the pods are created, Kubernetes tries to schedule them on the available nodes. by default, there are not restrictions or limitations and as such the scheduler places the pods across all of the nodes to balance them out equaly.

- In case we have a dedicated resources allocated to some specific nodes for some high capacity usage. we would like only those pods to be scheduled on these nodes,
	- first add a taint to those nodes for restrict all pod getting scheduled on them. ex: taint=blue
	- by default, pods wont have any toleration.
	- add the same toleration to specific pods to be scheduled on those specific nodes.
	- when Scheduler trying to schedule other pods to these taint nodes, the pods get rejected due to the taint. hence scheduler schedules them on other nodes.

Taints - Node
--------------
- there are 3 types of <taint-effect>, i.e. NoSchedule | PreferNoSchedule | NoExecute -- these decides what happens to the PODs that do not have stamped with same tolerence as same as the Node.
	- NoSchedule -- the PODs without tolerence wont be scheduled on the taint nodes.
	- PreferNoSchedule -- the system will try to avoid placing the PODs without tolerence on the taint nodes. i.e. not guranteed.
	- NoExecute  --  New PODs without tolerence wont be scheduled and exsiting PODs (may have been scheduled before the taint applied to the nodes) without tolerence stamped woll be evicted from the node.
	
- add tarint to a node
$ kubectl taint nodes <node-name> <key>=<value>:<taint-effect>

ex: 
$ kubectl taint nodes node1 app=myapp:NoSchedule

$ kubectl taint nodes node01 spray=mortein:NoSchedule
node/node01 tainted

- places a taint on node node1. The taint has key key1, value value1, and taint effect NoSchedule. This means that no pod will be able to schedule onto node1 unless it has a matching toleration.


- remove taint from a node
$ kubectl taint nodes <node-name> <taint-name>-   // - symbol at the end
$ kubectl taint nodes node1 key1=value1:NoSchedule-



ex:
$ kubectl describe node controlplane | grep -i taint
Taints:             node-role.kubernetes.io/master:NoSchedule

$ kubectl taint node controlplane node-role.kubernetes.io/master:NoSchedule-
node/controlplane untainted

$ kubectl taint nodes node01 node.kubernetes.io/disk-pressure:NoSchedule-
node/node01 untainted



Tolerations - Pod
-----------------
- toleration are added to pods.

$ kubectl taint nodes node1 app=myapp:NoSchedule

pod-definition.yml
-------------------
apiVersion: 
kind: Pod
metadata:
	name: myapp-pod
spec:
	containers:
	- name: nginx-container
	  image: nginx
	tolerations:
	- key: app
	  operator: Equal
	  value: myapp
	  effect: NoSchedule

bee-pod.yaml
------------
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    operator: Equal
    value: mortein
	effect: NoSchedule



- *** Tains and Tolerence does not tell the pods to go to a particular node instead it tell the node to only accept pods with certain tolerations. if we want to restrict a pod to certain nodes, we need to use "Node Afinity"

- as we know in K8s cluster there are master (kubemaster) and worker (kubelet) nodes, master nodes are technically just like workers and have all the capabilities of hosting pods + runs the cluster management softwares. When kubernets cluster is setup, it adds a special taints to its master nodes for preventing the Scheduler not able to schedule any pods to master.

- to see these taint on master node, run the below:

$ kubectl describe node kubemaster | grep Taint
Taints: 	node-role.kubernetes.io/master:NoSchedule

$ kubectl describe node controlplane | grep Taints
Taints:     node-role.kubernetes.io/master:NoSchedule

$ kubectl describe node node01 | grep Taint
Taints:             <none>

$ kubectl explain pod --recursive | grep -A5 tolerations
tolerations       <[]Object>
	 effect <string>
	 key    <string>
	 operator       <string>
	 tolerationSeconds      <integer>
	 value  <string>
	 


- add more than one taint to nodes:
$ kubectl taint nodes node1 key1=value1:NoSchedule
$ kubectl taint nodes node1 key2=value2:NoSchedule


- add more than one tolerations to pods

tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  
  


- The tolerationSeconds parameter specifies how long a pod can remain bound to a node before being evicted. If a taint with the NoExecute effect is added to a node, any pods that do not tolerate the taint are evicted immediately (pods that do tolerate the taint are not evicted). However, if a pod that to be evicted has the tolerationSeconds parameter, the pod is not evicted until that time period expires.

tolerations:
- key: "key1" 
  operator: "Equal" 
  value: "value1" 
  effect: "NoExecute" 
  tolerationSeconds: 3600 


- Sample pod configuration file with Exists operator

tolerations:
- key: "key1"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 3600
  
  


=============================================================================

Node Selectors

=============================================================================
- by default any Pods can get scheduled to any nodes. so, even if there are few nodes with higher harware resources allocated, a POD (ex: data processor app) if it requires high resources, ends up being scheduled to any node, whcih is not desired.

- 2 ways to achive this:
	- Node Selector - simple and easire method.
	- Node Afinity - complex usecase.
	
- on Pod, add "nodeSelector" section to limit the pod to run on specific nodes (ex: nodes stamped with size=Large)

	pod-definition.yaml
	-------------------
	apiVersion: v1
	kind: Pod
	metadata:
		name: myapp-pod
	spec:
		containers:
			- name: data-processor
			  image: data-processor
		nodeSelector:
			size: Large
	
	- size=Large are labels assigned to the specific nodes, the scheduler uses these labels to match and identify the right node to place the right pods on.
	- we must have label the nodes prior to add them in nodeSelector on pod.
	
- on Nodes - add labels
$ kubectl label nodes <node-name> <label-key>=<label-value>
ex:
$ kubectl label node01 size=Large


- List the nodes in your cluster, along with their labels:
$ kubectl get nodes --show-labels

NAME      STATUS    ROLES    AGE     VERSION        LABELS
worker0   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker0
worker1   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker1
worker2   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker2



$ kubectl apply -f pod-definition.yaml

- Verify that your chosen node has a disktype=ssd label:

$ kubectl get nodes --show-labels

NAME      STATUS    ROLES    AGE     VERSION        LABELS
worker0   Ready     <none>   1d      v1.13.0        ...,disktype=ssd,kubernetes.io/hostname=worker0
worker1   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker1
worker2   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker2


- Verify that the pod is running on your chosen node:
$ kubectl get pods -o wide

NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0



Node Selector - Limitations
----------------------------
- used a single label and nodeSelector to achieve simple goal, in case of complex requirements like: a) place the Pod on a Large OR Medium nodes, b) Place the Pod on any nodes that are not small.

- These complex usecase can not achieved by Node Selector, use Node Afinity




=====================================================================================

Node Name

=====================================================================================
- Create a pod that gets scheduled to a specific node
- You can also schedule a pod to one specific node via setting nodeName.

pod-nginx-specific-node.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: foo-node # schedule pod to specific node
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent






=====================================================================================

Node Afinity

=====================================================================================
- primary goal is to ensure that pods are scheduled on particular nodes. Node Selector works unless the expression/condition is simple like matching labels ex: size=Large, but we can not provide advanced option like Or/Not expressions/conditions with 'Node Selectors'

- Node Affinity has advanced ability to limit pod placements on specific nodes. 

pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
spec:
	containers:
		- name: data-processor
		  image: data-processor
	affinity:
		nodeAffinity:
			requiredDuringSchedulingIgnoredDuringExecution:
				nodeSelectorTerms:
					- matchExpressions:
					  - key: size
					    operator: In
						values:
						- Large
						- Medium
						

- Here the 'In' operator ensures, that the pod will be placed on a node whose label size has any value with the list of values specified in Pod definition under affinity section.

- for scheduleing Pods on any nodes other than size=Small, use 'NotIn' operator, nodeAffinity match the pods with node label size not set to 'Small'.

affinity:
  nodeAffinity:
	requiredDuringSchedulingIgnoredDuringExecution:
	  nodeSelectorTerms:
	  - matchExpressions:
	    - key: size
		  operator: NotIn
		  values:
		  - Small
					

- Incase we set the node label to Large and Medium but did not have any label (size) on Small nodes, use 'Exists' operator. it will check the nodes where the label named 'size' exists, and schedule the pods only to those nodes.

- Incase we set the node label to Large and Medium but did not have any label (size) on Small nodes, use 'Exists' operator. it will check if the label named 'size' exists
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
	  nodeSelectorTerms:
	  - matchExpressions:
		- key: size
		  operator: Exists


- when the pods are created, these affinity rules are considered and the pods are placed onto the right node. 

- The type of node affinity defines the behavior of the scheduler.
	- what if the Node affnity could not match with the given expression (ex: no nodes with a specific label)?
	- What if someone changes the label on node after some of the pods were scheduled based on node affinity rule. will the pod continue to run on the same node.
	
	- there are 2 stages in the life cycle of a pod when considering node affinity. 1/ DuringScheduling 2/ DuringExecution
		- requiredDuringScheduling -- schduler mandates the pod to be scheduled on nodes with the given affinity rule, if it can't find any nodes with the same labels, pods wont be scheduled. this type is used in cases where the placement of pod is crucial, if there is no matching nodes, pods will not be scheduled. 
		
		- preferredDuringScheduling -- in case the pod placement is less important, if matching nodes are not found, the scheduler will ignore the affinity rule and place the pod on any available nodes. saying the scheduler to try best to place the pod on matching nodes (labels), if can't find one such nodes, just place it anywhere but dont keep the pod in pending state.
		
		- DuringExecution -- is the stage where the pod has been running and a change is made to the system/node (ex: changing the label of a node). ex: admin removing label (size=Large) from the node on which there are already running pods with similar match expression, as of today, there is only one type affinity available (Ignored) for DuringExecution stage means the already running pods will continue to run i.e. change to node affinity will not have any impact once the pods are scheduled.
		
			- a new option for DuringExecution stage i.e. Required to be introduced soon, to evict already running pods that do not meet affinity rule at any point in time. ex: pod running on nodes (labelled with size=Large) will be evicted/terminated if the label size=Large is removed from the node.
			
	- requiredDuringSchedulingIgnoredDuringExecution
	- preferredDuringSchedulingIgnoredDuringExecution
	- requiredDuringSchedulingRequiredDuringExecution


- show the labels on node
$ kubectl get nodes node01 --show-labels
NAME     STATUS   ROLES    AGE     VERSION   LABELS
node01   Ready    <none>   7m21s   v1.20.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,color=blue,kubernetes.io/arch=amd64,kubernetes.io/hostname=node01,kubernetes.io/os=linux

$ kubectl describe node node01

Name:               node01
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node01
                    kubernetes.io/os=linux
					
$ kubectl label nodes node01 color=blue
node/node01 labeled

- create deployment with a image
$ kubectl create deployment blue --image=nginx
deployment.apps/blue created

$ kubectl get deployments.apps 
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   1/1     1            1           2m19s

- scale out and scale in the deployment
$ kubectl scale deployment blue --replicas=6
deployment.apps/blue scaled

$ kubectl get deployments.apps 
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   6/6     6            6           3m7s


- scale out and scale in the deployment
$ kubectl scale deployment blue --replicas=3
deployment.apps/blue scaled

$ kubectl get deployments.apps 
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   3/3     3            3           3m33s


- Set Node Affinity to the deployment to place the pods on node01 only
- 1st get the running deployment to yaml file.



Taints & Tolerations vs Node Affinity
======================================================
- lets say, we have 3 nodes and 3 pods each in 3 colors, blue, red and green. the ultimate aim is to place blue pod to blue node, red pod to red node and green pod to green node.
	- we are sharing the same kubernetes cluster with other teams, so other team has their own pods and other nodes. we dont want other team pod to be placed on our nodes, neither do we want our red/green/pods to be placed on other teams nodes.
	
- solving the problem with 'Taints & Tolerations'
	- apply taint to all of our 3 nodes. i.e. 
		$ kubectl taint nodes node-red color=red:NoSchedule
		$ kubectl taint nodes node-green color=green:NoSchedule
		$ kubectl taint nodes node-blue color=blue:NoSchedule
		
	- set respective tolerations on red/green/blue pods.
		red-pod.yaml
		------------
		apiVersion: v1
		kind: Pod
		metadata:
		  name: red
		spec:
		  containers:
		  - image: nginx
			name: red
		  tolerations:
		  - key: color
			operator: Equal
			value: red
			effect: NoSchedule
			
	- when the pods are created, nodes will ensure to accept the pods with right tolerations set. so the green pod gets placed to green nodes and blue pod gets placed on blue node however, taints & tolerations does not gurantee that the pods will only prefer these nodes, so the red node ends up on one of other nodes that dont have any taint set. -- not desired.
	
	
- solving the problem with 'Node Affinity'
	- label the nodes with respective colors.
		$ kubectl label nodes node-red color=red
		$ kubectl label nodes node-green color=green
		$ kubectl label nodes node-blue color=blue
		
	- add nodeSelectors on the 3 pods to tie them to these nodes.
		pod-definition.yaml
		-------------------
		apiVersion: v1
		kind: Pod
		metadata:
		  name: red
		spec:
		  containers:
		  - name: red
		    image: nginx
		  affinity:
			nodeAffinity:
			  requiredDuringSchedulingIgnoredDuringExecution:
			    nodeSelectorTerms:
				- matchExpressions:
				  - key: color
					operator: In
					values:
					- red
								
		- hence red/green/blue ends up on respective nodes, however it does not gurantee other pods being scheduled on these red/green/blue nodes. -- not desired.
		
- *** Hence combination of both 'Taints & Tolerations' and 'Node Affinity' to be used to completely dedicate specific nodes for specific pods.
	- apply taints and tolerations on red/green/blue nodes and pods for preventing other nodes being scheduled on these nodes.
	- use node affinity to prevent our pods from being placed on other nodes. 

	




===================================================================================

Multi-Container Pods

===================================================================================
- 2 pattern of multi-container pods in kubernetes.
	- Ambassador
	- Adaptor
	- Sidecar
	
- idea of decoupling a large monolithic application into sub component known as micro-services enables us to develop and deploy a set of independent small and re-usable code. this architecture can then help us scale up/down as well as modify each service as required as against with modifying the large application.
	- however at times, we need 2 services to work together. ex: a Web server and a Logging Agent service. You need one agent per web server instance paired together. You don’t want to merge and bloat the code of the two services, as each of them target different features, and you’d still like them to be developed and deployed separately.
	- You only need the two functionality to work together. You need one agent per web server instance paired together, that can scale up and down together. And that is why you have multi-container PODs, that share the same lifecycle – which means they are created together and destroyed together.
	- They share the same network space, which means they can refer to each other as localhost. 
	- They have access to the same storage volumes.
	- This way, you do not have to establish, volume sharing or services between the PODs to enable communication between them.
	
	
pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp
labels:
	name: simple-webapp
spec:
	containers:
	- name: simple-webapp
	  image: simple-webapp
	  ports:
	  - containerPort: 8080
	
	- name: log-agent
	  image: log-agent
	  


- There are 3 common patterns.

- Side car
	- The first and what we just saw with the logging service example is known as a side car pattern.
	- A good example of a side car pattern is deploying a logging agent along side a web server to collect logs and forward them to a central log server.
	
- Adaptor
	- Building on that example, say we have multiple applications generating logs in different formats. It would be hard to process the various formats on the central logging server.
	- say container A generating logs formatted like: 12-JULY-2018 16:05:49 "GET /index1.html" 200
	container B generating logs formatted like: 12/JUL/2018:16:05:49 -0800 "GET /index2.html" 200
	container C generating logs formatted like: GET 1531411549 "/index3.html" 200
	
	- So, before sending the logs to the central server, we would like to convert the logs to a common format. For this we deploy an adapter container. The adapter container processes the logs, before sending it to the central server
	12-JULY-2018 16:05:49 "GET /index1.html" 200  		-->  12-JULY-2018 16:05:49 "GET /index1.html" 200
	12/JUL/2018:16:05:49 -0800 "GET /index2.html" 200  	-->  12-JULY-2018 16:05:49 "GET /index2.html" 20
	GET 1531411549 "/index3.html" 200  					-->  12-JULY-2018 16:05:49 "GET /index3.html" 200
	
- Ambassador
	- your application communicates to different database instances at different stages of development. A local database for development, one for testing and another for production.
	-  You must ensure to modify this connectivity depending on the environment you are deploying your application to.
	- You may chose to outsource such logic to a separate container within your POD, so that your application can always refer to a database at localhost, and the new container, will proxy that request to the right database. This is known as an ambassador container.
	
exercise-1:
----------
Create a multi-container pod with 2 containers.
Use the spec given below.
If the pod goes into the crashloopbackoff then add sleep 1000 in the lemon container.
Name: yellow
Container 1 Name: lemon
Container 1 Image: busybox
Container 2 Name: gold
Container 2 Image: redis

- creates pod with name 'yellow' and a containet with name 'yellow' by given image.
$ kubectl run yellow --image=busybox --restart=Never --dry-run=Client -o yaml > pod.yaml

- update the file and the containers.

- check pod logs: incl all contianer within it
$ kubectl -n <namespace_name> logs <pod_name>


$ kubectl get pod -n elastic-stack
NAME             READY   STATUS    RESTARTS   AGE
app              1/1     Running   0          48m
elastic-search   1/1     Running   0          48m
kibana           1/1     Running   0          48m


root@controlplane:~# kubectl -n elastic-stack get pod,svc
NAME                 READY   STATUS    RESTARTS   AGE
pod/app              1/1     Running   0          49m
pod/elastic-search   1/1     Running   0          49m
pod/kibana           1/1     Running   0          49m

NAME                    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                         AGE
service/elasticsearch   NodePort   10.105.218.2     <none>        9200:30200/TCP,9300:30300/TCP   49m
service/kibana          NodePort   10.108.103.235   <none>        5601:30601/TCP                  48m


$ kubectl -n elastic-stack logs app	


- extract the pod definition.
$ kubectl -n elastic-stack get pod app -o yaml > app-pod.yaml


	
	
	
==================================================================================	

Observability in Kubernetes

==================================================================================
- Readiness and Liveness Probes, Logging and Monitoring concepts.

POD Status
----------

- A POD has a pod status and some conditions in the lifecycle of a POD

- 3 Pod success Status: Pending, ContainerCreating, Running. STATUS column in kubectl get pods, at any point in time the POD status can only be one of these values and only gives us a high level summary of a POD.

	- Pending
		- When a POD is first created, it is in a Pending state. This is when the Scheduler tries to figure out were to place the 
	POD. If the scheduler cannot find a node to place the POD, it remains in a Pending state. To find out why it’s stuck in a pending state, run the kubectl describe pod command, and it will tell you exactly why. 

	- ContainerCreating
		- Once the POD is scheduled to a node, it goes into a ContainerCreating status, where the images required for the application are pulled and the container starts.
		
	- Running
		- Once all the containers in a POD starts, it goes into a running state, where it continues to be until the program completes successfully or is terminated. 

	- Completed
		- in case the Pod is created out of Job object (kind: Job).

- Other failure Statues:
	- ImagePullBackOff
		- POD got scheduled on an available node but got stuck during image pull stage from docker (ex: incorrect image name)

- Other status:		

$ kubectl get pod
NAME                READY   STATUS    RESTARTS   AGE
simple-webapp-1   	1/1     Running   0          33m
simple-webapp-2   	0/1     Running   0          42s




POD Conditions
---------------
- Conditions compliment POD status. It is an array of true or false values that tell us the 
state of a POD.
- PodScheduled (true/false): When a POD is scheduled on a Node, the PodScheduled condition is set to True.
- Initialized (true/false): When the POD is initialized, it’s value is set to True.
- ContainersReady (true/false): We know that a POD has multiple containers. When all the containers in the POD are ready, the Containers 
Ready condition is set to True.
- Ready (true/false): finally the POD itself is considered to be Ready


$ kubectl describe pod simple-webapp-2

Name:         simple-webapp-2
Namespace:    default
Priority:     0
Node:         controlplane/10.30.83.6
Start Time:   Wed, 29 Dec 2021 12:19:25 +0000
Labels:       name=simple-webapp
Annotations:  <none>
Status:       Running
IP:           10.244.0.7
IPs:
  IP:  10.244.0.7
Containers:
  simple-webapp:
    Container ID:   docker://1c94827d91493f603021fb34e2fed547cd908b09d470935407c7eac7889e98fd
    Image:          kodekloud/webapp-delayed-start
    Image ID:       docker-pullable://kodekloud/webapp-delayed-start@sha256:666b95c2ef8e00a74fa0ea96def8d9d69104ec5d9b9b0f49d8a76bd4c94ad343
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 29 Dec 2021 12:19:30 +0000
    Ready:          False
    Restart Count:  0
    Readiness:      http-get http://:8080/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      APP_START_DELAY:  80
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8bvpf (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True


------ after sometime -----
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
  
-------


- To see the state of POD conditions run the kubectl describe POD command and look for the conditions section. can also see the Ready state of the POD, in the output (READY column) of the kubectl get pods command.


- The ready conditions indicate that the application inside the POD is running and is ready to accept user traffic.

- The containers could be running different kinds of applications in them. It could be a simple script that performs a job. It could be a database service. Or a large web server, serving front end users. The script may take a few milliseconds to get ready. The database service may take a few seconds to power up. Some web servers could take several minutes to warm up. 

	-  If you try to run an instance of a Jenkins server, you will notice that it takes about 10-15 seconds for the server to initialize before a user can access the web UI. Even after the Web UI is initialized, it takes a few seconds for the server to warm up 
and be ready to serve users. During this wait period if you look at the state of the POD, it continues to indicate that the POD is ready, which is not very true.

- **  how does kubernetes know weather that the application inside the container is actually running or not? But before we get into 
that discussion, why does it matter if the state is reported incorrectly. 
	- Let us look at a simple scenario were you create a POD and expose it to external users using a service. The service will route traffic to the POD immediately. The service relies on the pod’s READY condition to route traffic. 
	- By default, Kubernetes assumes that as soon as the container is created, it is ready to serve user traffic. So it sets the value of the “Ready Condition” for each container to True. But if the application within the container took longer to get ready, the service is unaware of it and sends traffic through as the container is already in a ready state, causing users to hit a POD that isn’t yet running a live application. 
	

Readiness Probes
-------------------------
- There are different ways that you can define if an application inside a container is actually ready. You can setup different kinds of tests or Probes.
	- In case of a web application it could be when the API server is up and running. So you could run a HTTP test to see if the API server responds. ex: HTTP Test - /api/ready 
	- In case of database, you may test to see if a particular TCP socket is listening. ex: TCP Test - 3306
	- Or You may simply execute a command within the container to run a custom script that would exit successfully if the application is ready. 
	ex: exec command


- add a new field called readinessProbe and use the httpGet option. Specify the port and the ready api.	
- Now when the container is created, kubernetes does not immediately set the ready condition on the container to true, instead, it performs a test to see if the api responds positively. Until then the service does not forward any traffic to the pod, as it sees that the POD is not ready.


- successThreshold/failureThreshold:   The number of consecutive successes is configured via the successThreshold field and the number of consecutive failures required to transition from success to failure is failureThreshold. 

- periodSeconds/timeoutSeconds:   The probe runs every periodSeconds and each probe will wait up to timeoutSeconds to complete.  


pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
labels:
  name: simple-webapp
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
  ports:
  - containerPort: 8080
  readinessProbe:
    httpGet:
      path: /api/ready
      port: 8080
	  

- For http, use the httpGet option with the path and port.
- httpGet: Send and HTTP GET request to the container at a specified path and port. If the HTTP response status code is a 2xx or 3xx then the container is a success, otherwise, it is a failure.

readinessProbe:
  httpGet:
    path: /api/ready
    port: 8080
  initialDelaySeconds: 10  // delay in sec for app to warm up and then kubernetes will start probing.
  periodSeconds: 5         // interval in sec between each probe
  failureThreshold: 8      // by deafult 3 unsuccessful attemps of probe then the probe will stop.


ex:
ports:
    - name: liveness-port
      containerPort: 8080
      hostPort: 8080

    livenessProbe:
      httpGet:
        path: /healthz
        port: liveness-port



- For TCP use the tcpSocket option with port.
- tcpSocket: Attempt to open a socket to the container on a specified port. If the connection cannot be established, the probe fails.

readinessProbe:
  tcpSocket:
    port: 3306



- For executing a command specify the exec option with the command and options in an array format. 
- exec: Issue a command in the container. If the exit code is zero the container is a success, otherwise it is a failed probe.

readinessProbe:
  exec:
    command:
    - cat
    - /app/is_ready 
  initialDelaySeconds: 10
  periodSeconds: 5

	
How readinessProbes are useful in a multi-pod setup
----------------------------------------------------
- Say you have a replica set or deployment with multiple pods. And a service serving traffic to all the pods. There are two PODs already serving users. Say you were to add an additional pod. And let’s say the Pod takes a minute to warm up. Without the readinessProbe configured correctly, the service would immediately start routing traffic to the new pod. That will result in service disruption to atleast some of the users.

- Instead if the pods were configured with the correct readinessProbe, the service will continue to serve traffic only to the older pods and wait until the new pod is ready. 

- Once ready, traffic will be routed to the new pod as well, ensuring no users are affected.



Exercise: readinessProbe
-------------------------
1. Create a Pod that uses a readinessProbe with an HTTP GET action:

pod-readiness.yaml
------------------

apiVersion: v1
kind: Pod
metadata:
  labels:
    test: readiness
  name: readiness-http
spec:
  containers:
  - name: readiness
    image: httpd:2.4.38-alpine
    ports:
    - containerPort: 80
    # Sleep for 30 seconds before starting the server
    command: ["/bin/sh","-c"]
    args: ["sleep 30 && httpd-foreground"]
    readinessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 3
      periodSeconds: 3


$ kubectl create -f pod-readiness.yaml

- The probe will check for a successful HTTP response from the Pod's IP on port 80. The container command delays starting the server by sleeping for 30 seconds to simulate an intense startup routine. 


2. Describe the Pod to see how events related to the readiness probe:

$ kubectl describe pod readiness-http


There will be about 10 (x10) failed probes since the container sleeps for 30 seconds and the probe runs every 3 seconds after an initial 3-second delay. You can see when the probe succeeds by looking at the Conditions section:

Readiness:      http-get http://:80/ delay=3s timeout=1s period=3s #success=1 #failure=3

The Ready and ContainerReady Status will both be True once the probe succeeds. They will be False until then. Also, note the Containers section summarizes the configured readiness probe:


- To confirm the readiness probes are always running, kill the httpd server processes running in the container

$ kubectl exec readiness-http -- pkill httpd


- Describe the Pod and observe the Ready Conditions are False:

$ kubectl describe pod readiness-http

Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  9m51s                 default-scheduler  Successfully assigned default/readiness-http to ip-10-0-0-11.us-west-2.compute.internal
  Normal   Pulling    9m50s                 kubelet            Pulling image "httpd:2.4.38-alpine"
  Normal   Pulled     9m45s                 kubelet            Successfully pulled image "httpd:2.4.38-alpine" in 5.211465198s
  Normal   Created    64s (x2 over 9m44s)   kubelet            Created container readiness
  Normal   Started    64s (x2 over 9m44s)   kubelet            Started container readiness
  Normal   Pulled     64s                   kubelet            Container image "httpd:2.4.38-alpine" already present on machine
  Warning  Unhealthy  35s (x19 over 9m41s)  kubelet            Readiness probe failed: Get "http://192.168.23.129:80/": dial tcp 192.168.23.129:80: connect: connection refused


Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True


$ kubectl get po
NAME             READY   STATUS    RESTARTS   AGE
readiness-http   1/1     Running   1          18m

Note: The httpd server processes will recover from being killed after a minute. If more than a minute has passed since you issued the previous instruction's command, you may see both conditions are True.  If that happened, you can kill the httpd processes again and quickly describe the Pod.





Liveness Probes
====================================================
- You run the same web application with kubernetes. Every time the application crashes, kubernetes makes an attempt to restart the 
container to restore service to users. You can see the count of restarts increase in the output (RESTART column) of kubectl get pods command.

- Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes provides liveness probes to detect and remedy such situations.


- However, what if the application is not really working but the container continues to stay alive? 
	- Say for example, due to a bug in the code, the application is stuck in an infinite loop. As far as kubernetes is concerned, the container is up, so the application is assumed to be up. But the users hitting the container are not served.
	-  In that case, the container needs to be restarted, or destroyed and a new container is to be brought up. 
	-  A liveness probe can be configured on the container to periodically test whether the application within the container is actually healthy. If the test fails, the container is considered unhealthy and is destroyed and recreated.
	- as a developer, you get to define what it means for an application to be healthy. 
	

pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
labels:
  name: simple-webapp
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
  ports:
  - containerPort: 8080
  livenessProbe:
    httpGet:
      path: /api/healthy 
      port: 8080
	
- In case of a web application it could be when the API server is up and running. ex: HTTP Test - /api/healthy 
	
livenessProbe:
  httpGet:
    path: /api/ping
    port: 8080
  initialDelaySeconds: 10  // delay in sec for app to warm up and then kubernetes will start probing. 
  periodSeconds: 5         // interval in sec between each probe. default 10 sec
  failureThreshold: 8      // by deafult 3 unsuccessful attemps of probe then the probe will stop.



	
- For TCP use the tcpSocket option with port.

livenessProbe:
  tcpSocket:
    port: 3306



- For executing a command specify the exec option with the command and options in an array format. 

livenessProbe:
  exec:
    command:
    - cat
    - /app/is_ready 
  initialDelaySeconds: 10

	
ex:
- create a Pod that runs a container based on the k8s.gcr.io/busybox image. 

pods/probe/exec-liveness.yaml
-------------------------------
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5

- In the configuration file, you can see that the Pod has a single Container. The periodSeconds field specifies that the kubelet should perform a liveness probe every 5 seconds. The initialDelaySeconds field tells the kubelet that it should wait 5 seconds before performing the first probe. To perform a probe, the kubelet executes the command cat /tmp/healthy in the target container. If the command succeeds, it returns 0, and the kubelet considers the container to be alive and healthy. If the command returns a non-zero value, the kubelet kills the container and restarts it.

- When the container starts, it executes this command:

/bin/sh -c "touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600"


- For the first 30 seconds of the container's life, there is a /tmp/healthy file. So during the first 30 seconds, the command cat /tmp/healthy returns a success code. After 30 seconds, cat /tmp/healthy returns a failure code.


- Create the Pod:
$ kubectl apply -f https://k8s.io/examples/pods/probe/exec-liveness.yaml


- Within 30 seconds, view the Pod events:  The output indicates that no liveness probes have failed yet:

$ kubectl describe pod liveness-exec

FirstSeen    LastSeen    Count   From            SubobjectPath           Type        Reason      Message
--------- --------    -----   ----            -------------           --------    ------      -------
24s       24s     1   {default-scheduler }                    Normal      Scheduled   Successfully assigned liveness-exec to worker0
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulling     pulling image "k8s.gcr.io/busybox"
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulled      Successfully pulled image "k8s.gcr.io/busybox"
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Created     Created container with docker id 86849c15382e;
23s       23s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Started     Started container with docker id 86849c15382e


- After 35 seconds, view the Pod events again:

$ kubectl describe pod liveness-exec

- At the bottom of the output, there are messages indicating that the liveness probes have failed, and the containers have been killed and recreated.

FirstSeen LastSeen    Count   From            SubobjectPath           Type        Reason      Message
--------- --------    -----   ----            -------------           --------    ------      -------
37s       37s     1   {default-scheduler }                    Normal      Scheduled   Successfully assigned liveness-exec to worker0
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulling     pulling image "k8s.gcr.io/busybox"
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Pulled      Successfully pulled image "k8s.gcr.io/busybox"
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Created     Created container with docker id 86849c15382e; 
36s       36s     1   {kubelet worker0}   spec.containers{liveness}   Normal      Started     Started container with docker id 86849c15382e
2s        2s      1   {kubelet worker0}   spec.containers{liveness}   Warning     Unhealthy   Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory


- Wait another 30 seconds, and verify that the container has been restarted:

$ kubectl get pod liveness-exec

NAME            READY     STATUS    RESTARTS   AGE
liveness-exec   1/1       Running   1          1m




Exercise: livenessProbe
-------------------------


1. Create a Pod that uses a liveness probe to detect broken states:

pod-liveness.yaml
-----------------
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-tcp
spec:
  containers:
  - name: liveness
    image: busybox:1.30.1
    ports:
    - containerPort: 8888
    # Listen on port 8888 for 30 seconds, then sleep
    command: ["/bin/sh", "-c"]
    args: ["timeout 30 nc -p 8888 -lke echo hi && sleep 600"]
    livenessProbe:
      tcpSocket:
        port: 8888
      initialDelaySeconds: 3
      periodSeconds: 5


$ kubectl create -f pod-liveness.yaml


- Recall that liveness probes have the same configuration as readiness probes. The nc (netcat) command listens (the -l option) for connections on port (-p) 8888 and responds with the message hi for the first 30 seconds, after which timeout kills the nc server. The liveness probe attempts to establish a connection with the server every 5 seconds.


- Watch the describe output for the Pod to observe when the liveness probe fails and the Pod is restarted:

$ watch kubectl describe pod liveness-tcp

Restart Count:  3
Liveness:       tcp-socket :8888 delay=3s timeout=1s period=5s #success=1 #failure=3
	



=======================================================================

Container Logging

=======================================================================
Logs - Docker
--------------
- Running a docker container for the image: kodekloud/event-simulator which generate random events simulating a web server, streamed to the standard output by the application

$ docker run kodekloud/event-simulator

2018-10-06 15:57:15,937 - root - INFO - USER1 logged in
2018-10-06 15:57:16,943 - root - INFO - USER2 logged out
2018-10-06 15:57:17,944 - root - INFO - USER2 is viewing page2
2018-10-06 15:57:18,951 - root - INFO - USER3 is viewing page3
2018-10-06 15:57:19,954 - root - INFO - USER4 is viewing page1
2018-10-06 15:57:20,955 - root - INFO - USER2 logged out
2018-10-06 15:57:21,956 - root - INFO - USER1 logged in


- run the docker container in the background, in a detached mode using the –d option, the logs wont appear on std output/console.
$ docker run -d kodekloud/event-simulator

- use the docker logs command followed by the container ID. The –f option helps us see the live log trail. 
$ docker logs -f <container-id>


Logs - Kubernetes
-----------------
- Once the pod is running, we can view the logs using the kubectl logs command with the pod name. Use the –f option to stream the logs live. 

- Kubernetes has basic support for container logging built-in. Kubernetes will capture anything written to standard output and standard error as a log message. For logs that are not written to standard output, More effort is required. You can always copy log files in containers outside of the container or issue commands by running a shell in the container to retrieve the logs you need. 

$ kubectl logs <pod_name>  // if single container pod. use -f option for tail.

$ kubectl logs <pod_name> <container_name>   // if multi container pod. use -f option for tail. passing only the pod_name will throw error.


event-simulator.yaml
--------------------
apiVersion: v1
kind: Pod
metadata:
  name: event-simulator-pod
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
  - name: image-processor
    image: some-image-processor

$ kubectl create –f event-simulator.yaml


$ kubectl logs –f event-simulator-pod

2018-10-06 15:57:15,937 - root - INFO - USER1 logged in
2018-10-06 15:57:16,943 - root - INFO - USER2 logged out
2018-10-06 15:57:17,944 - root - INFO - USER2 is viewing page2
2018-10-06 15:57:18,951 - root - INFO - USER3 is viewing page3
2018-10-06 15:57:19,954 - root - INFO - USER4 is viewing page1
2018-10-06 15:57:20,955 - root - INFO - USER2 logged out
2018-10-06 15:57:21,956 - root - INFO - USER1 logged in


$ kubectl logs –f event-simulator-pod event-simulator

2018-10-06 15:57:15,937 - root - INFO - USER1 logged in
2018-10-06 15:57:16,943 - root - INFO - USER2 logged out
2018-10-06 15:57:17,944 - root - INFO - USER2 is viewing page2
2018-10-06 15:57:18,951 - root - INFO - USER3 is viewing page3
2018-10-06 15:57:19,954 - root - INFO - USER4 is viewing page1
2018-10-06 15:57:20,955 - root - INFO - USER2 logged out
2018-10-06 15:57:21,956 - root - INFO - USER1 logged in


- list the containers under a pod.
$ kubectl get pods <pod_name> -o jsonpath='{.spec.containers[*].name}'
simple-webapp db


$ kubectl logs event-simulator-pod event-simulator



- Create a multi-container Pod that runs a server and a client that sends requests to the server

- The server is similar to the one in the previous lab step except it includes the -v option for verbose output. This causes the server to write some text to standard output. 

- The client container uses nc as well, but instead of listening for connections on a port it connects to the server.

pod-logs.yaml
--------------
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: logs
  name: pod-logs
spec:
  containers:
  - name: server
    image: busybox:1.30.1
    ports:
    - containerPort: 8888
    # Listen on port 8888
    command: ["/bin/sh", "-c"]
    # -v for verbose mode
    args: ["nc -p 8888 -v -lke echo Received request"]
    readinessProbe:
      tcpSocket:
        port: 8888
  - name: client
    image: busybox:1.30.1
    # Send requests to server every 5 seconds
    command: ["/bin/sh", "-c"]
    args: ["while true; do sleep 5; nc localhost 8888; done"]


$ kubectl create -f pod-logs.yaml


- Retrieve the logs (standard output messages) from the server container:

syntax: kubectl logs <pod_name> <container_name>

$ kubectl logs pod-logs server 

listening on [::]:8888 ...
connect to [::ffff:192.168.23.129]:8888 from [::ffff:10.0.0.11]:38170 ([::ffff:10.0.0.11]:38170)
connect to [::ffff:127.0.0.1]:8888 from [::ffff:127.0.0.1]:37211 ([::ffff:127.0.0.1]:37211)
connect to [::ffff:127.0.0.1]:8888 from [::ffff:127.0.0.1]:33259 ([::ffff:127.0.0.1]:33259)
..
..


- After an initial listening message, connection messages for each client request are displayed



- Display the most recent log (--tail=1) including the timestamp and stream (-f for follow) the logs from the client container:
- You will see a new request being made every five seconds.

$ kubectl logs -f --tail=1 --timestamps pod-logs client

2022-02-21T10:52:48.331111461Z Received request
2022-02-21T10:52:53.333188054Z Received request
2022-02-21T10:52:58.336012161Z Received request




- Now Create an Apache web server and allow access to it via a load balancer:

pod-webserver.yaml
-------------------

apiVersion: v1
kind: Pod
metadata:
  labels:
    test: logs
  name: webserver-logs
spec:
  containers:
  - name: server
    image: httpd:2.4.38-alpine
    ports:
    - containerPort: 80
    readinessProbe:
      httpGet:
        path: /
        port: 80


$ kubectl create -f pod-webserver.yaml


- The expose command uses the container port for the Service's port when the --port option isn't provided. In this case, the Service uses port 80 (HTTP).

$ kubectl expose pod webserver-logs --type=LoadBalancer


$ kubectl get service
NAME             TYPE           CLUSTER-IP      EXTERNAL-IP                                                              PORT(S)        AGE
webserver-logs   LoadBalancer   10.110.244.95   a11db3276b6544b7a861f535dc048df3-307068056.us-west-2.elb.amazonaws.com   80:30537/TCP   58s


- The httpd container directs access logs and errors to standard output and standard error, instead of the files.
- there are lots of request at "/" path which is configured as readiness probe (every 10 secs)

$ kubectl logs webserver-logs server

10.0.0.10 - - [21/Feb/2022:10:56:36 +0000] "GET / HTTP/1.1" 200 45
10.0.0.10 - - [21/Feb/2022:10:56:46 +0000] "GET / HTTP/1.1" 200 45
10.0.0.10 - - [21/Feb/2022:10:56:56 +0000] "GET / HTTP/1.1" 200 45
192.168.23.128 - - [21/Feb/2022:10:56:56 +0000] "GET / HTTP/1.1" 200 45
192.168.23.128 - - [21/Feb/2022:10:56:56 +0000] "GET /favicon.ico HTTP/1.1" 404 209
10.0.0.10 - - [21/Feb/2022:10:57:06 +0000] "GET / HTTP/1.1" 200 45
192.168.23.128 - - [21/Feb/2022:10:57:12 +0000] "GET /oops HTTP/1.1" 404 202
10.0.0.10 - - [21/Feb/2022:10:57:16 +0000] "GET / HTTP/1.1" 200 45



- Retrieve the last 10 lines from the conf/httpd.conf file:
- The tail command with -10 prints the last 10 lines of the given file. Although httpd.conf isn't a log file, you could use the command to access log files in the same way.

$ kubectl exec webserver-logs -it -- tail -10 conf/httpd.conf
#
# Note: The following must must be present to support
#       starting without SSL on platforms with no /dev/random equivalent
#       but a statically compiled-in mod_ssl.
#
<IfModule ssl_module>
SSLRandomSeed startup builtin
SSLRandomSeed connect builtin
</IfModule>



- Copy the conf/httpd.conf from the container to the bastion host:

- The cp command takes a source file spec (webserver-logs:conf/httpd.conf) and a destination file spec (local-copy-of-httpd.conf). You can also copy from the local file system to a container using cp. To indicate the Pod file system, begin the file spec with the Pod name followed by a colon and then the path. 

	- There are several examples in the help if you ever forget the syntax (kubectl cp --help).

$ kubectl cp webserver-logs:conf/httpd.conf local-copy-of-httpd.conf

$ ls -ltr local-copy-of-httpd.conf 
-rw-rw-r-- 1 ubuntu ubuntu 20673 Feb 22 03:30 local-copy-of-httpd.conf


- Copy /tmp/foo local file to /tmp/bar in a remote pod in a specific container

$ kubectl cp /tmp/foo <some-pod>:/tmp/bar -c <specific-container>


- Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace <some-namespace>

$ kubectl cp /tmp/foo <some-namespace>/<some-pod>:/tmp/bar



- Copy /tmp/foo from a remote pod to /tmp/bar locally

$ kubectl cp <some-namespace>/<some-pod>:/tmp/foo /tmp/bar




Kubernetes Logging Using a Logging Agent and the Sidecar Pattern
-------------------------------------------------------------------

- The sidecar multi-container pattern uses a "sidecar" container to extend the primary container in the Pod. In the context of logging, the sidecar is a logging agent. The logging agent streams logs from the primary container, such as a web server, to a central location that aggregates logs. 

- To allow the sidecar access to the log files, both containers mount a volume at the path of the log files. 

- In this lab step, you will use an S3 bucket to collect logs. 

- You will use a sidecar that uses Fluentd, a popular data collector often used as a logging layer, with an S3 plugin installed to stream log files in the primary container to S3.


1. Store the name of the logs S3 bucket created for you by the Cloud Academy Lab environment:

$ s3_bucket=$(aws s3api list-buckets --query "Buckets[].Name" --output table | grep logs | tr -d \|)


2. To verify that the Amazon S3 bucket name was retrieved successfully, enter the following:

$ echo $s3_bucket

- You will see the name of an S3 bucket beginning with cloudacademylabs-k8slogs-.


3. Create a ConfigMap that stores the fluentd configuration file:

- ConfigMaps allow you to separate configuration from the container images. This increases the reusability of images.
- ConfigMaps can be mounted into containers using Kubernetes Volumes.
- fluent.conf configuration file: 
	- two log sources are configured in the /var/log directory and their log messages will be tagged with count.format1 and count.format2. 
	- The configuration also describes streaming all the logs to the S3 logs bucket in the match section.  
	

fluentd-sidecar-config.yaml
---------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluent.conf: |
    # First log source (tailing a file at /var/log/1.log)
    <source>
      @type tail
      format none
      path /var/log/1.log
      pos_file /var/log/1.log.pos
      tag count.format1
    </source>

    # Second log source (tailing a file at /var/log/2.log)
    <source>
      @type tail
      format none
      path /var/log/2.log
      pos_file /var/log/2.log.pos
      tag count.format2
    </source>

    # S3 output configuration (Store files every minute in the bucket's logs/ folder)
    <match **>
      @type s3

      s3_bucket $s3_bucket
      s3_region us-west-2
      path logs/
      buffer_path /var/log/
      store_as text
      time_slice_format %Y%m%d%H%M
      time_slice_wait 1m
      
      <instance_profile_credentials>
      </instance_profile_credentials>
    </match>


$ kubectl create -f fluentd-sidecar-config.yaml



4. Create a multi-container Pod using a fluentd logging agent sidecar (count-agent): 

- The count container writes the date and a counter variable ($i) in two different log formats to two different log files in the /var/log directory every second.

- The /var/log directory is mounted as a Volume in both the primary count container and the count-agent sidecar so both containers can access the logs.

- The sidecar also mounts the ConfigMap to access the fluentd configuration file. By using a ConfigMap, the same sidecar container can be used for any configuration compared to storing the configuration in the image and having to manage separate container images for each configuration.


pod-counter.yaml
-----------------
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    command: ["/bin/sh", "-c"]
    args:
    - >
      i=0;
      while true;
      do
        # Write two log files along with the date and a counter
        # every second
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    # Mount the log directory /var/log using a volume
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-agent
    image: lrakai/fluentd-s3:latest
    env:
    - name: FLUENTD_ARGS
      value: -c /fluentd/etc/fluent.conf
    # Mount the log directory /var/log using a volume
    # and the config file
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: config-volume
      mountPath: /fluentd/etc
  # Use host network to allow sidecar access to IAM instance profile credentials
  hostNetwork: true
  # Declare volumes for log directory and ConfigMap
  volumes:
  - name: varlog
    emptyDir: {}
  - name: config-volume
    configMap:
      name: fluentd-config


$ kubectl create -f pod-counter.yaml


$ kubectl exec counter -c count -it -- /bin/sh
$ tail -f /var/log/1.log

177: Tue Feb 22 04:07:28 UTC 2022
178: Tue Feb 22 04:07:29 UTC 2022
179: Tue Feb 22 04:07:30 UTC 2022
..
..


$ tail -f /var/log/2.log

Tue Feb 22 04:07:36 UTC 2022 INFO 185
Tue Feb 22 04:07:37 UTC 2022 INFO 186
Tue Feb 22 04:07:38 UTC 2022 INFO 187
..
..


$ kubectl exec counter -c count-agent -it -- /bin/sh

# ls -ltr /fluentd/etc/
total 0
lrwxrwxrwx    1 root     root            18 Feb 22 04:36 fluent.conf -> ..data/fluent.conf

# cat /fluentd/etc/fluent.conf 
# First log source (tailing a file at /var/log/1.log)
<source>
  @type tail
  format none
  path /var/log/1.log
  pos_file /var/log/1.log.pos
  tag count.format1
</source>

# Second log source (tailing a file at /var/log/2.log)
<source>
  @type tail
  format none
  path /var/log/2.log
  pos_file /var/log/2.log.pos
  tag count.format2
</source>

# S3 output configuration (Store files every minute in the bucket's logs/ folder)
<match **>
  @type s3

  s3_bucket   cloudacademylabs-k8slogs-p3npoajzwjxl  
  s3_region us-west-2
  path logs/
  buffer_path /var/log/
  store_as text
  time_slice_format %Y%m%d%H%M
  time_slice_wait 1m
  
  <instance_profile_credentials>
  </instance_profile_credentials>
</match>


- navigate to aws console -> go to -> s3://cloudacademylabs-k8slogs-p3npoajzwjxl/logs/202202220436_0.txt

2022-02-22T04:36:51+00:00	fluent.info	{"worker":0,"message":"fluentd worker is now running worker=0"}
2022-02-22T04:36:52+00:00	count.format1	{"message":"6: Tue Feb 22 04:36:52 UTC 2022"}
2022-02-22T04:36:52+00:00	count.format2	{"message":"Tue Feb 22 04:36:52 UTC 2022 INFO 6"}
2022-02-22T04:36:53+00:00	count.format2	{"message":"Tue Feb 22 04:36:53 UTC 2022 INFO 7"}
2022-02-22T04:36:53+00:00	count.format1	{"message":"7: Tue Feb 22 04:36:53 UTC 2022"}
2022-02-22T04:36:54+00:00	count.format1	{"message":"8: Tue Feb 22 04:36:54 UTC 2022"}
2022-02-22T04:36:54+00:00	count.format2	{"message":"Tue Feb 22 04:36:54 UTC 2022 INFO 8"}
2022-02-22T04:36:55+00:00	count.format2	{"message":"Tue Feb 22 04:36:55 UTC 2022 INFO 9"}
2022-02-22T04:36:55+00:00	count.format1	{"message":"9: Tue Feb 22 04:36:55 UTC 2022"}
2022-02-22T04:36:56+00:00	count.format2	{"message":"Tue Feb 22 04:36:56 UTC 2022 INFO 10"}
2022-02-22T04:36:56+00:00	count.format1	{"message":"10: Tue Feb 22 04:36:56 UTC 2022"}
2022-02-22T04:36:57+00:00	count.format2	{"message":"Tue Feb 22 04:36:57 UTC 2022 INFO 11"}
2022-02-22T04:36:57+00:00	count.format1	{"message":"11: Tue Feb 22 04:36:57 UTC 2022"}
2022-02-22T04:36:58+00:00	count.format2	{"message":"Tue Feb 22 04:36:58 UTC 2022 INFO 12"}
2022-02-22T04:36:58+00:00	count.format1	{"message":"12: Tue Feb 22 04:36:58 UTC 2022"}
2022-02-22T04:36:59+00:00	count.format2	{"message":"Tue Feb 22 04:36:59 UTC 2022 INFO 13"}
2022-02-22T04:36:59+00:00	count.format1	{"message":"13: Tue Feb 22 04:36:59 UTC 2022"}




===========================================================================================

Monitor and Debug Applications

===========================================================================================

- How do you monitor resource consumption on Kubernetes?
	- Node level metrics such as the number of nodes in the cluster, how many of them are healthy as well as performance metrics such as CPU. Memory, network and disk utilization.
	- POD level metrics such as the number of PODs, and performance metrics of each POD such the CPU and Memory consumption.

- we need a solution that will monitor these metrics, store them and provide analytics around this data.

- Kubernetes does not come with a full featured built-in monitoring solution. 

- open-source solutions available today, such as the Metrics-Server, Prometheus, the Elastic Stack, and proprietary solutions like Datadog and Dynatrace. 


Metrics-Server
--------------
- You can have one metrics server per kubernetes cluster.

- The metrics server retrieves metrics from each of the kubernetes nodes and pods, aggregates them and stores them in memory. 

- Note that the metrics server is only an in-memory monitoring solution and does not store the metrics on the disk, and as a 
result you cannot see historical performance data. For that you must rely on one of the advanced monitoring solutions we talked about earlier in this lecture. 


How are the metrics generated for the PODs on these nodes?
-----------------------------------------------------------
- Kubernetes runs an agent on each node known as the kubelet, which is responsible for receiving instructions from the kubernetes API master server and running PODs on the nodes. 

- The kubelet also contains a subcomponent known as as cAdvisor or Container Advisor.

- cAdvisor is responsible for retrieving performance metrics from pods, and exposing them through the kubelet API to make the metrics available for the Metrics Server.

Metrics Server – Getting Started
---------------------------------
- If you are using minikube for your local cluster, run the command minikube addons enable metrics-server.

$ minikube addons enable metrics-server


-  For all other environments deploy the metrics server by cloning the metrics-server deployment files from the github repository. And then deploying the required components using the kubectl create command.

$ git clone https://github.com/kubernetes-incubator/metrics-server.git


- deploys a set of pods, services and roles to enable metrics server to poll for performance metrics from the nodes in the cluster.

$ kubectl create –f deploy/1.8+/

clusterrolebinding "metrics-server:system:auth-delegator" created
rolebinding "metrics-server-auth-reader" created
apiservice "v1beta1.metrics.k8s.io" created
serviceaccount "metrics-server" created
deployment "metrics-server" created
service "metrics-server" created
clusterrole "system:metrics-server" created
clusterrolebinding "system:metrics-server" created


$ kubectl get pod
NAME       READY   STATUS    RESTARTS   AGE
elephant   1/1     Running   0          2m9s
lion       1/1     Running   0          2m9s
rabbit     1/1     Running   0          2m9s

- View Nodes:
$ kubectl top node

NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
controlplane   389m         1%     1250Mi          0%        
node01         69m          0%     339Mi           0%


- View Pods
$ kubectl top pod

NAME       CPU(cores)   MEMORY(bytes)   
elephant   23m          32Mi            
lion       1m           18Mi            
rabbit     151m         253Mi



$ kubectl top node

NAME                                       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-10-0-0-10.us-west-2.compute.internal    60m          3%     585Mi           15%       
ip-10-0-0-100.us-west-2.compute.internal   163m         8%     1056Mi          27%       
ip-10-0-0-11.us-west-2.compute.internal    53m          2%     595Mi           15%



The output displays CPU usage in cores and %. The master and nodes both have 2 CPU cores available. The CPU(cores) column would have a maximum value of 2000m (2000 milliCPU cores = 2 CPU cores). The CPU metrics indicate that the cluster is not under CPU pressure. The MEMORY columns indicate that there is greater than 50% memory available on the nodes. That indicates the cluster if oversized for the current workload. As new Pods are added you should monitor the memory pressure. You should also set CPU and memory limits and requests in your Pod containers to avoid an unexpected degradation in performance.


- Use top to view resource utilization metrics for the Pods in the kube-system Namespace:

$ kubectl top pods -n kube-system

NAME                                                               CPU(cores)   MEMORY(bytes)   
calico-kube-controllers-69496d8b75-hcctr                           2m           12Mi            
calico-node-667qz                                                  33m          94Mi            
calico-node-c92km                                                  22m          95Mi            
calico-node-lm56l                                                  25m          97Mi            
coredns-74ff55c5b-h7f7t                                            2m           8Mi             
coredns-74ff55c5b-kk4cw                                            2m           8Mi             
etcd-ip-10-0-0-100.us-west-2.compute.internal                      15m          45Mi            
kube-apiserver-ip-10-0-0-100.us-west-2.compute.internal            51m          301Mi           
kube-controller-manager-ip-10-0-0-100.us-west-2.compute.internal   12m          50Mi            
kube-proxy-bzbcn                                                   1m           17Mi            
kube-proxy-d8hvk                                                   1m           15Mi            
kube-proxy-dqkcz                                                   1m           16Mi            
kube-scheduler-ip-10-0-0-100.us-west-2.compute.internal            3m           17Mi            
metrics-server-5986c4b4d6-7lg48                                    1m           13Mi



- Display the resource utilization of individual containers:
- The --containers option allows you to understand the resource utilization at a finer granularity when Pods contain multiple containers. In this example, all Pods are only running a single container, however. The NAME column refers to container names.

$ kubectl top pod -n kube-system --containers

POD                                                                NAME                      CPU(cores)   MEMORY(bytes)   
calico-kube-controllers-69496d8b75-hcctr                           calico-kube-controllers   2m           12Mi            
calico-node-667qz                                                  calico-node               28m          94Mi            
calico-node-c92km                                                  calico-node               26m          95Mi            
calico-node-lm56l                                                  calico-node               29m          97Mi            
coredns-74ff55c5b-h7f7t                                            coredns                   2m           8Mi             
coredns-74ff55c5b-kk4cw                                            coredns                   2m           8Mi             
etcd-ip-10-0-0-100.us-west-2.compute.internal                      etcd                      22m          46Mi            
kube-apiserver-ip-10-0-0-100.us-west-2.compute.internal            kube-apiserver            50m          301Mi           
kube-controller-manager-ip-10-0-0-100.us-west-2.compute.internal   kube-controller-manager   11m          50Mi            
kube-proxy-bzbcn                                                   kube-proxy                1m           17Mi            
kube-proxy-d8hvk                                                   kube-proxy                1m           15Mi            
kube-proxy-dqkcz                                                   kube-proxy                1m           16Mi            
kube-scheduler-ip-10-0-0-100.us-west-2.compute.internal            kube-scheduler            3m           17Mi            
metrics-server-5986c4b4d6-7lg48                                    metrics-server            1m           13Mi



- Use a label selector to show only resource utilization for Pods with a k8s-app=kube-dns label:

$ kubectl top pod -n kube-system --containers -l k8s-app=kube-dns

POD                       NAME      CPU(cores)   MEMORY(bytes)   
coredns-74ff55c5b-h7f7t   coredns   2m           8Mi             
coredns-74ff55c5b-kk4cw   coredns   3m           8Mi






==========================================================================================================

POD Design

==========================================================================================================

Labels, Selectors
------------------
- Labels and Selectors are a standard method to group things together

- We have created a lot of different types of Objects in Kuberentes. Pods, Services, ReplicaSets and Deployments. For Kubernetes, all of these are different objects. Over time you may end up having 100s and 1000s of these objects in your cluster. Then you will need a 
way to filter and view different objects by different categories. 

- For each object attach labels as per your needs, like app, function etc.

- Then while selecting, specify a condition to filter specific objects. For example app == 
App1


pod-definition.yaml
-------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
labels:
  app: App1
  function: Front-end
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
    ports:
    - containerPort: 8080
	

$ kubectl get pod
NAME          READY   STATUS    RESTARTS   AGE
app-1-zzxdf   1/1     Running   0          29s
app-1-4lhxh   1/1     Running   0          30s
app-1-kl4sz   1/1     Running   0          30s
db-1-69mbm    1/1     Running   0          30s
app-1-4bfdp   1/1     Running   0          30s
db-1-btfvv    1/1     Running   0          30s
db-1-8rmd2    1/1     Running   0          30s
auth          1/1     Running   0          30s
app-2-5fkzg   1/1     Running   0          30s
db-1-4kqdg    1/1     Running   0          30s
db-2-9pnts    1/1     Running   0          30s

$ kubectl get pod --selector env=dev
NAME          READY   STATUS    RESTARTS   AGE
app-1-4lhxh   1/1     Running   0          42s
app-1-kl4sz   1/1     Running   0          42s
db-1-69mbm    1/1     Running   0          42s
app-1-4bfdp   1/1     Running   0          42s
db-1-btfvv    1/1     Running   0          42s
db-1-8rmd2    1/1     Running   0          42s
db-1-4kqdg    1/1     Running   0          42s


Usage of labels/selectors: ReplicaSet
--------------------------------------
- ReplicaSet uses labels in order to filter the pods for which the defined replica needs to be created.
- labels mentioned under spec->template is for the filterring pod. 
- On creation, if the labels match, the replicaset is created successfully. 

replicaset-definition.yaml
--------------------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: simple-webapp
  labels:
    app: App1
    function: Front-end
spec:
  replicas: 3
  selector:
    matchLabels:
      app: App1
  template:
    metadata:
      labels:
        app: App1
        function: Front-end
    spec:
      containers:
      - name: simple-webapp
        image: simple-webapp
	  

	
Usage of labels/selectors: Service
--------------------------------------
- When a service is created, it uses the selector defined in the service definition file to match the labels set on the pods in the replicaset-definition file.	

service-definition.yaml
-----------------------
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: App1
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
	
	
	
Annotations
------------
- While labels and selectors are used to group and select objects, annotations are used to record other details for informatory purpose. 	

- For example tool details like name, version build information etc or contact details, phone numbers, email ids etc, that may be used for some kind of integration purpose

replicaset-definition.yaml
--------------------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: simple-webapp
  labels:
    app: App1
    function: Front-end
  annotations:
    buildversion: 1.34
spec:
  replicas: 3
  selector:
    matchLabels:
      app: App1
  template:
    metadata:
      labels:
        app: App1
        function: Front-end
    spec:
      containers:
      - name: simple-webapp
        image: simple-webapp


- select any objects by labels.
$ kubectl get all --selector env=prod

NAME              READY   STATUS    RESTARTS   AGE
pod/app-1-zzxdf   1/1     Running   0          5m49s
pod/auth          1/1     Running   0          5m50s
pod/app-2-5fkzg   1/1     Running   0          5m50s
pod/db-2-9pnts    1/1     Running   0          5m50s

NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/app-1   ClusterIP   10.43.135.206   <none>        3306/TCP   5m49s

NAME                    DESIRED   CURRENT   READY   AGE
replicaset.apps/app-2   1         1         1       5m51s
replicaset.apps/db-2    1         1         1       5m51s

$ kubectl get all --selector env=prod --no-headers | wc -l
7

- Select pods by mutiple labels
$ kubectl get pod --selector env=prod,bu=finance,tier=frontend
NAME          READY   STATUS    RESTARTS   AGE
app-1-zzxdf   1/1     Running   0          8m35s








==============================================================================

Rolling Updates & Rollbacks in Deployments

==============================================================================

$ kubectl run nginx --image=nginx
deployment "nginx" created

- this creates a deployment and not just a POD. the output of the command says Deployment nginx created.
- creating a deployment by only specifying the image name and not using a definition file. 
- A replicaset and pods are automatically created in the backend.


Deployment
-------------
- The deployment provides us with capabilities to upgrade the underlying instances seamlessly using rolling updates, undo changes, and pause and resume changes to deployments.

- Each container is encapsulated in PODs. Multiple such PODs are deployed using Replication Controllers or Replica Sets. And then comes Deployment which is a kubernetes object that comes higher in the hierarchy. Container -> POD -> ReplicaSet -> Deployment


Create a deployment
--------------------
- deployment-definition file are exactly similar to the replicaset definition file, except for the kind = Deployment
-  it has an apiVersion which is apps/v1, metadata which has name and labels and a spec that has template, replicas and selector. The template has a POD definition inside it.

deployment-definition.yml
--------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx
  replicas: 3
  selector:
    matchLabels:
      type: front-end
	
$  kubectl create –f deployment-definition.yml
deployment "myapp-deployment" created

$ kubectl get deployments
NAME 				DESIRED 	CURRENT 	UP-TO-DATE 	AVAILABLE 	AGE
myapp-deployment 	3 			3 			3 			3 			21s

$ kubectl get replicaset
NAME 						DESIRED 	CURRENT 	READY 	AGE
myapp-deployment-6795844b58 3 			3 			3 		2m

$ kubectl get pods
NAME 								READY 	STATUS 	RESTARTS 	AGE
myapp-deployment-6795844b58-5rbjl 	1/1 	Running 0 			2m
myapp-deployment-6795844b58-h4w55 	1/1 	Running 0 			2m
myapp-deployment-6795844b58-lfjhv 	1/1 	Running 0 			2m


$ kubectl get all
NAME 									DESIRED 	CURRENT 	UP-TO-DATE 	AVAILABLE 	AGE
deploy/myapp-deployment 				3 			3 			3 			3 			9h

NAME 									DESIRED 	CURRENT 	READY 	AGE
rs/myapp-deployment-6795844b58 			3 			3 			3 		9h

NAME 									READY 	STATUS 	RESTARTS 	AGE
po/myapp-deployment-6795844b58-5rbjl 	1/1 	Running 0 			2m
po/myapp-deployment-6795844b58-h4w55 	1/1 	Running 0 			2m
po/myapp-deployment-6795844b58-lfjhv 	1/1 	Running 0 			2m


- T see wha container image is used:
$ kubectl describe deployments.apps frontend | grep -i image
    Image:        kodekloud/webapp-color:v1






Rollout and Versioning
-----------------------
- Whenever you create a new deployment or make any changes (upgrade the images version,  updating the labels or updating the 
number of replicas etc.) in an existing deployment it triggers a Rollout.
	- A rollout is the process of gradually deploying or upgrading your application containers.
	- A new rollout creates a new Deployment revision. ex: revision 1
	- when the container version is updated to a new one – a new rollout is triggered and a new deployment revision is created named Revision 2. This helps us keep track of the changes made to our deployment and enables us to rollback to a previous version of 
deployment if necessary.

Rollout Command
---------------
- to see the status of your rollout for the deployment.

$  kubectl rollout status deployment/myapp-deployment  // given 10 replicas

Waiting for rollout to finish: 0 of 10 updated replicas are available...
Waiting for rollout to finish: 1 of 10 updated replicas are available...
Waiting for rollout to finish: 2 of 10 updated replicas are available...
Waiting for rollout to finish: 3 of 10 updated replicas are available...
Waiting for rollout to finish: 4 of 10 updated replicas are available...
Waiting for rollout to finish: 5 of 10 updated replicas are available...
Waiting for rollout to finish: 6 of 10 updated replicas are available...
Waiting for rollout to finish: 7 of 10 updated replicas are available...
Waiting for rollout to finish: 8 of 10 updated replicas are available...
Waiting for rollout to finish: 9 of 10 updated replicas are available...
deployment "myapp-deployment" successfully rolled out
	

- To see the revisions and history of rollout 

$ kubectl rollout history deployment/myapp-deployment
deployments "myapp-deployment"
REVISION CHANGE-CAUSE
1 <none>
2 kubectl apply --filename=deployment-definition.yml --record=true


Deployment Strategy
-------------------
- 2 deployment strategies.
	- Recreate strategy: 
		- Upgrade to newer version all at once. NOT the default.
		- the application will be down in between.
		
	- RollingUpdate 
		- we do not destroy all of them at once. Instead we take down the older version and bring up a newer version one by one. 
		- RollingUpdate is the default Deployment Strategy.
		- RollingUpdateStrategy has two parameters to control rolling updates by creating an upper bound and lower bound on the total number of Pods in the Deployment (for additional information issue kubectl explain deployments.spec.strategy.rollingUpdate):
			- max unavailable: The maximum number of Pods that can be unavailable during the update.
			- max surge: The maximum number of Pods that can be scheduled above the desired number of Pods.
		
- The difference between the recreate and rollingupdate strategies can also be seen when you view the deployments in detail.
	-  kubectl describe deployment command to see detailed information regarding the deployments
	- You will notice when the Recreate strategy was used the events indicate that the old replicaset was scaled down to 0 first and the new replica set scaled up to 5
	- However when the RollingUpdate strategy was used the old replica set was scaled down one at a time simultaneously scaling up the new replica set one at a time.
	

- .spec.strategy specifies the strategy used to replace old Pods by new ones. .spec.strategy.type can be "Recreate" or "RollingUpdate". "RollingUpdate" is the default value.

- All existing Pods are killed before new ones are created when .spec.strategy.type==Recreate

- The Deployment updates Pods in a rolling update fashion when .spec.strategy.type==RollingUpdate. You can specify maxUnavailable and maxSurge to control the rolling update process


Update Deployment
------------------
$  kubectl apply –f deployment-definition.yml
deployment "myapp-deployment" configured

OR if just changing the image version. without updating the definition file.

$ kubectl set image deployment/myapp-deployment nginx-container=nginx:1.9.1  // set image <deployment_name> <container_name>=<container_image_name>:<image_tag>

deployment "myapp-deployment" image is updated


- to see the status of the upgrade.
$ kubectl rollout status deployment.apps/<deployment_name>
OR
$ kubectl rollout status deployment <deployment_name>

ex: 
$ kubectl rollout status deployment.apps/frontend
deployment "frontend" successfully rolled out

- to see the roll out history:
$ kubectl rollout history deployment.apps/<deployment_name>
OR
$ kubectl rollout history deployment <deployment_name>

ex:
$ kubectl rollout history deployment.apps/frontend
deployment.apps/frontend 
REVISION  CHANGE-CAUSE
1         <none>


How a deployment performs an upgrade under the hoods. 
-----------------------------------------------------
- When a new deployment is created, say to deploy 5 replicas, it first creates a Replicaset automatically, which in turn creates the number of PODs required to meet the number of replicas.

- When you upgrade your application, the kubernetes deployment object creates a NEW replicaset under the hoods and starts deploying the containers there. 

- At the same time taking down the PODs in the old replica-set following a RollingUpdate strategy. 

- This can be seen when you try to list the replicasets using the kubectl get replicasets command. Here we see the old replicaset with 0 PODs and the new replicaset with 5 PODs.


Rollback
--------
- in case Something’s wrong with the new version of build, we can rollback to previous version.
$ kubectl rollout undo deployment.apps/<deployment_name>

- The deployment will then destroy the PODs in the new replicaset and bring the older ones up in the old replicaset. And your 
application is back to its older format.

- kubectl get replicasets command will show, Before the rollback the first replicaset had 0 PODs and the new replicaset had 5 PODs and this is reversed after the rollback is finished

- if we rollback from the current revision (ex: revision:3), it will be rolled back to revision: 2, a new revision will be added (revision: 4) same as revision: 2 but the old revision: 2 will be removed from the history.


- before rollback:
$ kubectl get replicasets
NAME 							DESIRED 	CURRENT 	READY 	AGE
myapp-deployment-67c749c58c 	0 			0 			0 		22m
myapp-deployment-7d57dbdb8d 	5 			5 			5 		20m

$ kubectl rollout undo deployment/myapp-deployment
deployment “myapp-deployment” rolled back

- after rollback
$ kubectl get replicasets
NAME 							DESIRED 	CURRENT 	READY 	AGE
myapp-deployment-67c749c58c 	5 			5 			5 		22m
myapp-deployment-7d57dbdb8d 	0 			0 			0 		20m


Summarize Commands
------------------

- create deployment
> kubectl create –f deployment-definition.yml

> kubectl create deployment nginx --image=nginx:1.16   // deployment_name will nginx, replicaSet: nginx-<alphnu>
deployment.apps/nginx created

> kubectl run <pod_name> --image nginx // this creates deployment,replicaset,pod


- Get deployment
------------------
> kubectl get deployments


- Update deployment
---------------------
> kubectl apply –f deployment-definition.yml

> kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1


- Rollout status for the Deployment
> kubectl rollout status deployment/myapp-deployment

$ kubectl rollout status deployment nginx
Waiting for deployment "nginx" rollout to finish: 0 of 1 updated replicas are available...
deployment "nginx" successfully rolled out


- Rollout history
------------------
> kubectl rollout history deployment/myapp-deployment

$ kubectl rollout history deployment nginx
deployment.extensions/nginx
REVISION CHANGE-CAUSE
1     <none>



Using the --revision flag
-------------------------------
- check the status of each revision individually by using the --revision flag

$ kubectl rollout history deployment nginx --revision=1
deployment.extensions/nginx with revision #1
 
Pod Template:
 Labels:    app=nginx    pod-template-hash=6454457cdb
 Containers:  nginx:  Image:   nginx:1.16
  Port:    <none>
  Host Port: <none>
  Environment:    <none>
  Mounts:   <none>
 Volumes:   <none>
 

Using the --record flag:
-------------------------
- The --record option will record the command as an annotation in the resource to help understand what caused the change.

We can use the --record flag to save the command used to create/update a deployment against the revision number. the same will be shows in "change-cause" in kubectl rollout history command.

$ kubectl create -f deployment-definition.yaml --record


$ kubectl set image deployment nginx nginx=nginx:1.17 --record
deployment.extensions/nginx image updated

syntax: kubectl set image deployment <deployment_name> <container_name>=<newimage>

ex:
$ kubectl set image deployment web-server httpd=httpd:2.4.38-alpine --record
deployment.apps/web-server image updated


$ kubectl rollout history deployment nginx
deployment.extensions/nginx
 
REVISION 	CHANGE-CAUSE
1     		<none>
2     		kubectl set image deployment nginx nginx=nginx:1.17 --record=true


$ kubectl edit deployments nginx --record
deployment.extensions/nginx edited


$ kubectl rollout history deployment nginx
REVISION 	CHANGE-CAUSE
1     		<none>
2     		kubectl set image deployment nginx nginx=nginx:1.17 --record=true
3     		kubectl edit deployments. nginx --record=true


- --record adds the recent change cause in Annotations field:
$ kubectl describe deployments.apps web-server

Name:                   web-server
Namespace:              deployment
CreationTimestamp:      Mon, 21 Feb 2022 08:17:48 +0000
Labels:                 app=web-server
Annotations:            deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: kubectl edit deployments.apps web-server --record=true
Selector:               app=web-server
Replicas:               6 desired | 6 updated | 6 total | 6 available | 0 unavailable
StrategyType:           RollingUpdate
..
..
..


$ kubectl edit deployments.apps web-server --record
deployment.apps/web-server edited

$ kubectl rollout history deployment web-server 
deployment.apps/web-server 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl edit deployments.apps web-server --record=true
3         kubectl edit deployments.apps web-server --record=true


$ kubectl describe deployments.apps web-server 
Name:                   web-server
Namespace:              deployment
CreationTimestamp:      Mon, 21 Feb 2022 08:17:48 +0000
Labels:                 app=web-server
Annotations:            deployment.kubernetes.io/revision: 3
                        kubernetes.io/change-cause: kubectl edit deployments.apps web-server --record=true
Selector:               app=web-server
Replicas:               6 desired | 6 updated | 6 total | 6 available | 0 unavailable
StrategyType:           RollingUpdate




$ kubectl rollout history deployment nginx --revision=3
deployment.extensions/nginx with revision #3
 
Pod Template: Labels:    app=nginx
    pod-template-hash=df6487dc Annotations: kubernetes.io/change-cause: kubectl edit deployments. nginx --record=true
 
 Containers:
  nginx:
  Image:   nginx:latest
  Port:    <none>
  Host Port: <none>
  Environment:    <none>
  Mounts:   <none>
 Volumes:   <none>



- Rollback
-----------
- Rollback the previous change:

> kubectl rollout undo deployment myapp-deployment

The undo subcommand will roll back to the most previous revision. If you need to roll back to an even earlier version you can use the --to-revision option and set its value to the revision number, which can be obtained from the history.



Expose Deployment
----------------------
- Expose the webserver Deployment to the internet by creating a Service of type LoadBalancer:

$ kubectl expose deployment web-server --type=LoadBalancer --port=80
service/web-server exposed

Services provide a single endpoint for communicating with a set of Pods. Services also use label selectors to define the set of Pods.


-  Watch the output of get services until the EXTERNAL-IP column has a DNS address
$ kubectl get service
NAME         TYPE           CLUSTER-IP    EXTERNAL-IP                                                              PORT(S)        AGE
web-server   LoadBalancer   10.104.4.32   a3d022efb997b490b9dba79c397013ac-808661487.us-west-2.elb.amazonaws.com   80:30874/TCP   58s




	

==============================================================

Jobs

==============================================================

- There are different types of workloads (Web, application and database) that a container can serve. 

-  We have deployed simple web servers that serve users. These workloads are meant to continue to run for a long period of time, until manually taken down. There are other kinds of workloads such as batch processing, analytics or reporting that are meant to carry out a specific task and then finish. 

- For example, performing a computation, processing an image, performing some kind of analytics on a large data set, generating a report and sending an email etc. These are workloads that are meant to live for a short period of time, perform a set of tasks and then finish.

- A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. Suspending a Job will delete its active Pods until the Job is resumed again.

- A simple case is to create one Job object in order to reliably run one Pod to completion. The Job object will start a new Pod if the first Pod fails or is deleted (for example due to a node hardware failure or a node reboot).

- You can also use a Job to run multiple Pods in parallel.

- an example Job config. It computes PI to 2000 places and prints it out. It takes around 10s to complete.

controllers/job.yaml
---------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4


$ kubectl create job one-off3 --image=alpine --dry-run=client -o yaml -- sleep 30
apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: one-off3
spec:
  template:
    metadata:
      creationTimestamp: null
    spec:
      containers:
      - command:
        - sleep
        - "30"
        image: alpine
        name: one-off3
        resources: {}
      restartPolicy: Never
status: {}


$ kubectl get job one-off -o yaml

apiVersion: batch/v1
kind: Job
metadata:
  labels:
    controller-uid: 078adfab-91c2-460d-baa2-51d80b0fc03b
    job-name: one-off
  name: one-off
spec:
  backoffLimit: 6
  completions: 1
  parallelism: 1
  selector:
    matchLabels:
      controller-uid: 078adfab-91c2-460d-baa2-51d80b0fc03b
  template:
    metadata:
      creationTimestamp: null
      labels:
        controller-uid: 078adfab-91c2-460d-baa2-51d80b0fc03b
        job-name: one-off
    spec:
      containers:
      - command:
        - sleep
        - "30"
        image: alpine
        imagePullPolicy: Always
        name: one-off
      dnsPolicy: ClusterFirst
      restartPolicy: Never
status:
  completionTime: "2022-02-21T09:52:43Z"
  conditions:
  - lastProbeTime: "2022-02-21T09:52:43Z"
    lastTransitionTime: "2022-02-21T09:52:43Z"
    status: "True"
    type: Complete
  startTime: "2022-02-21T09:52:09Z"
  succeeded: 1


- backoffLimit: Number of times a Job will retry before marking a Job as failed
- completions: Number of Pod completions the Job needs before being considered a success
- parallelism: Number of Pods the Job is allowed to run in parallel
- spec.template.spec.restartPolicy: Job Pods default to never attempting to restart. Instead, the Job is responsible for managing the restart of failed Pods.


- The activeDeadlineSeconds and ttlSecondsAfterFinished are useful for automatically terminating and deleting Jobs.

- The Pods will remain until you delete them or the Job associated with them. Setting a Job's ttlSecondsAfterFinished can free you from manually cleaning up the Pods.



$ kubectl describe jobs/pi

Name:           pi
Namespace:      default
Selector:       controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
Labels:         controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
                job-name=pi
Annotations:    kubectl.kubernetes.io/last-applied-configuration:
                  {"apiVersion":"batch/v1","kind":"Job","metadata":{"annotations":{},"name":"pi","namespace":"default"},"spec":{"backoffLimit":4,"template":...
Parallelism:    1
Completions:    1
Start Time:     Mon, 02 Dec 2019 15:20:11 +0200
Completed At:   Mon, 02 Dec 2019 15:21:16 +0200
Duration:       65s
Pods Statuses:  0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
           job-name=pi
  Containers:
   pi:
    Image:      perl
    Port:       <none>
    Host Port:  <none>
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  14m   job-controller  Created pod: pi-5rwd7


- To view completed Pods of a Job, use 
$ kubectl get pods

$ kubectl logs <pod_name>


- The output is similar to this:

3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901



In Docker
-------
-  The docker container comes up, performs the requested operation, prints the output and exits.

$ docker run ubuntu expr 3 + 2

- The return code of the operation performed is shown in the bracket as well. In this case since the task was completed successfully, the return code is zero. 

$ docker ps -a
CONTAINER ID 	IMAGE 	CREATED 		STATUS 						PORTS
45aacca36850 	ubuntu 	43 seconds ago 	Exited (0) 41 seconds ago


In Kubernetes
--------------
- When the pod is created, it runs a container performs the computation task and exits and the pod goes into a Completed state. But, It then recreates the container in an attempt to keep it running. Again the container performs the required computation task and exits. And kubernetes brings it up again. And this continuous to happen until a threshold is reached. 


	
pod-definition.yaml
--------------------
apiVersion: v1
kind: Pod
metadata:
  name: math-pod
spec:
  containers:
  - name: math-add
    image: ubuntu
    command: ['expr', '3', '+', '2']
	
$ kubectl create –f pod-definition.yaml

$ kubectl get pods
NAME 		READY 	STATUS 		RESTARTS 	AGE
math-pod 	0/1 	Completed 	0 			1d


- RestartPolicy (restartPolicy: Never):
	- Kubernetes wants your applications to live forever. The default behavior of PODs is to attempt to restart the container in an effort to keep it running.
	
	- the property restartPolicy set on the POD, by default set to 'Always'. And that is why the POD ALWAYS recreates the container when it exits. other values are:  'Never' or 'OnFailure'
	
pod-restart-definition.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: math-pod
spec:
  restartPolicy: Never   // default is: Always
  containers:
  - name: math-add
    image: ubuntu
    command: ['expr', '3', '+', '2']
	
	
Kubernetes Jobs
---------------
- We have large data sets that requires multiple pods to process the data in parallel. We want to make sure that all PODs perform the task assigned to them successfully and then exit.

- While a ReplicaSet is used to make sure a specified number of PODs are running at all times, a Job is used to run a set of PODs to perform a given task to completion.

- kind: Job, apiVersion is batch/v1
- under the spec section, just like in replicasets or deployments, we have template. And under template we move all of the content from pod definition specification.  

job-definition.yaml
-------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: math-add-job
spec:  // job spec
  template:
    spec: // pod spec
      restartPolicy: Never   // default is: Always
      containers:
      - name: math-add
        image: ubuntu
        command: ['expr', '3', '+', '2']
	  
$ kubectl create –f job-definition.yaml
OR
$ kubectl create job <job_name> --image <image_name> --dry-run=client -o yaml > job.yaml

$ kubectl get jobs
NAME 			DESIRED 	SUCCESSFUL 	AGE
math-add-job 	1 			1 			38


- We see that it is in a completed state with 0 Restarts, indicating that kubernetes did not try to restart the pod. 

$ kubectl get pods
NAME 				READY 	STATUS 		RESTARTS 	AGE
math-add-job-l87pn 	0/1 	Completed 	0 			2m


$ watch "kubectl get all"  // watch linux command runs the given expression every 2 sec

- Look at pod logs to see the output of the job
- In our case, we just had the addition performed on the command line inside the container. So the output should be in the pods standard output. The standard output of a container can be seen using the logs command.
- For example, if the job was created to process an image, the processed image stored in a persistent volume would be the output or if the job was to generate and email a report, then the email with the report would be the result of the job.

- *** Kubernetes Job will continously attempt to run the job execution untill the job returned success i.e. exit status 0
- ** BUT kubernets has a default limit untill when it will keep on attempting before failing the job (backoffLimit).

$ kubectl logs math-add-job-ld87pn
5


$ kubectl delete job math-add-job
job.batch "math-add-job" deleted


- describe on job shows the total no. of attempt to run the job to completion. (Pods Statuses:  0 Running / 3 Succeeded / 1 Failed) i.e. 3 Succeeded + 1 Failed = 4 attepmt.

$ kubectl describe job throw-dice-job 

Name:           throw-dice-job
Namespace:      default
Selector:       controller-uid=18b2a1b4-a10e-4b8c-88e3-a92b9d58390c
Labels:         controller-uid=18b2a1b4-a10e-4b8c-88e3-a92b9d58390c
                job-name=throw-dice-job
Annotations:    <none>
Parallelism:    1
Completions:    3
Start Time:     Thu, 30 Dec 2021 15:35:49 +0000
Completed At:   Thu, 30 Dec 2021 15:36:06 +0000
Duration:       17s
Pods Statuses:  0 Running / 3 Succeeded / 1 Failed
Pod Template:
  Labels:  controller-uid=18b2a1b4-a10e-4b8c-88e3-a92b9d58390c
           job-name=throw-dice-job
  Containers:
   throw-dice-job:
    Image:        kodekloud/throw-dice
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From            Message
  ----    ------            ----   ----            -------
  Normal  SuccessfulCreate  4m2s   job-controller  Created pod: throw-dice-job-g74kv
  Normal  SuccessfulCreate  3m58s  job-controller  Created pod: throw-dice-job-rgcmq
  Normal  SuccessfulCreate  3m54s  job-controller  Created pod: throw-dice-job-zm588
  Normal  SuccessfulCreate  3m50s  job-controller  Created pod: throw-dice-job-cjmfs
  Normal  Completed         3m45s  job-controller  Job completed


Multiple Jobs - Sequential/Parallelism
---------------------------------------
- To run multiple pods for a given job definition, we set a value for completions under the job specification. And we set it to 3 to run 3 PODs. This time, when we create the job, We see the Desired count is 3, and the successful count is 0. 

- *** by default, the PODs are created one after the other (Sequential JOB execution). The second pod is created only after the first is finished.

job-definition.yaml
-------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: math-add-job
spec:
  completions: 3
  template:
    restartPolicy: Never   // default is: Always
    containers:
    - name: math-add
      image: ubuntu
      command: ['expr', '3', '+', '2']

$ kubectl create –f job-definition.yaml

$ kubectl get jobs
NAME 			DESIRED 	SUCCESSFUL 	AGE
math-add-job 	3 			3 			38

$ kubectl get pods
NAME 				READY 	STATUS 		RESTARTS 	AGE
math-add-job-l87pn 	0/1 	Completed 	0 			2m
math-add-job-87g4m 	0/1 	Completed 	0 			2m
math-add-job-d5z95 	0/1 	Completed 	0 			2m


-  But if the pods fail to run the job? ex: using a image: kodekloud/random-error which randomly completes or fails.

- When I create this job, first pod completes successfully, the second one fails, so a third one is created and that completes  successfully and the fourth one fails, and so does the fifth one and so to have 3 completions, the job creates a new pod which happen to complete successfully. And that completes the job.

job-random-definition.yaml
--------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name:  random-error-job
spec:
  completions: 3
  template:
    restartPolicy: Never   // default is: Always
    containers:
    - name: random-error
      image: kodekloud/random-error


$ kubectl create –f job-random-definition.yaml

$ kubectl get jobs
NAME 				DESIRED 	SUCCESSFUL 	AGE
random-error-job 	3 			 			38s

$ kubectl get pods
NAME 					READY 	STATUS 		RESTARTS
random-exit-job-ktmtt 	0/1 	Completed 	0
random-exit-job-sdsrf 	0/1 	Error 		0
random-exit-job-wwqbn 	0/1 	Completed 	0
random-exit-job-fkhfn 	0/1 	Error 		0
random-exit-job-fvf5t 	0/1 	Error 		0
random-exit-job-nmghp 	0/1 	Completed 	0



Parallelism
-------------
- Instead of getting the pods created sequentially we can get them created in parallel. 


job-parallel-definition.yaml
------------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name:  random-error-job
spec:
  completions: 3
  parallelism: 3  // create 3 pods in parallel, if we dont specify: will create pod sequentially after one is completed.
  template:
    restartPolicy: Never   // default is: Always
    containers:
    - name: random-error
      image: kodekloud/random-error	

$ kubectl create –f job-parallel-definition.yaml

$ kubectl get jobs
NAME 				DESIRED 	SUCCESSFUL 	AGE
random-error-job 	3 			3 			38s

- Two of which completes successfully. So we only need one more, so it’s intelligent enough to create one pod at a time until we get a total of 3 completed pods

$ kubectl get pods
NAME 					READY 	STATUS 		RESTARTS
random-exit-job-ktmtt 	0/1 	Completed 	0
random-exit-job-sdsrf 	0/1 	Error 		0
random-exit-job-wwqbn 	0/1 	Completed 	0
random-exit-job-fkhfn 	0/1 	Error 		0
random-exit-job-fvf5t 	0/1 	Error 		0





==========================================================================

CronJobs

==========================================================================

- A cronjob is a job that can be scheduled. Just like cron tab in Linux

- Say for example you have a job that generates a report and sends an email. the kind: Job runs instantly. Instead you could create a cronjob (kind: CronJob) to schedule and run it periodically as per the given cron expression

cron-job-definition.yaml
------------------------
apiVersion: batch/v1beta1
kind: CronJob
metadata:
spec:
  schedule: “*/1 * * * *”
  jobTemplate:
    spec:
	  completions: 3
	  parallelism: 3
	  template:
	    spec:
		  restartPolicy: Never
		  containers:
		  - name: reporting-tool
		    image: reporting-tool
			  
$ kubectl create –f cron-job-definition.yaml
OR
$ kubectl create cronjob throw-dice-cron-job --image kodekloud/throw-dice --schedule "30 21 * * *" --dry-run=client -o yaml > cronjob.yaml

cronjob.yaml
------------
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  jobTemplate:
    metadata:
      name: throw-dice-cron-job
    spec:
      template:
        spec:
          containers:
          - image: kodekloud/throw-dice
            name: throw-dice-cron-job
          restartPolicy: OnFailure
  schedule: 30 21 * * *

$ kubectl get cronjob




cronjob-example.yaml
---------------------

apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: cronjob-example
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - image: alpine
            name: fail
            command: ['date']
          restartPolicy: Never
		  
		  
$ kubectl create -f cronjob-example.yaml

- The CronJob spec is mainly a schedule with a template for creating Jobs (jobTemplate). 

- Observe the Events of the CronJob to confirm that it is creating Jobs every minute: 
$ watch kubectl describe cronjob cronjob-example




================================================================================

Networking Basic in Kubernetes

================================================================================

- Unlike in the docker world where an IP address is always assigned to a Docker CONTAINER, in Kubernetes the IP address is assigned to a POD. Each POD in kubernetes gets its own internal IP Address.

- When you deploy multiple PODs, they all get a separate IP assigned. The PODs can communicate to each other through this IP. But accessing other PODs using this internal IP address MAY not be a good idea as its subject to change when PODs are recreated. 

- When a kubernetes cluster is SETUP, kubernetes does NOT automatically setup any kind of networking to handle these issues.

	- all the containers or PODs in a kubernetes cluster MUST be able to communicate with eacg other without having to configure NAT
	
- Fortunately, we don’t have to set it up ALL on our own as there are multiple pre-built solutions available. Some of them are the cisco ACI networks, Cilium, Big Cloud Fabric, Flannel, Vmware NSX-t and Calico.
	- Depending on the platform you are deploying your Kubernetes cluster on you may use any of these solutions.
	
	
	



=============================================================================	

Kubernetes Services

=============================================================================

- Kubernetes Services enable communication between various components within and outside of the application.

- Kubernetes Services helps us connect applications together with other applications or users, front-end, back-end, external data source
	- Services enable the front-end application to be made available to users via Service (NodePort)
	-  it helps communication between back-end and front-end PODs via service (ClusterIp)
	
- Types of services:
	- NodePort
	-----------
		-  NodePort service listens to a port on the Node and forwards requests to PODs. NodePort were the service makes an internal POD accessible on a Port on the Node
		-  3 ports are involved. 
			- the port on the POD where the actual web server is running is port 80. referred as targetPort, because that is where the service forwards the requests to. 
			- the port on the service itself. referred as the port. The service is in fact like a virtual server inside the node. Inside the cluster it has its own IP address. And that IP address is called the Cluster-IP of the service. 
			- the port on the Node itself which we use to access the web server externally, known as the NodePort. NodePorts can only be in a valid range which is from 30000 to 32767
			
service-definition.yml
----------------------
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
  - targetPort: 80
	port: 80
	nodePort: 30008  // it should be a free port, if not provided, k8s assigns any random free port from reserved 30000 to 32767
  selector:
	app: front-end


> kubectl create –f service-definition.yml
service "myapp-service" created

> kubectl get services
NAME 			TYPE 		CLUSTER-IP 		EXTERNAL-IP 	PORT(S) 		AGE
kubernetes 		ClusterIP 	10.96.0.1 		<none> 			443/TCP 		16d
myapp-service 	NodePort 	10.106.127.123 	<none> 			80:30008/TCP 	5m
	

> curl http://192.168.1.2:30008  // external user access the pod outside kubernetes cluster., using any node IP and the nodeport
> curl http://192.168.1.3:30008
> curl http://192.168.1.4:30008


			- selector links the service to the pod.

			- ports is an array hence we can have multiple such port mappings within a single service.
			
			- when the service is created, it looks for matching PODs with the labels and finds 3 of them. The service then automatically selects all the 3 PODs as endpoints to forward the external requests coming from the user.
			
			- uses a random algorithm to balance load, acts as a built-in load balancer to distribute load across different PODs.
	
	
	
	- ClusterIP (default service type)
	----------------------------------
		- service creates a virtual IP inside the cluster to enable communication between different services such as a set of front-end servers to a set of backend servers
		
		- You may have a number of PODs running a front-end web server, another set of PODs running a backend server, a set of PODs running a key value store like Redis, another set of PODs running a persistent database like MySQL. The requests are forwarded to one of the PODs under the service randomly.
		
		-  Each service gets an IP and name assigned to it inside the cluster and that is the name that should be used by other PODs to access the service.
		
		
service-definition.yml
----------------------
apiVersion: v1
kind: Service
metadata:
  name: back-end
spec:
  type: ClusterIP
  ports:
  - targetPort: 80
    port: 80
  selector:
	app: back-end
		

> kubectl create –f service-definition.yml


> kubectl get services
NAME 		TYPE 		CLUSTER-IP 		EXTERNAL-IP 	PORT(S) 	AGE
kubernetes 	ClusterIP 	10.96.0.1 		<none> 			443/TCP 	16d
back-end 	ClusterIP 	10.106.127.123 	<none> 			80/TCP 		2m




	-  LoadBalancer
	---------------
		- it provisions a load balancer for our service in supported cloud providers.
		
		- Using NodePort service type we expose the application to the end users on a high end port of the Nodes (30000 to 32767), but we cant share IP of kubernetes cluster nodes to end user.
		
		- For this, you will be required to setup a separate Load Balancer VM in your environment. In this case I deploy a new VM for load balancer purposes and configure it to forward requests that come to it to any of the Ips of the Kubernetes nodes. then configure my organizations DNS to point to this load balancer when a user access http://myapp.com
		
		-  Now setting up that load balancer by myself is a tedious task, and I might have to do that in my local or onprem environment. However, if I happen to be on a supported CloudPlatform, like Google Cloud Platform, I could leverage the native load balancing functionalities of 
the cloud platform to set this up. 

		- Kubernetes has built-in integration with supported cloud platforms.


service-definition.yml
-----------------------
apiVersion: v1
kind: Service
metadata:
  name: front-end
spec:
  type: LoadBalancer
  ports:
  - targetPort: 80
    port: 80
  selector:
    app: myapp
    type: front-end



> kubectl create –f service-definition.yml
service “front-end" created


> kubectl get services
NAME 			TYPE 		CLUSTER-IP 		EXTERNAL-IP 	PORT(S) 	AGE
kubernetes 		ClusterIP 	10.96.0.1 		<none> 			443/TCP 	16d
front-end 		LoaBalancer 10.106.127.123 	<Pending> 		80/TCP 		2m



- Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.

	- ClusterIP: Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType.
	
	- NodePort: Exposes the Service on each Node's IP at a static port (the NodePort). A virtual ClusterIP Service, to which the NodePort Service routes, is automatically created. You'll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort>.
	
	- LoadBalancer: Exposes the Service externally using a cloud provider's load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. 
	In that case, instead of creating a service of type NodePort for your wear application, you could set it to type LoadBalancer. When you do that Kubernetes would still do everything that it has to do for a NodePort, which is to provision a high port for the service, but in addition to that kubernetes also sends a request to Google Cloud Platform to provision a native load balancer for this service. GCP would then automatically deploy a LoadBalancer configured to route traffic to the service ports on all the nodes and return its information to kubernetes. The LoadBalancer has an external IP that can be provided to users to access the application. In this case we set the DNS to point to this IP and users access the application using the URL.



Discovering services
-----------------------------------------

- Kubernetes supports 2 primary modes of finding a Service - environment variables and DNS.

- Environment variables

When a Pod is run on a Node, the kubelet adds a set of environment variables for each active Service. It adds {SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT variables, where the Service name is upper-cased and dashes are converted to underscores. It also supports variables (see makeLinkVariables) that are compatible with Docker Engine's "legacy container links" feature.

For example, the Service redis-master which exposes TCP port 6379 and has been allocated cluster IP address 10.0.0.11, produces the following environment variables:

REDIS_MASTER_SERVICE_HOST=10.0.0.11
REDIS_MASTER_SERVICE_PORT=6379
REDIS_MASTER_PORT=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP_PROTO=tcp
REDIS_MASTER_PORT_6379_TCP_PORT=6379
REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11


- When you have a Pod that needs to access a Service, and you are using the environment variable method to publish the port and cluster IP to the client Pods, you must create the Service before the client Pods come into existence. Otherwise, those client Pods won't have their environment variables populated.




- DNS

- You can (and almost always should) set up a DNS service for your Kubernetes cluster using an add-on.

- If DNS has been enabled throughout your cluster then all Pods should automatically be able to resolve Services by their DNS name.

- For example, if you have a Service called my-service in a Kubernetes namespace my-ns, the control plane and the DNS Service acting together create a DNS record for my-service.my-ns. Pods in the my-ns namespace should be able to find the service by doing a name lookup for my-service (my-service.my-ns would also work).

Pods in other namespaces must qualify the name as my-service.my-ns. These names will resolve to the cluster IP assigned for the Service


=======================================================================================

Naming comvension as per DNS Subdomain Names

=======================================================================================
- Most resource types require a name that can be used as a DNS subdomain name
- This means the name must:
	- contain no more than 253 characters
	- contain only lowercase alphanumeric characters, '-' or '.'
	- start with an alphanumeric character
	- end with an alphanumeric character
	

- RFC 1123 Label Names
This means the name must:
	- contain at most 63 characters
	- contain only lowercase alphanumeric characters or '-'
	- start with an alphanumeric character
	- end with an alphanumeric character


- Multi-Port Services

For some Services, you need to expose more than one port. Kubernetes lets you configure multiple port definitions on a Service object. When using multiple ports for a Service, you must give all of your ports names so that these are unambiguous. For example:

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
    - name: https
      protocol: TCP
      port: 443
      targetPort: 9377



- Choosing your own IP address
	- You can specify your own cluster IP address as part of a Service creation request. To do this, set the .spec.clusterIP field. For example, if you already have an existing DNS entry that you wish to reuse, or legacy systems that are configured for a specific IP address and difficult to re-configure.

	- The IP address that you choose must be a valid IPv4 or IPv6 address from within the service-cluster-ip-range CIDR range that is configured for the API server. If you try to create a Service with an invalid clusterIP address value, the API server will return a 422 HTTP status code to indicate that there's a problem


=======================================================================================

Ingress Resource and Ingress controller

=======================================================================================

- read the info detailed in Ingress section in KodeKloud-Kubernetes+-CKAD.pdf to understand the pain points of using NodePort/LoadBalancer service types in exposing the applications running on pods via deployments.

- Ingress helps your users access your application using a single Externally accessible URL, that you can configure to route to different services within your cluster based on the URL path, at the same time terminate TLS.

-  ingress as a layer 7 load balancer built-in to the kubernetes cluster that can be configured using native kubernetes primitives

- Ingress controller: Now remember, even with Ingress you still need to expose it to make it accessible outside the cluster. So you still have to either publish it as a NodePort or with a Cloud Native LoadBalancer. But that is just a one time thing. Going forward you are going to perform all your load balancing, Auth, SSL and URL based routing configurations on the Ingress controller

	- Without ingress, we would use a reverse-proxy or a load balancing solution like NGINX or HAProxy or Traefik. I would deploy them on my 
kubernetes cluster and configure them to route traffic to other services. The configuration involves defining URL Routes, SSL certificates etc.

	- Ingress is implemented by Kubernetes in the same way. You first deploy a supported 
solution (NGINX, Contour, HAPROXY, TRAFIK and Istio PODs  deployed as deployments), and then specify a set of rules to configure Ingress. The solution you deploy is called as an Ingress Controller. 

- ress resources: the set of rules/routing you configure is called as Ingress Resources. Ingress resources are created using definition files.


Ingress controller
----------------------------------------------------------
- a kubernetes cluster does NOT come with an Ingress Controller by default. So if you simply create ingress resources and expect them to work, 
they wont.

- There are a number of solutions available for Ingress, a few of them being GCE - which is Googles Layer 7 HTTP Load Balancer. NGINX, Contour, HAPROXY, TRAFIK and Istio. Out of this, GCE and NGINX are currently being supported and maintained by the Kubernetes project. 
	
- we will use NGINX as an example. An NGINX Controller is deployed as just another deployment in Kubernetes.

- the image used is nginx-ingress-controller with the right version. This is a special build of NGINX built specifically to be used as 
an ingress controller in kubernetes. So it has its own requirements. Within the image the nginx program is stored at location /nginx-ingress-controller. So you must pass that as the command to start the nginx-service. 

- it has a set of configuration options such as the path to store the logs, keep-alive threshold, ssl settings, session timeout etc. In order to decouple these configuration data from the nginx-controller image, you must create a ConfigMap object and pass that in.

- Now remember the ConfigMap object need not have any entries at this point. A blank object will do. But creating one makes it easy for you to modify a configuration setting in the future. You will just have to add it in to this ConfigMap.


- You must also pass in two environment variables that carry the POD’s name and namespace it is deployed to. The nginx service requires these to read the configuration data from within the POD. 

- Specify the ports (http and https) used by the ingress controller. 


ingress-controller-configmap.yaml
-----------------------------------
kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration



ingress-controller-deployment.yaml
-----------------------------------
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      labels:
        name: nginx-ingress
    spec:
      containers:
      - name: nginx-ingress-controller
        image: quay.io/kubernetes-ingresscontroller/nginx-ingress-controller:0.21.0
        args:
        - /nginx-ingress-controller
        - --configmap=$(POD_NAMESPACE)/nginx-configuration
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
          ports:
          - name: http
            containerPort: 80
          - name: https
            containerPort: 443



- We then need a service to expose the ingress controller to the external world. So we create a service of type NodePort with the nginx-ingress label selector to link the service to the deployment. 


ingress-controller-service.yaml
-----------------------------------
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec:
  type: NodePort
ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  - port: 443
    targetPort: 443
    protocol: TCP
    name: https
selector:
  name: nginx-ingress


- So with these three objects (1 configMap (nginx configs), 1 deployment for ingress-controller(nginx), 1 NodePort service for exposing the ingress-controller to external world) we should be ready with an ingress controller in its simplest form. 


> kubectl create -f ingress-controller-configmap.yaml

> kubectl create -f ingress-controller-deployment.yaml


> kubectl create -f ingress-controller-service.yaml




Ingress Resources
-------------------------------------------
- An Ingress Resource is a set of rules and configurations applied on the ingress controller. 

- Ingress resources deployed in the cluster gets automatically detected by the ingress controller.

- You can configure rules to say, 

1/ simply forward all incoming traffic to a single application, 

Ingress-wear.yaml
-----------------
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear
spec:
  backend:
    serviceName: wear-service
    servicePort: 80

> kubectl create –f Ingress-wear.yaml
ingress.extensions/ingress-wear created


> kubectl get ingress
NAME 			HOSTS 	ADDRESS 	PORTS 
ingress-wear 	* 		80 			2s


here, traffic is routed to the application services and not PODs directly. The Backend section defines where the traffic will be routed to. 
So if it’s a single backend, then you don’t really have any rules.




2/ route traffic to different applications (deployments) based on the URL. 
ex: 
my-online-store.com/wear gets routed to wear app service created for wear app deployment, 
my-online-store.com/watch gets routed to watch app service created for wear app deployment.


Ingress-wear-watch.yaml
---------------------------
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - http:
      paths:
      - path: /wear
	    backend:
          serviceName: wear-service
          servicePort: 80
      - path: /watch
		backend:
		  serviceName: watch-service
		  servicePort: 80


> kubectl describe ingress ingress-wear-watch

Name: ingress-wear-watch
Namespace: default
Address:
Default backend: default-http-backend:80 (<none>)
Rules:
	Host 	Path 		Backends
	---- 	---- 		--------
	*
			/wear 		wear-service:80 (<none>)
			/watch 		watch-service:80 (<none>)
Annotations:
Events:
	Type 	Reason 	Age 	From 						Message
	---- 	------ 	---- 	---- 						-------
	Normal 	CREATE 	14s 	nginx-ingress-controller 	Ingress default/ingress-wear-watch



here, since the domain is same (my-online-store.com), we just need single rule, but need multiple paths (/wear and /watch) for each URL paths.

*** NOte, we also need to deploy an additional service: default-http-backend, in case URLs that does not match any of these rules, then the user is 
directed to the service specified as the default backend. i.e.  default backend service to display this 404 Not Found error page



3/
route user based on the different domain names itself. i.e.
wear.my-online-store.com
watch.my-online-store.com


ingress-wear-watch.yaml
------------------------
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - host: wear.my-online-store.com
    http:
      paths:
        - backend:
          serviceName: wear-service
          servicePort: 80
		
  - host: watch.my-online-store.com
    http:
      paths:
      - backend:
        serviceName: watch-service
        servicePort: 80



here,  Now that we have two domain names, we create two rules. One for each domain. To split traffic by domain name, we use the 
host field. The host field in each rule matches the specified value with the domain name used in the request URL and routes traffic to the appropriate backend.

- You can still have multiple path specifications in each of these to handle different URL paths.



TLS
- You can secure an Ingress by specifying a Secret that contains a TLS private key and certificate. The Ingress resource only supports a single TLS port, 443, and assumes TLS termination at the ingress point (traffic to the Service and its Pods is in plaintext).
- 


Exercise:
---------------------------------

- describe service
$ kubectl -n critical-space describe service

Name:              pay-service
Namespace:         critical-space
Labels:            <none>
Annotations:       <none>
Selector:          app=webapp-pay
Type:              ClusterIP
IP Families:       <none>
IP:                10.100.108.183
IPs:               10.100.108.183
Port:              <unset>  8282/TCP
TargetPort:        8080/TCP
Endpoints:         10.244.0.9:8080
Session Affinity:  None
Events:            <none>


$ kubectl get deployments.apps --all-namespaces 
NAMESPACE       NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
app-space       default-backend            1/1     1            1           2m23s
app-space       webapp-video               1/1     1            1           2m23s
app-space       webapp-wear                1/1     1            1           2m23s
ingress-space   nginx-ingress-controller   1/1     1            1           2m24s
kube-system     coredns                    2/2     2            2           6m43s


$ kubectl get ingress -n app-space 
NAME                 CLASS    HOSTS   ADDRESS   PORTS   AGE
ingress-wear-watch   <none>   *                 80      4m35s



- describe ingress resource definition
$ kubectl -n app-space describe ingress ingress-wear-watch
Name:             ingress-wear-watch
Namespace:        app-space
Address:          
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
Rules:
  Host        Path  Backends
  ----        ----  --------
  *           
              /wear    wear-service:8080 (10.244.0.7:8080)
              /watch   video-service:8080 (10.244.0.4:8080)
Annotations:  nginx.ingress.kubernetes.io/rewrite-target: /
              nginx.ingress.kubernetes.io/ssl-redirect: false
Events:       <none>

$ kubectl -n app-space get ingress ingress-wear-watch -o yaml > ingress.yaml


$ kubectl -n app-space delete ingress ingress-wear-watch 
ingress.networking.k8s.io "ingress-wear-watch" deleted

$ kubectl apply -f ingress.yaml 
ingress.networking.k8s.io/ingress-wear-watch created

$ kubectl get ingress -n app-space 
NAME                 CLASS    HOSTS   ADDRESS   PORTS   AGE
ingress-wear-watch   <none>   *                 80      10s




- Ingress controller exercise:
-----------------------------------------------------------------------------------------------
- create a namespace:
$ kubectl create namespace ingress-space
namespace/ingress-space created

$ kubectl get namespaces

NAME              STATUS   AGE
app-space         Active   91s
default           Active   4m46s
ingress-space     Active   3s
kube-node-lease   Active   4m49s
kube-public       Active   4m49s
kube-system       Active   4m50s

- create a empty configMap
$ kubectl create configmap nginx-configmap -n ingress-space 
configmap/nginx-configmap created

- create a service account
$ kubectl -n ingress-space create serviceaccount ingress-serviceaccount
serviceaccount/ingress-serviceaccount created

- created the Roles and RoleBindings for the ServiceAccount.
$ kubectl -n ingress-space get roles.rbac.authorization.k8s.io              
NAME           CREATED AT
ingress-role   2022-01-01T18:08:19Z

$ kubectl -n ingress-space describe roles.rbac.authorization.k8s.io ingress-role 
Name:         ingress-role
Labels:       app.kubernetes.io/name=ingress-nginx
              app.kubernetes.io/part-of=ingress-nginx
Annotations:  <none>
PolicyRule:
  Resources   Non-Resource URLs  Resource Names                     Verbs
  ---------   -----------------  --------------                     -----
  configmaps  []                 []                                 [get create]
  configmaps  []                 [ingress-controller-leader-nginx]  [get update]
  endpoints   []                 []                                 [get]
  namespaces  []                 []                                 [get]
  pods        []                 []                                 [get]
  secrets     []                 []                                 [get]
  
$ kubectl -n ingress-space get rolebindings.rbac.authorization.k8s.io 
NAME                   ROLE                AGE
ingress-role-binding   Role/ingress-role   4m37s

$ kubectl -n ingress-space describe rolebindings.rbac.authorization.k8s.io ingress-role-binding 
Name:         ingress-role-binding
Labels:       app.kubernetes.io/name=ingress-nginx
              app.kubernetes.io/part-of=ingress-nginx
Annotations:  <none>
Role:
  Kind:  Role
  Name:  ingress-role
Subjects:
  Kind            Name                    Namespace
  ----            ----                    ---------
  ServiceAccount  ingress-serviceaccount
  
  
- deploy the Ingress Controller. ingress controller of type nginx image is used here.
- NOTE: ingress controller is deployed as regular deployments hence it gets, Deployment, ReplicaSet and Pod at the end.

ingress-controller.yaml
------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-controller
  namespace: ingress-space
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      labels:
        name: nginx-ingress
    spec:
      serviceAccountName: ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --default-backend-service=app-space/default-http-backend
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
			  


$ kubectl -n app-space get deployments.apps default-backend -o yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: default-backend
  namespace: app-space
spec:
  replicas: 1
  selector:
    matchLabels:
      app: default-backend
  template:
    metadata:
      labels:
        app: default-backend
    spec:
      containers:
      - image: kodekloud/ecommerce:404
        imagePullPolicy: Always
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP
			  

$ kubectl create -f ingress-controller.yaml 
deployment.apps/ingress-controller created

$ kubectl -n ingress-space get all
NAME                                     READY   STATUS    RESTARTS   AGE
pod/ingress-controller-5857685bf-jr9cb   1/1     Running   0          85s

NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ingress-controller   1/1     1            1           85s

NAME                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/ingress-controller-5857685bf   1         1         1       85s
			  




- create a service for the ingress-controller deployment to make Ingress available to external users. 
- NOTE *** the selector gets automatically taken from the depoloyment as we are exposing the deployment.
- kubectl expose --help

$ kubectl -n ingress-space expose deployment ingress-controller --name ingress --type NodePort --port 80 --target-port 80 --dry-run=client -o yaml > ingress-svc.yaml

ingress-svc.yaml
----------------
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  name: ingress
  namespace: ingress-space    // added this manually, as no option to pass using kubectl imperative command above
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    NodePort: 30080			// added this manually, as no option to pass using kubectl imperative command above
  selector:
    name: nginx-ingress
  type: NodePort


$ kubectl apply -f ingress-svc.yaml 
service/ingress created




- Create the ingress resource to make the applications available at /wear and /watch on the Ingress service.

$ kubectl -n app-space get service
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
default-http-backend   ClusterIP   10.110.140.184   <none>        80/TCP     46m
video-service          ClusterIP   10.111.81.241    <none>        8080/TCP   46m
wear-service           ClusterIP   10.110.169.135   <none>        8080/TCP   46m

ingress-resource.yaml
---------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
            name: wear-service
            port:
              number: 8080
      - path: /watch
        pathType: Prefix
        backend:
          service:
            name: video-service
            port:
              number: 8080
			  
			  
			  
$ kubectl apply -f ingress-res.yaml 
ingress.networking.k8s.io/minimal-ingress created

$ kubectl -n app-space describe ingress minimal-ingress 
Name:             minimal-ingress
Namespace:        app-space
Address:          
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
Rules:
  Host        Path  Backends
  ----        ----  --------
  *           
              /wear    wear-service:8080 (10.244.0.6:8080)
              /watch   video-service:8080 (10.244.0.4:8080)
Annotations:  nginx.ingress.kubernetes.io/rewrite-target: /
Events:       <none>








===============================================================================

Network Policy

===============================================================================

-  you have a web server serving front-end to users, an app server serving backend API’s and a database server. The user sends in a request to the web server at port 80. The web server then sends a request to the API server at port 5000 in the backend. The API server then fetches data from the 
database server at port 3306. And then sends the data back to the user.

- two types of traffic here. Ingress and Egress.

-  For example, for a web server, the incoming traffic from the users is an Ingress Traffic. And the outgoing requests to the app server is Egress traffic.

-  remember you are only looking at the direction in which the traffic originated. The response back to the user

- the backend API server receives ingress traffic from the web server on port 80 and has egress traffic to port 3306 to the database server.

- from the database servers perspective, it receives Ingress traffic on port 3306 from the API server.

hence the traffic flow and rules will be:

1/ An Ingress rule that is required to accept HTTP traffic on port 80 on the web server.
2/ An Egress rule to allow traffic from the web server to port 5000 on the API server.
3/ An ingress rule to accept incoming traffic on port 5000 on the API server.
4/ An egress rule to allow traffic from api server to port 3306 on the database server.
5/ An egress rule to allow traffic to port 3306 on the database server.


Network Security
----------------------------------------

- So we have a cluster with a set of nodes hosting a set of pods and services. Each node has an IP address and so does each pod as well as service. One of the pre-requisite for networking in kubernetes, is whatever solution you implement, the pods should be able to communicate with each other without having to configure any additional settings, like routes. 

- all pods are on a virtual private network that spans across the nodes in the kubernetes cluster.

- Kubernetes is configured by default with an “All Allow” rule that allows traffic from any pod to any other pod or services for an given namespace.

-  For each component in the application we deploy a POD. One for the front-end web server, for the API server and one for the database. We create services to enable communication between the PODs as well as to the end user. 

- the security teams and audits require, the front-end web server should NOT be allowed to communicate with the database server directly. Hence a Network Policy is needed to allow traffic to the db server only from the api server.

- Network Policy acts like one more layer on top of selected PODs, to allow/deny access from other objects in kubernetes.

- A Network policy is another object in the kubernetes namespace. Just like PODs, ReplicaSets or Services. You apply a network policy on selected pods. 

- You link a network policy to one or more pods. You can define rules within the network policy. In this case I would say, only allow Ingress Traffic from the API Pod on Port 3306. Once this policy is created, it blocks all other traffic to the Pod and only allows traffic that matches the specified rule. Again, this is only applicable to the Pod on which the network policy is applied. 

- Ingress/Egress rules defined on the Network Policy on selected PODs will only be ALLOWED and rest all traffic will be DENIED.

-  under the spec section, we will first move the pod selector to apply this policy to the db pod and then the network rule.

- this policy applied on DB Pods, ingress rule from api-pod on port 3306, rest all traffics will be blocked,.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db
	  
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: api-pod
		  
	ports:
    - protocol: TCP
      port: 3306
	  

- Network Policies are enforced by the Network Solution implemented on the Kubernetes Cluster. 

- Not all network solutions support network policies. A few of them that are supported are kube-router, Calico, Romana and Weave-net. 

- If you used Flannel as the networking solution, it does not support network policies as of this recording

- remember, even in a cluster configured with a solution that does not support network policies, you can still create the policies, but they will just not be enforced. You will not get an error message saying the networking solution does not support network policies.


- *** podSelector: This selects particular Pods in the same namespace as the NetworkPolicy which should be allowed as ingress sources or egress destinations.



- An example NetworkPolicy might look like this:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
	  
	  

- Behavior of to and from selectors 
	- podSelector: This selects particular Pods in the same namespace as the NetworkPolicy which should be allowed as ingress sources or egress destinations.
	
	- namespaceSelector: This selects particular namespaces for which all Pods should be allowed as ingress sources or egress destinations.
	
	- namespaceSelector and podSelector: A single to/from entry that specifies both namespaceSelector and podSelector selects particular Pods within particular namespaces. Be careful to use correct YAML syntax; this policy:
	
	...
	  ingress:
	  - from:
		- namespaceSelector:
			matchLabels:
			  user: alice
		- podSelector:
			matchLabels:
			  role: client
	  ...
  
	- contains two elements in the from array, and allows connections from Pods in the local Namespace with the label role=client, or from any Pod in any namespace with the label user=alice.




- Default policies
	- By default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to and from pods in that namespace. 
	

- Default deny all ingress traffic. selects all pods but does not allow any ingress traffic to those pods.

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress




- Allow all ingress traffic

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-ingress
spec:
  podSelector: {}
  ingress:
  - {}
  policyTypes:
  - Ingress





- Default deny all egress traffic. selects all pods but does not allow any egress traffic from those pods.

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-egress
spec:
  podSelector: {}
  policyTypes:
  - Egress


- Allow all egress traffic

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress
spec:
  podSelector: {}
  egress:
  - {}
  policyTypes:
  - Egress



- Default deny all ingress and all egress traffic

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress




- Targeting a range of Ports. with the usage of the endPort field.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: multi-port-egress
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 32000
      endPort: 32768
	  
 


Exercises on Network Policy
----------------------------------------------------------------------------------

- *** Network policy wont be visible in kubectl get all command.

$ kubectl get netpol

NAME             POD-SELECTOR   AGE
payroll-policy   name=payroll   3m49s


$ kubectl describe netpol payroll-policy 

Name:         payroll-policy
Namespace:    default
Created on:   2022-01-02 02:24:42 +0000 UTC
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     name=payroll
  Allowing ingress traffic:
    To Port: 8080/TCP
    From:
      PodSelector: name=internal
  Not affecting egress traffic
  Policy Types: Ingress
  


- exercise:
- Create a network policy to allow traffic from the Internal application only to the payroll-service and db-service.
- Note: Network policy is applicable only to PODs and Namespaces and NOT on Services
--------------------------------------------
$ kubectl get all

NAME           READY   STATUS    RESTARTS   AGE
pod/external   1/1     Running   0          2m48s
pod/internal   1/1     Running   0          2m48s
pod/mysql      1/1     Running   0          2m48s
pod/payroll    1/1     Running   0          2m48s

NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/db-service         ClusterIP   10.99.126.77    <none>        3306/TCP         2m48s
service/external-service   NodePort    10.100.216.6    <none>        8080:30080/TCP   2m48s
service/internal-service   NodePort    10.105.72.135   <none>        8080:30082/TCP   2m48s
service/kubernetes         ClusterIP   10.96.0.1       <none>        443/TCP          31m
service/payroll-service    NodePort    10.111.126.95   <none>        8080:30083/TCP   2m48s

ingress-policy.yaml -- allows egress from internal pod to mysql and payroll pods.
----------------------------------------------------------------------------------
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306
  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080
	  

$ kubectl create -f ingress-policy.yaml

networkpolicy.networking.k8s.io/internal-policy created



$ kubectl get netpol     
  
NAME              POD-SELECTOR    AGE
internal-policy   name=internal   32s
payroll-policy    name=payroll    2m22s




$ kubectl describe netpol internal-policy

Name:         internal-policy
Namespace:    default
Created on:   2022-01-02 03:29:37 +0000 UTC
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     name=internal
  Not affecting ingress traffic
  Allowing egress traffic:
    To Port: 3306/TCP
    To:
      PodSelector: name=mysql
    ----------
    To Port: 8080/TCP
    To:
      PodSelector: name=payroll
  Policy Types: Egress




CloudAcademy Exercise:
------------------------
1. Inspect the deny-metadata network policy in the default namespace.

$ kubectl get networkpolicy deny-metadata -o yaml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
spec:
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 169.254.169.254/32
  podSelector: {}

- The policyTypes list can include one or both of Egress and Ingress depending on what type of traffic the policy applies to.
- In this case, only Egress is included, so the default ingress behavior of allowing all incoming traffic is not impacted
- A corresponding egress key is defined. If no egress key was defined, the default behavior would be to block all egress traffic.

- The egress policy whitelists all outgoing traffic (CIDR notation 0.0.0.0/0) except for the EC2 instance metadata IP address (CIDR notation 169.254.169.254/32). 


2. Create a pod in the default namespace and run /bin/sh in it:

$ kubectl run busybox --image=busybox --rm -it /bin/sh

3. Use the wget command to connect to https://google.com:

/ # wget https://google.com


4. Attempt to connect to the EC2 instance metadata endpoint:

/ # wget 169.254.169.254
Connecting to 169.254.169.254 (169.254.169.254:80)



5. Retry connecting to the EC2 instance metadata endpoint using a container in a different namespace:

$ kubectl create namespace test
$ kubectl run busybox --image=busybox --rm -it -n test /bin/sh

/ # wget 169.254.169.254
Connecting to 169.254.169.254 (169.254.169.254:80)
saving to 'index.html'

- This time the connection succeeds. The EC2 instance metadata endpoint can potentially be used by malicious containers to extract credentials and modify resources in your AWS account. That is why it is important to prevent access to the metadata endpoint.


6. Create a network policy resource file that uses pod selectors to allow incoming connections to a web server application tier from a cache tier:

cat > app-policy.yaml <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-tiers
  namespace: test
spec:
  podSelector:
    matchLabels:
      app-tier: web
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app-tier: cache
    ports:
    - port: 80
EOF

7. Create the network policy resource:

$ kubectl create -f app-policy.yaml

$ kubectl get netpol -n test
NAME        POD-SELECTOR   AGE
app-tiers   app-tier=web   2m22s


8. Create an Nginx web server pod in the web server tier:

$ kubectl run web-server -n test -l app-tier=web --image=nginx:1.15.1 --port 80


9. Create a busybox pod with app-tier=cache as label to test the ingress network communication for the app-tiers pod:

# Get the web server pod's IP address
$ web_ip=$(kubectl get pod -n test -o jsonpath='{.items[0].status.podIP}')


# Pass in the web server IP addpress as an environment variable
$ kubectl run busybox -n test -l app-tier=cache --image=busybox --env="web_ip=$web_ip" --rm -it /bin/sh


# Send a requst to the web server on port 80
/ # wget $web_ip
Connecting to 192.168.23.131 (192.168.23.131:80)
saving to 'index.html'


10. Repeat the test, but using a pod that is not in the cache tier:

$ kubectl run busybox2 -n test -l app-tier=backend --image=busybox --env="web_ip=$web_ip"

The request will never succeed.



==========================================================================================

Persistent volumes

==========================================================================================

persist data in Docker
--------------------------
- To persist data processed by the containers, we attach a volume to the containers when they are created. The data processed by the container is now placed in this volume, thereby retaining it permanently.


Docker Storage
--------------------------
- when we install docker on a system, it created the below folder structure at var/lib/docker:
- var/lib/docker
	- aufs
	- containers
	- image
	- volumes
- this is where docker stores all of its data (files related to images/containers etc.)
- any volumes created goes to volume folder.


Docker Volume Mount
---------------------------
$ docker volume ls      -----> list all the registered volumes.

- volume mount is to mount a location/directory of /var/lib/docker/volumes on docker host to the containres.

- when we stop the container, the entire CONTAINER layer gets purged along with the container. what if we want to persist specially the DB data files created by a DB container.

- to do this, we first need to create a volume, using "docker volume create data_volume", it creates a new folder "data_volume" under "/var/lib/docker/volumes/"

- then we run the "docker run -v data_volume:/var/lib/mysql mysql", this way we mount the external host location to the /var/lib/mysql directory of the mysql container. NOTE: "/var/lib/mysql" is the default location where mysql stores data files.

- Even we dont create the volume first, docker creates new volume if we use -v option during docker run.

$ docker run -v data_volume2:/var/lib/mysql mysql  

- -v is old syntax:

$ docker run \
--mount type=mount,source=data_volume2,target=/var/lib/mysql mysql

- source: location on docker host
- target: is location on the container

- this will create a new volume i.e. data_volume2 and then mount and start the container.

- we should see all these directories if we list /var/lib/docker/volumes/ directory on the docker host.


Docker Bind Mount (external - NAS)
--------------------------------------
- - Bind mount is to mount any location/directory on docker host to the containres.
- If we have an external NAS storage which is mounted on Docker host on a different directory than docker volueme directory (/var/lib/docker/volumes)
- in this case we need to give complete path ex: NAS mount is: /data/mysql

$ docker run -v /data/mysql:/var/lib/mysql mysql

OR

$ docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql




Volumes in Kubernetes
-------------------------------------
- Similar to Docker, the PODs created in Kubernetes are transient in nature

- When a POD is created to process data and then deleted, the data processed by it gets deleted as well. For this we attach a volume to the POD. The data generated by the POD is now stored in the volume



Volumes & Mounts in Kubernetes
-------------------------------------
- to retain the data processed by the POD, we need to create 
1/ a volume with its storage option (various options available) and 
2/ mount that volume on some path for the underlying container of the POD.

apiVersion: v1
kind: Pod
metadata:
  name: random-number-generator
spec:
  containers:
  - image: alpine
    name: alpine
    command: ["/bin/sh","-c"]
    args: ["shuf -i 0-100 -n 1 >> /opt/number.out;"]
    volumeMounts:
    - mountPath: /opt
      name: data-volume
  
  volumes:
  - name: data-volume
    hostPath:
      path: /data
	  type: Directory
	  

- a simple implementation of volumes

	- We have a single node kubernetes cluster. We create a simple POD that generates a random between 1 and 100 and writes that to a file at /data/number.out and then gets deleted along with the random number. 
	
	- To retain the number generated by the pod, we create a volume. And a Volume needs a storage.
	
	- When you create a volume you can chose to configure it’s storage in different ways. but for now we will simply configure it to use a directory on the host. In this case I specify a path /data on the host. This way any files created in the volume would be stored in the directory data on my node. 
	
	volumes:
	- name: data-volume
	  hostPath:
	    path: /data
	    type: Directory
	
	- Once the volume is created, to access it from a container we mount the volume to a directory inside the container. 
	
	-  use the volumeMounts field in each container to mount the data-volume to the directory /opt within the container. The random number will now be written to /opt mount inside the container, which happens to be on the data-volume which is in fact /data directory on the host.
	
	- When the pod gets deleted, the file with the random number still lives on the host.
	

- is not recommended for use in a multi-node cluster. This is because the PODs would use the /data directory on all the nodes, and expect all of them to be the same and have the same data. Since they are on different servers, they are in fact not the same, unless you configure some kind of external replicated clustered storage solution. 


- Kubernetes supports several types of standard storage solutions such as NFS, glusterFS, Flocker, FibreChannel, CephFS, ScaleIO or public cloud solutions like AWS EBS, Azure Disk or File or Google’s Persistent Disk. 


Volume from configMap
---------------------
- create a configMap

config-map-definition.yaml
---------------------------
apiVersion: v1
kind: ConfigMap
metadata:
	name: app-config
data:
	DB_host: mysql
	DB_user: root
	DB_password: passwd
	
$ kubectl create -f config-map-definition.yaml 
configmap/app-config created


$ kubectl get configmap

NAME               DATA   AGE
kube-root-ca.crt   1      9m3s
app-config         3      17s


$ kubectl describe configmap app-config 

Name:         app-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
DB_user:
----
root
DB_host:
----
mysql
DB_password:
----
passwd

BinaryData
====

Events:  <none>


- now create the pod using a volume of cofigMap

pod-definition.yaml
---------------------
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx:alpine
    name: nginx
    volumeMounts:
    - name: app-config-vol
      mountPath: /etc/app-config
  volumes:
  - name: app-config-vol
    configMap:
      name: app-config
	  

$ kubectl create -f pod.yaml 
pod/nginx created


$ kubectl get pod
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          7s

$ kubectl exec -it nginx -- ls -lrt /etc/app-config
total 0
lrwxrwxrwx    1 root     root            14 Jan 14 05:01 DB_user -> ..data/DB_user
lrwxrwxrwx    1 root     root            18 Jan 14 05:01 DB_password -> ..data/DB_password
lrwxrwxrwx    1 root     root            14 Jan 14 05:01 DB_host -> ..data/DB_host


$ kubectl exec -it nginx -- cat /etc/app-config/DB_user
root

$ kubectl exec -it nginx -- cat /etc/app-config/DB_password
passwd

$ kubectl exec -it nginx -- cat /etc/app-config/DB_host
mysql

- So every properties defined in the configMap ends up creating individual files inside the volume of pod.



Volume Types in Kubernetes
----------------------------------
- various storage options available. a/ hostPath, b/ configMap, c/ secret, d/ awsElasticBlockStore, e/ emptyDir

- hostPath
	- hostPath option for storage will be allocated on node where the POD is running, not for production.
	- A hostPath volume mounts a file or directory from the host node's filesystem into your Pod.
	- For example, some uses for a hostPath are:
		- running a container that needs access to Docker internals; use a hostPath of /var/lib/docker
	- In addition to the required path property, you can optionally specify a type for a hostPath volume.
	- The supported values for field type are:
		- DirectoryOrCreate:	If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet.
		- Directory:	A directory must exist at the given path
		- FileOrCreate:	If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet.
		- File:	A file must exist at the given path
		
apiVersion: v1
kind: Pod
metadata:
  name: test-webserver
spec:
  containers:
  - name: test-webserver
    image: k8s.gcr.io/test-webserver:latest
    volumeMounts:
    - mountPath: /opt/aaa1
      name: mydir
    - mountPath: /opt/aaa2/1.txt
      name: myfile
	- mountPath: /test-pd
      name: test-volume
  volumes:
  - name: mydir
    hostPath:
      path: /var/local/aaa     # Ensure the directory is created.
      type: DirectoryOrCreate
  - name: myfile
    hostPath:
      path: /var/local/aaa/1.txt  # Ensure the file file is created.
      type: FileOrCreate
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data                # directory location on host must exist
      # this field is optional
      type: Directory


- Cloud volume provider:
	- to configure an AWS Elastic Block Store volume as the storage or the volume, we replace hostPath field of the volume with awsElasticBlockStore field along with the volumeID and filesystem type. The Volume storage will now be on AWS EBS


volumes:
- name: data-volume
  awsElasticBlockStore:
    volumeID: <volume-id>
    fsType: ext4
	


emptyDir
--------
- An emptyDir volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. As the name says, the emptyDir volume is initially empty.

- All containers in the Pod can read and write the same files in the emptyDir volume, though that volume can be mounted at the same or different paths in each container.

- When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently.


apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}

	


Persistent Volumes in Kubernetes
-----------------------------------------
- Manage the Storage more centrally rather adding storage to every pod definitions.

- When we created volumes in the previous section we configured volumes within the POD definition file. So every configuration information required to configure storage for the volume goes within the pod definition file.


- Now, when you have a large environment with a lot of users deploying a lot of PODs, the users would have to configure storage every time for each POD. Whatever storage solution is used, the user who deploys the PODs would have to configure that on all POD definition files in his environment. Every time a change is to be made, the user would have to make them on all of his PODs. 


- You would like it to be configured in a way that an administrator can create a large pool of storage, and then have users carve out pieces from it as required.

- A Persistent Volume is a Cluster wide pool of storage volumes configured by an Administrator, to be used by users deploying applications on the cluster. The users can now select storage from this pool using Persistent Volume Claims.


pv-definition.yaml
-------------------
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi
  awsElasticBlockStore:
	volumeID: <volume-id>
	fsType: ext4

- Under spec, 
	- accessModes
		- Access Mode defines how the Volume should be mounted on the hosts. Weather in a ReadOnly mode, or ReadWrite mode. 
		- ReadOnlyMany, ReadWriteOnce or ReadWriteMany mode.

	- capacity
		- Specify the amount of storage to be reserved for this Persistent Volume. Which is set to 1GB here.
		
	- storage volume type
		- the hostPath option that uses storage from the node’s local directory. not to be used in a production environment.
		- public cloud storage options like: awsElasticBlockStore


kubectl create –f pv-definition.yaml
pv-definition.yaml

> kubectl get persistentvolume
NAME 	CAPACITY 	ACCESS MODES 	RECLAIM POLICY 		STATUS 		CLAIM 	STORAGECLASS 	REASON 	AGE
pv-vol1 1Gi 		RWO 			Retain 				Available 									3m




Ephemeral Volume Types in Kubernetes
----------------------------------------------------------------------
- In Kubernetes, ephemeral volumes have their lifetime tied to the lifetime of the Pod they are mounted in. Once the Pod is deleted, so is the volume and its data. 

- *** It is important to note that the data DOES survive across container restarts.


- There are a variety of use cases for ephemeral volumes, including:
	- Sharing data between containers in a Pod (multi-container Pods)
	- Providing read-only input configuration data to a Pod
	- Temporary data caches
	
- Providing read-only configuration data to Pods is best suited by using Kubernetes ConfigMaps and Secrets. 
	- When ConfigMaps and Secrets are mounted as volumes, the data is stored in an ephemeral volume.
	
	

1. Use an emptyDir volume to store the logs of a Pod's container that is recording random coin tosses:

cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: coin-toss
spec:
  containers:
  - name: coin-toss
    image: busybox:1.33.1
    command: ["/bin/sh", "-c"]
    args:
    - >
      while true;
      do
        # Record coint tosses
        if [[ $(($RANDOM % 2)) -eq 0 ]]; then echo Heads; else echo Tails; fi >> /var/log/tosses.txt;
        sleep 1;
      done
    # Mount the log directory /var/log using a volume
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  # Declare log directory volume an emptyDir ephemeral volume
  volumes:
  - name: varlog
    emptyDir: {}
EOF


- The fields for creating a volume (spec.volumes) and mounting the volume in the container (spec.containers.volumeMounts) are independent of whether you use ephemeral or persistent volumes. 

- The only way to know the Pod is using an ephemeral volume is by seeing the volume's emptyDir field. 

- In the manifest the braces (emptyDir: {}) symbolize an empty object because the emptyDir is using default configuration values. 

- The emptyDir volume type creates an empty directory for use by the Pod and shares the lifetime of the Pod, i.e. it is ephemeral. 

- The directory is created on the Node's file system by default and can therefore survive node restarts (assuming the Pod remains scheduled on the Node).



- List the contents of the volume on the Node that is running the Pod:
	- The volume starts empty but now has the tosses.txt file which logs the coin toss results for the Pod.
	- You will use kubectl exec to access the file in the coming instructions

pod_node=$(kubectl get pod coin-toss -o jsonpath='{.status.hostIP}')
pod_id=$(kubectl get pod coin-toss -o jsonpath='{.metadata.uid}')
ssh $pod_node -oStrictHostKeyChecking=no sudo ls /var/lib/kubelet/pods/$pod_id/volumes/kubernetes.io~empty-dir/varlog


*** If you are concerned about storing potentially sensitive data in a volume that is accessible on a Node's file system, you can configure emptyDir to use memory rather than disk storage. (emptyDir.medium field to "Memory")

$ kubectl explain pod.spec.volumes.emptyDir

- The medium can be configured to use tmpfs memory-based storage by setting medium: Memory. 

- While tmpfs is very fast, be aware that unlike disks, tmpfs is cleared on node reboot and any files you write count against your container's memory limit.

- This will result in higher performance than using disk-backed volumes. However, the memory used does counts against container memory limits. Note that when using memory (RAM) the data will not survive Node restarts in contrast to using disk. 


$ kubectl exec coin-toss -- wc -l /var/log/tosses.txt
452 /var/log/tosses.txt




- set the Pod to use a new image and wait for the Pod to restart its container:
- It takes under a minute to restart. so the watch on get pods shows 2 rows and finally the one with restart count 1 running

kubectl set image pod coin-toss coin-toss=busybox:1.34.0
kubectl get pods -w

NAME        READY   STATUS    RESTARTS   AGE
coin-toss   1/1     Running   0          9m2s
coin-toss   1/1     Running   1          9m39s



- re-issue the exec command to verify the count didn't restart from zero:

$ kubectl exec coin-toss -- wc -l /var/log/tosses.txt
969 /var/log/tosses.txt

- This confirms the ephemeral volume is tied to the Pod's lifetime, not the container's. If the ephemeral volume had not been used and only the container's writable layer storage was used, the data would have been lost on restarting the container. 



- Delete the Pod:

$ kubectl delete pod coin-toss


- Confirm the volume has been deleted by attempting to list the Pod's volume's contents on the Node hosting the Pod:
- This confirms the data is lost once the Pod is deleted.

$ ssh $pod_node -oStrictHostKeyChecking=no sudo ls /var/lib/kubelet/pods/$pod_id/volumes/kubernetes.io~empty-dir/varlog

No such file or directory



*** Because ephemeral storage is finite on each Node, you can configure requests and limits for it, just as you can for CPU and memory. 

- Create a Pod that uses an emptyDir ephemeral volume for its data and requests and limits the Pod's total ephemeral storage (ephemeral-storage) to the very low value of 1 kibibyte (1Ki):

cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: cache
spec:
  containers:
  - name: cache
    image: redis:6.2.5-alpine
    resources:
      requests:
        ephemeral-storage: "1Ki"
      limits:
        ephemeral-storage: "1Ki"
    volumeMounts:
    - name: ephemeral
      mountPath: "/data"
  volumes:
    - name: ephemeral
      emptyDir:
        sizeLimit: 1Ki
EOF

- The resources object is the same one used for CPU and memory although only ephemeral-storage is declared in this example.



- Describe the Pod and see what the result of the low ephemeral storage limit is:
$ kubectl describe pod cache

Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  13s   default-scheduler  Successfully assigned ephemeral/cache to ip-10-0-0-11.us-west-2.compute.internal
  Normal   Pulling    12s   kubelet            Pulling image "redis:6.2.5-alpine"
  Normal   Pulled     9s    kubelet            Successfully pulled image "redis:6.2.5-alpine" in 3.662684914s
  Normal   Created    8s    kubelet            Created container cache
  Normal   Started    8s    kubelet            Started container cache
  Warning  Evicted    3s    kubelet            Container cache exceeded its local ephemeral storage limit "1Ki".
  Normal   Killing    3s    kubelet            Stopping container cache
  
  
$ kubectl get po
NAME    READY   STATUS    RESTARTS   AGE
cache   0/1     Evicted   0          74s


- The Pod is Scheduled onto a Node because there is a Node in the cluster satisfying the ephemeral storage request. After the Pod is Started, its container exceeds the ephemeral storage limit set (Usage of EmptyDir volume "ephemeral" exceeds the limit "1Ki"). 

- The kubelet then evicts the Pod and it is subsequently killed.








Persistent Volume Claim
----------------------------------------------------------------------

- Persistent Volume Claim to make the storage available to a node

- Persistent Volumes and Persistent Volume Claims are two separate objects in the Kubernetes namespace.

- An Administrator creates a set of Persistent Volumes and a user creates Persistent Volume Claims to use the storage.

- Once the Persistent Volume Claims are created, Kubernetes binds the Persistent Volumes to Claims based on the request and properties set on the volume.

- There is a one-to-one relationship between Claims and Volumes

Binding
----------------------------------------------------------------------

- Once the Persistent Volume Claims are created, Kubernetes binds the Persistent Volumes to Claims based on the request and properties set on the volume. 

- Every Persistent Volume Claim is bound to a single Persistent volume. During the binding process, kubernetes tries to find a Persistent Volume that has sufficient Capacity as requested by the Claim, and any other requested properties such as Access Modes, Volume Modes, Storage Class etc. 

- However, if there are multiple possible matches for a single claim, and you would like to specifically use a particular Volume, you could still use labels and selectors to bind to the right volumes.

- a smaller Claim may get bound to a larger volume if all the other criteria matches and there are no better options. 

- There is a one-to-one relationship between Claims and Volumes, so no other claim can utilize the remaining capacity in the volume.

- If there are no volumes available the Persistent Volume Claim will remain in a pending state, until newer volumes are made available to the cluster. Once newer volumes are available the claim would automatically be bound to the newly available volume.

- persistentvolume-controller helps in binding PVC to pv.


- A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.

- Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster

pvc-definition.yaml
---------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
	  
	  
> kubectl create –f pvc-definition.yaml

> kubectl get persistentvolumeclaim
NAME 		STATUS 	VOLUME 	CAPACITY 	ACCESS MODES 
myclaim 	Pending



- When the claim is created, kubernetes looks at the volume created previously. The access Modes match. The capacity requested is 500 Megabytes but the volume is configured with 1 GB of storage. Since there are no other volumes available, the PVC is bound to the PV


- View PVCs

> kubectl get persistentvolumeclaim
NAME 		STATUS 	VOLUME 		CAPACITY 	ACCESS MODES 	STORAGECLASS 	AGE
myclaim 	Bound 	pv-vol1 	1Gi 		RWO 							43m



- Delete PVCs
- when PVC is deleted, Underlying PV is retained by default.

- 3 options for the mapped PVs in case PVC is deleted.
	- Retain (default). will remain until it is manually deleted by the administrator, BUT the PV will no longer be available for re-use by any other claims.
	
	- delete: This way as soon as the claim is deleted, the volume will be deleted as well. 
	
	- recycle: the data in the volume will be scrubbed before making it available to other claims

> kubectl delete persistentvolumeclaim myclaim
persistentvolumeclaim "myclaim" deleted


Claims As Volumes in Pod
-----------------------------------------
- Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. For volumes that support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.

- Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods and access their claimed PVs by including a persistentVolumeClaim section in a Pod's volumes block. 

- Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod's namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod.

- The volume is then mounted to the host and into the Pod.

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim


A Note on Namespaces
-------------------------------------
- PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with "Many" modes (ROX, RWX) is only possible within one namespace



Storage Object in Use Protection 
------------------------------------
- The purpose of the Storage Object in Use Protection feature is to ensure that PersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs) that are bound to PVCs are not removed from the system, as this may result in data loss.

- PVC is in active use by a Pod when a Pod object exists that is using the PVC.

- If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. PVC removal is postponed until the PVC is no longer actively used by any Pods. Also, if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is postponed until the PV is no longer bound to a PVC.

- *** Incase we try to delete a pvc while the same pvc is being used in a pod, the "kubectl delete pvc" command will be in terminating state.
$ kubectl delete pvc claim-log-1
persistentvolumeclaim "claim-log-1" deleted

$ kubectl get pvc
NAME          STATUS        VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim-log-1   Terminating   pv-log   100Mi      RWX                           10m




Reclaiming
--------------------------------------
- When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. 

- Currently, volumes can either be Retained (default), Recycled, or Deleted

- Retain
	- The Retain reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered "released". But it is not yet available for another claim because the previous claimant's data remains on the volume. An administrator can manually reclaim the volume with the following steps.
		- Delete the PersistentVolume. The associated storage asset in external infrastructure (such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume) still exists after the PV is deleted.
		- Manually clean up the data on the associated storage asset accordingly.
		- Manually delete the associated storage asset.
		
	- If you want to reuse the same storage asset, create a new PersistentVolume with the same storage asset definition
	
- Delete 
	- For volume plugins that support the Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume. 

- Recycle
	- Warning: The Recycle reclaim policy is deprecated. Instead, the recommended approach is to use dynamic provisioning.
	- If supported by the underlying volume plugin, the Recycle reclaim policy performs a basic scrub (rm -rf /thevolume/*) on the volume and makes it available again for a new claim.

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle



Reserving a PersistentVolume
---------------------------------------
- The control plane can bind PersistentVolumeClaims to matching PersistentVolumes in the cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.

- If other PersistentVolumeClaims could use the PV that you specify, you first need to reserve that storage volume. Specify the relevant PersistentVolumeClaim in the claimRef field of the PV so that other PVCs can not bind to it.

apiVersion: v1
kind: PersistentVolume
metadata:
  name: foo-pv
spec:
  storageClassName: ""
  claimRef:
    name: foo-pvc
    namespace: foo
  ...
  
- *** This is useful if you want to consume PersistentVolumes that have their claimPolicy set to Retain, including cases where you are reusing an existing PV


Volume Mode 
-----------------------------------------
- Kubernetes supports two volumeModes of PersistentVolumes: Filesystem and Block.

- volumeMode is an optional API parameter. Filesystem is the default mode

- A volume with volumeMode: Filesystem is mounted into Pods into a directory. If the volume is backed by a block device and the device is empty, Kubernetes creates a filesystem on the device before mounting it for the first time.

- You can set the value of volumeMode to Block to use a volume as a raw block device. Such volume is presented into a Pod as a block device, without any filesystem on it. This mode is useful to provide a Pod the fastest possible way to access a volume, without any filesystem layer between the Pod and the volume. . the application running in the Pod must know how to handle a raw block device. 		


end to end life cycle:
---------------------------
- Create a PersistentVolume 

pv-volume.yaml
--------------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"


- The configuration file specifies that the volume is at /mnt/data on the cluster's Node. The configuration also specifies a size of 10 gibibytes and an access mode of ReadWriteOnce, which means the volume can be mounted as read-write by a single Node. It defines the StorageClass name manual for the PersistentVolume, which will be used to bind PersistentVolumeClaim requests to this PersistentVolume.

$ kubectl apply -f pv-volume.yaml

$ kubectl get pv task-pv-volume

NAME             CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE
task-pv-volume   10Gi       RWO           Retain          Available             manual                   4s



- Create a PersistentVolumeClaim 

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi


$ kubectl apply -f pv-claim.yaml

$ kubectl get pv task-pv-volume

NAME             CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                   STORAGECLASS   REASON    AGE
task-pv-volume   10Gi       RWO           Retain          Bound     default/task-pv-claim   manual                   2m


$ kubectl get pvc task-pv-claim

NAME            STATUS    VOLUME           CAPACITY   ACCESSMODES   STORAGECLASS   AGE
task-pv-claim   Bound     task-pv-volume   10Gi       RWO           manual         30s



- Create a Pod

apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage


$ kubectl apply -f pv-pod.yaml

$ kubectl get pod task-pv-pod

$ kubectl exec -it task-pv-pod -- /bin/bash

- In your shell, verify that nginx is serving the index.html file from the hostPath volume:

# Be sure to run these 3 commands inside the root shell that comes from
# running "kubectl exec" in the previous step
apt update
apt install curl
curl http://localhost/


The output shows the text that you wrote to the index.html file on the hostPath volume:

Hello from Kubernetes storage


- Clean up

kubectl delete pod task-pv-pod
kubectl delete pvc task-pv-claim
kubectl delete pv task-pv-volume


open a new shell the same way that you did earlier.

In the shell on your Node, remove the file and directory that you created:

# This assumes that your Node uses "sudo" to run commands
# as the superuser
sudo rm /mnt/data/index.html
sudo rmdir /mnt/data


Mounting the same persistentVolume in two places
----------------------------------------------------

- Using subPath

Sometimes, it is useful to share one volume for multiple uses in a single pod. The volumeMounts.subPath property specifies a sub-path inside the referenced volume instead of its root.


pv-duplicate.yaml
------------------
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
    - name: test
      image: nginx
      volumeMounts:
        # a mount for site-data
        - name: config
          mountPath: /usr/share/nginx/html
          subPath: html
        # another mount for nginx config
        - name: config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
  volumes:
    - name: config
      persistentVolumeClaim:
        claimName: test-nfs-claim


mysql-duplicate.yaml
----------------------
apiVersion: v1
kind: Pod
metadata:
  name: my-lamp-site
spec:
    containers:
    - name: mysql
      image: mysql
      env:
      - name: MYSQL_ROOT_PASSWORD
        value: "rootpasswd"
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: site-data
        subPath: mysql
    - name: php
      image: php:7.0-apache
      volumeMounts:
      - mountPath: /var/www/html
        name: site-data
        subPath: html
    volumes:
    - name: site-data
      persistentVolumeClaim:
        claimName: my-lamp-site-data


- The PHP application's code and assets map to the volume's html folder and the MySQL database is stored in the volume's mysql folder.

- using subPath, it creates 2 separate folder under the same volume and then these folders can be mapped to corresponding mountPaths to containers

- You can perform 2 volume mounts on your nginx container:

/usr/share/nginx/html for the static website /etc/nginx/nginx.conf for the default config




Exercise:
-----------------------------------
1. Configure a volume to store these logs at /var/log/webapp on the host.

spec:
  containers:
    image: kodekloud/event-simulator
    imagePullPolicy: Always
	volumeMounts:
	- mountPath: /log
	  name: log-vol

volumes:
  - name: log-vol
    hostPath:
      path: /var/log/webapp
      type: DirectoryOrCreate


$ kubectl exec -it webapp -- ls -lrt /log
total 12
-rw-r--r--    1 root     root         11160 Jan 13 06:05 app.log

- file system lookup on same node.
$ ls -ltr /var/log/webapp/
total 12
-rw-r--r-- 1 root root 11675 Jan 13 06:05 app.log



2. Create a Persistent Volume with the given specification.
Volume Name: pv-log
Storage: 100Mi
Access Modes: ReadWriteMany
Host Path: /pv/log
Reclaim Policy: Retain

- https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume

- Kubernetes supports hostPath for development and testing on a single-node cluster. A hostPath PersistentVolume uses a file or directory on the Node to emulate network-attached storage.

- In a production cluster, you would not use hostPath. Instead a cluster administrator would provision a network resource like a Google Compute Engine persistent disk, an NFS share, or an Amazon Elastic Block Store volume. Cluster administrators can also use StorageClasses to set up dynamic provisioning.

pv-volume.yaml
--------------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
  labels:
    type: local
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /pv/log
	
$ kubectl create -f pv.yaml 
persistentvolume/pv-log created


$ kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv-log   100Mi      RWX            Retain           Available                                   11m


3. Let us claim some of that storage for our application. Create a Persistent Volume Claim with the given specification.

Volume Name: claim-log-1
Storage Request: 50Mi
Access Modes: ReadWriteOnce

pvc.yaml
----------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Mi
	  
$ kubectl get pvc
NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim-log-1   Pending                                                     6s


- since the access mode is different between pv vs pvc, the pvc will stay in pending state.

- changing the accessModes will result in binding the pvc to pv even if the sixe of pv is larger than pvc.
$ kubectl get pvc
NAME          STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim-log-1   Bound    pv-log   100Mi      RWX                           6s



- *** Incase we try to delete a pvc while the same pvc is being used in a pod, the "kubectl delete pvc" command will be in terminating state.
$ kubectl delete pvc claim-log-1
persistentvolumeclaim "claim-log-1" deleted

$ kubectl get pvc
NAME          STATUS        VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim-log-1   Terminating   pv-log   100Mi      RWX                           10m



- deleting the pod will result in deleting the pvc
$ kubectl delete pod webapp 
pod "webapp" deleted


$ kubectl get pvc
No resources found in default namespace.


$ kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                 STORAGECLASS   REASON   AGE
pv-log   100Mi      RWX            Retain           Released   default/claim-log-1                           33m




CloudAcademy Exercise:
--------------------------
# Create namespace
kubectl create namespace persistence
# Set namespace as the default for the current context
kubectl config set-context $(kubectl config current-context) --namespace=persistence

1. Create a PVC:

cat << 'EOF' > pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: db-data
spec:
  # Only one node can mount the volume in Read/Write
  # mode at a time
  accessModes:
  - ReadWriteOnce 
  resources:
    requests:
      storage: 2Gi
EOF
kubectl create -f pvc.yaml


$ kubectl get pvc

NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
db-data   Bound    pvc-4fc248a1-4010-4a74-ada3-9f43184b7ddc   2Gi        RWO            gp2            57s




- Similar information is displayed. There is a RECLAIM POLICY associated with the PV. The Delete policy means the PV is deleted once the PVC is deleted. It is also possible to keep the PV using other reclaim policies.

$ kubectl get pv

NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
pvc-4fc248a1-4010-4a74-ada3-9f43184b7ddc   2Gi        RWO            Delete           Bound    persistence/db-data   gp2                     59s


2. Create a Pod that mounts the volume provided by the PVC:

cat << 'EOF' > db.yaml
apiVersion: v1
kind: Pod
metadata:
  name: db 
spec:
  containers:
  - image: mongo:4.0.6
    name: mongodb
    # Mount as volume 
    volumeMounts:
    - name: data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
  volumes:
  - name: data
    # Declare the PVC to use for the volume
    persistentVolumeClaim:
      claimName: db-data
EOF
kubectl create -f db.yaml


- The Pod uses the MongoDB image, which is a NoSQL database that stores its database files at /data/db by default. The PVC is mounted as a volume at that path causing the database files to be written to the EBS Persistent volume.


3. Run the MongoDB CLI client to insert a document that contains the message "I was here" into a test database and then confirm it was inserted:

kubectl exec db -it -- mongo testdb --quiet --eval \
  'db.messages.insert({"message": "I was here"}); db.messages.findOne().message'

I was here


$ kubectl exec db -it -- mongo testdb --quiet --eval   'db.messages.findOne()'
{ "_id" : ObjectId("62158b352c7eea3d6494aa0e"), "message" : "I was here" }


4. Delete the Pod:  
- At this point, a regular (emptyDir) volume would be destroyed and the database files would be lost.

kubectl delete -f db.yaml

5. Create a new database Pod:
- Although the Pod is new, the Pod's spec refers to the same PVC as before.


6. Attempt to find a document in the test database:

$ kubectl exec db -it -- mongo testdb --quiet --eval 'db.messages.findOne()'
{ "_id" : ObjectId("62158b352c7eea3d6494aa0e"), "message" : "I was here" }





  
  
  




=========================================================================================

Storage Classes

=========================================================================================
- A StorageClass provides a way for administrators to describe the "classes" of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. Kubernetes itself is unopinionated about what classes represent. This concept is sometimes called "profiles" in other storage systems

- before this, we know, how to create PV and then claim that PV using PVC (gets bound based on requested size and accessMode) and finally use the PVC in POD as volumes.

in this case the problem is before creating the PV, we must have to create the disk on the cloud providers (ex: gcp)

> gcloud beta compute disk create --size 1GB --region us-east-1 pd-disk

pv-volume1.yaml
----------------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
spec:
  capacity:
    storage: 500Mi
  accessModes:
    - ReadWriteOnce
  gcePresistentDisk:
    pdName: pd-disk
	fsType: ext4
	

- Static Provisioning: everytime an application requires storage, we need to first manually provision storage on google cloud then manually create a PV using the same name as that of the disk we created.

- Dynamic Provisioning: Nice if the PV get automatically created as and when an application requires it. With Storage Class object, we can define a provisioner such as Google storage (kubernetes.io/gce-pd), AWS-EBS (kubernetes.io/aws-ebs) that can automatically provision new storage on google cloud and attach that to the POD when a claim is made.


- create a Storage class object, multiple available parameters based on the selected provisioner.

- Each StorageClass contains the fields provisioner, parameters, and reclaimPolicy, which are used when a PersistentVolume belonging to the class needs to be dynamically provisioned.

	- Reclaim Policy: PersistentVolumes that are dynamically created by a StorageClass will have the reclaim policy specified in the reclaimPolicy field of the class, which can be either Delete or Retain. If no reclaimPolicy is specified when a StorageClass object is created, it will default to Delete.
	
	- Provisioner: Each StorageClass has a provisioner that determines what volume plugin is used for provisioning PVs. This field must be specified.
	
	- Allow Volume Expansion: PersistentVolumes can be configured to be expandable. This feature when set to true.
	
	- Volume Binding Mode: The volumeBindingMode field controls when volume binding and dynamic provisioning should occur. When unset, "Immediate" mode is used by default.
		- The "Immediate" mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created.
		- "WaitForFirstConsumer" mode which will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.


sc-definition1.yaml
--------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard // or pd-ssd
  replication-type: none // or regional-pd
	

sc-definition2.yaml
--------------------	
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Retain
allowVolumeExpansion: true
mountOptions:
  - debug
volumeBindingMode: Immediate



- we no longer need a PV definition as the PV and any associated storage is going to be created automatically when the storage class is created. 
	
- for the PVC to use the Storage Class we defined, we use the storage class name. thats how the PVC knows which storage class to use.
	
pvc.yaml
----------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim-sc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: google-storage
  resources:
    requests:
      storage: 500Mi

- With PVC created, the Storage Class associated with it use the defined provisioner to provision a new disk with the exact required size on GCP, and creates a persistent volume automatically and then binds the PVC to the Volume defined in POD definition.

- It still creates a PV but we dont manually create anymore. its created by the Storage Class.


- The usage of Storage class is to categorize different types of storage and make them available for the app user to consume based on the need.
- ex: silver storage class with standard HDD, gold storage class with SSD and a platinum class with SSD and replication


sc-silver-definition.yaml
-------------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: silver
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
  replication-type: none


sc-gold-definition.yaml
-------------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gold
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
  replication-type: none
  
  
sc-platinum-definition.yaml
----------------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: platinum
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
  replication-type: regional-pd
  

- Next time when we create PVC, we can define the class of storage we want to satisfy the need.

$ kubectl get sc
NAME                        PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path (default)        rancher.io/local-path           Delete          WaitForFirstConsumer   false                  14m
local-storage               kubernetes.io/no-provisioner    Delete          WaitForFirstConsumer   false                  2m14s
portworx-io-priority-high   kubernetes.io/portworx-volume   Delete          Immediate              false                  2m14s

- here, VOLUMEBINDINGMODE can be Immediate (provisoning of storage to pvc will occur as soon as the pvc is created) / WaitForFirstConsumer (pvc created but in pending status and waiting for the first POD use the same pvc)

- The Storage Class makes use of VolumeBindingMode set to WaitForFirstConsumer. This delays the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.

local-storage.yaml
-------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{},"name":"local-storage"},"provisioner":"kubernetes.io/no-provisioner","volumeBindingMode":"WaitForFirstConsumer"}

  name: local-storage
provisioner: kubernetes.io/no-provisioner
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer


$ kubectl get pvc
NAME        STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
local-pvc   Pending

$ kubectl create -f pod.yaml 
pod/nginx created


$ kubectl get pvc
NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
local-pvc   Bound    local-pv   500Mi      RWO            local-storage   37m


- on creating a new pod with the same pvc, the pvc status shows 'Bound'

- example of:
Create a new Storage Class called delayed-volume-sc that makes use of the below specs:
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer


pv.yaml
-------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: delayed-volume-sc
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer






=======================================================================================

::StatefulSets::

=======================================================================================

Stateful applications are applications that have a memory of what happened in the past. Databases are an example of stateful applications. Kubernetes provides support for stateful applications through StatefulSets and related primitives.


- why we need stateful states and why cant we leave with deployments.


- lets understand a deployment pattern of mysql without k8s i.e. on bear metal boxes.

- we installed mysql server on one host and user are writing to it. 
- to withstand failures, we installed mysql on additional servers, these server are now empty, Now, how do we replicate data from original server to the newly installed servers.
- there are different toplogies available, the most easy way is a single master and multi slave topology, where writes is always to master and read can be served either by master or slave.
- So,
	- master server should be setup first and then the slaves.
	- once the slave is deployed, perform the initial data load from the master to slave-1.
	- enable continous replication from master to slave-1, so the db on slave-1 is in sync with master.
	- we then do the same for other slave servers. but load from master will impact the resource on master, soecially the network interface. since the data is available on slave-1, its better to copy from slave-1
	- so wait for slave-1 to be ready.
	- clone data from slave-1 to slave-2
	- enable continous replication from master to slave-2, so the db on slave-2 is in sync with master.
	- master address is to be configured on both the slaves.
	
- Now in kubernetes world, each of these instances of master and slave are POD part of a deployment hence can easily scale it up/down as required.

with deployment we have 2 issues:
- but with deployment we dont have a order i.e. all pod come up part of the deployment at the same time. 
- master pod should have a dedicated constant identifier or address that does not change which these slave nodes will use. can rely on IP address in k8s world, but deployment create pod with random names that can't help.

stateful sets:
- similar to deployments as it create pod based on template, scale up/down, perfom rolling updates or rollback. 
- PODs are created  seqetial order, after the 1st pod is deployed, it must be in a running/ready state before the next pod is deployed. helps in deploying master first and other slaves in order.
- stateful sets assigns unique identifier to each pod, a numver starting from 0 for the first pod and increment by 1, each pod gets a unique name derived from this index combined with stateful sets name. ex: mysql-0, mysql-1, mysql-3, no more random names hence ensure pod names is always same.
- can use the master pod name to all the slaves. i.e mysql-0, even if any pod fails and on restart, the name of the pod remains same. stateful sets maintain a sticky identity to each of its pods.




Stateful Sets Introduction
-------------------------------------------------------------------------
- we might not need stateful set always as it depends on application type, if the instance need to comeup in particualr order or it need a constant name to its pods.
- stateful set manifest is exactly same as deployment.

statefulset-definition.yaml
-----------------------------
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql

	serviceName: mysql-h
	
	podManagementPolicy: parallel  // this attribute to override the default nature and deploy all pods in parallel.
	
	
- serviceName needs to be specified. i.e. the headless service name.

$ kubectl create -f statefulset-definition.yaml

- it creates each replica of this pod one after another. i.e ordered, graceful deployment.
- each pod gets a stable unique DNS record on the network that any other application can use to access a pod.

- when we scaled stateful set, each pod comesup becomes ready and only then the next one comes up.
$ kubectl scale statefulset mysql --replicas=5


- it works in reverse order when we scale it down, i.e. the last pod is removed first followed by the 2nd last one.
$ kubectl scale statefulset mysql --replicas=4


- the same is true during pod termination. when we delete stateful state, the pods are deleted in reverse order.
$ kubectl delete statefulset mysql

- these are the default behavior of statefulsets but we can override to not follow the ordered launch but still have the other benefits sch as stable and unique network id. 

podManagementPolicy: parallel to deploy all pods in parallel.




Headless Services
---------------------------------------------------------------
- when we create a StatefulSet, it deploys one pod at a time, each pod gets an ordinal index and each pod has a stable unique name (ex: mysql-0, mysql-1 and mysql-2), so we can point slaves to reach the master as mysql-0

- Regular service: the way we point an application to another application within the cluster is through a service. ex: if we have a webserver then to make a db server acsessible, we create a service for the mysql db. this service will now acts like a load balancer, the trafic in to the service is balanced across all the pods (mysql-0, mysql-1, mysql-2) in the deployment.
	- service has a clusterIP and a DNS name (mysql.default.svc.cluster.local i.e. <service_name>.<ns_name>.<svc>.cluster.local)
	- webserver running in same env, use this dns name to reach mysql db.
	- BUT since this a master-slave topology. the reads could be serverd by both master or slaves BUT, writes has to be by master only. hence read request can still use the mysql clusterIP service but the write requests can't.
	
- ways to reach a single pod of a deployment:
	- pod IP (ex: 10.40.2.8) but IP address are dynamic can change if the pod gets recreated.
	- each pod can be reached to its DNS address but the POD's dns is create using its IP address like: 10-40-2-8.default.pod.cluster.local i.e. <pod_ip_separated_by_hyphen>.<namespace>.pod.cluster.local
	
	
Headless Service:
-----------------------------------------

	- Sometimes you don't need load-balancing and a single Service IP. In this case, you can create what are termed "headless" Services, by explicitly specifying "None" for the cluster IP (.spec.clusterIP).
	
	- For headless Services, a cluster IP is not allocated, kube-proxy does not handle these Services, and there is no load balancing or proxying done by the platform for them. How DNS is automatically configured depends on whether the Service has selectors defined
	
	- With selectors: For headless Services that define selectors, the endpoints controller creates Endpoints records in the API, and modifies the DNS configuration to return A records (IP addresses) that point directly to the Pods backing the Service
	
	- does not load balance the request but creates individual DNS entries for each pod the service is pointing to, using the pod name and the sub domain.
	
	- its created like a normal service but it does not have an IP address on its own like a clusterIP for normal service.
	- on creating a headless service named: mysql-h, each pod gets an DNS name in the format of: <podname>.<headless-servicename>.<namespace>.svc.<cluster-domain-name>
	
	ex:
	mysql-0.mysql-h.default.svc.cluster.local
	mysql-1.mysql-h.default.svc.cluster.local
	mysql-2.mysql-h.default.svc.cluster.local
	
	- the web app can now use the dns entry for the master pod (mysql-0.mysql-h.default.svc.cluster.local), this dns entry will always point to the master pod of mysql deployment.
	
	- create headless service by explicitly specifying "None" for the cluster IP (.spec.clusterIP).
	
	- "Headless" (without a cluster IP) Services are also assigned a DNS A or AAAA record, depending on the IP family of the service, for a name of the form my-svc.my-namespace.svc.cluster-domain.example. Unlike normal Services, this resolves to the set of IPs of the pods selected by the Service. Clients are expected to consume the set or else use standard round-robin selection from the set.
	
headless-service.yaml
---------------------
apiVersion: v1
kind: Service
metadata:
  name: mysql-h
spec:
  type: None
  
  ports:
      # By default the `targetPort` is set to the same value as the `port` field.
    - port: 3306
	
  selector:
    app: mysql
	

- Now the DNS entries are created for the pod only if the 2 conditions are met. under the spec section, 2 optional field, subdomain and hostname
	- on adding the subdomain value to the headless service name, it creates a DNS A-record (mysql-h.default.svc.cluster.local) for the name of the service to point to the pod.
	- how its still does not create A record for invidual pods. giving a name to hostname and subdomain field in pod manifest, it creates DNS record with the Pod name (mysql-pod.mysql-h.default.svc.cluster.local)
	
pod-definition.yaml
----------------------
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: mysql
spec:
  containers:
  - image: mysql
    name: mysql
	
  subdomain: mysql-h  // MUST be the same name of headless service
  hostname: mysql-pod // any name, gets prefixed on pod DNS name created by the headless service.
  

- without hostname/subdomain, each pod can be reached to its DNS address but the POD's dns is created using its IP address like: 10-40-2-8.default.pod.cluster.local

- Now when we deploy pod using deployment, by default it does not add a hostname/subdomain to the pod if hostname or subdomain is not defined, so the headless service too does not create A record for the pods.

- in we specify like below, then it assigns the same subdomain and hostna e to all the pods as deployment simply duplicates all the pod properties for the same pod.

deployment-definition.yaml
--------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deployment
  labels:
    app: mysql
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql
        ports:
        - containerPort: 3306
	  
	  hostname: mysql-pod
	  subdomain: mysql-h
	 

- the above deploymet will create the below same DNS A-records for 3 of its pods. so this does not help in addressing each pod individually. that is where StatefulSet differs from a Deployment.

mysql-pod.mysql-h.default.svc.cluster.local
mysql-pod.mysql-h.default.svc.cluster.local
mysql-pod.mysql-h.default.svc.cluster.local

  
- in StatefulSet, we dont need to mention subdomain/hostname, instead add the headless service name in StatefulSet definition (under spec section, ex: serviceName: mysql-h) and it automatically assigns the right hostname for each pod based on the pod name and also assigns the subdomain name based on the headless service name.
	
  
statefulset-definition.yaml
----------------------------
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-statefulset
  labels:
    app: mysql
spec:

  serviceName: mysql-h

  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql
        ports:
        - containerPort: 3306


- all pods now get a separate DNS record.

mysql-0.mysql-h.default.svc.cluster.local
mysql-1.mysql-h.default.svc.cluster.local
mysql-2.mysql-h.default.svc.cluster.local	




Storage in StatefulSets
----------------------------------------
- Static Provisoning: with Persistem Volumes, we create volume objects in kubernetes which are then claimed by PVC and finally used in pod definition file to use the storage in pod. this is single PV mapped to an single PVC to a single POD

- Dynamic provisioning: With StorageClass definition, we take out the manula creation of PV, and use the Storage provisioners to automatically provision volume on cloud providers, now the PV is created automatically, will still create a PVC manually and associate to the POD.


- How does these above behaviors work with StatefulSets, when we specify the same PVC under the pod spec on StatefulSet manifest, all pods created by that StatefulSet, tries to use the same volume, thats okay if the application design is fine with it, as like we want multiple pod to share and access the same storage and that also depends on the ind of volume created and the provisioner used, Note: NOT all storage types support these type of operation of RW access by multiple instance at the same time.

- If we want separate volume for each pod of StatefulSet as in case of mysql replication usecase, the PODs dont want to share the data instead each POD needs its own local storage, each instance has its own DB and replication of data between the Dbs is done at mysql level, so then each POD needs its own PVC bound to a individual PV. and ofcourse these PVs can be created from single StorageClass or different StorageClass.

- How do we automatically create separate PVC in Statedulset.
	- VolumeClaimTemplate: is nothing but a PVC manifest. i.e. instead of creating the PVC manually then specifying it inside StatefulSet manifest, we move the PVC manifest into a section called "volumeClaimTemplates", its an array, we can specify multiple templates.
	

sc-definition.yaml
-------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd


pvc-definition.yaml (dont create this - but add inside statefulset as a template)
----------------------------------------------------------------------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-volume
spec:
  accessMode: 
  - ReadWriteOnce
  storageClassName: google-storage
  resource:
    requests:
	  storage: 500Mi

statefulset-definition.yaml
-----------------------------
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql
	serviceName: mysql-h	
	
	volumeClaimTemplates:
	-  metadata:
		  name: data-volume
	   spec:
	     accessMode: 
		 - ReadWriteOnce
		 storageClassName: google-storage
		   resource:
			  requests:
			    storage: 500Mi




- it creates the first POD, during creation, a PVC is created, PVC is associated to StorageClass, so the StorageClass provision a volume on GCP, creates a PV automatically which gets bound to PVC
	- then the 2nd POD gets created, it create a PVC, the StorageClass provisions a new volume, associates that to a PV and bind the PV to PVC
	
- Incase any of these POD gets recreated or reschedules on a separate node, StatedulSet DO NOT automatically delete the PVC or the associated volume, instead ensures, the POD is re-attached to the same PVC that it was attached to before. thus StatefulSets ensures, stable storage for PODs.



CloudAcademy Exercise:
---------------------------------------------------------------------
This Lab will deploy a replicated MySQL database as a StatefulSet. MySQL is one of the most popular databases in the world. This Lab won't focus on the details specific to configuring MySQL. The focus is on the features of Kubernetes that allow a stateful application to be deployed. The Kubernetes community maintains a wide variety of stateful applications that are ready to deploy through Helm. Helm acts as a package manager for Kubernetes and can deploy entire applications to a cluster using templates called charts. A list of charts is available here. In addition to MySQL, there are charts for many other popular databases including MongoDB and PostgreSQL, as well as popular applications like WordPress and Joomla. The concepts illustrated in this Lab will help you understand the common elements used to deploy all of these stateful applications in Kubernetes.

Background:

ConfigMaps: A type of Kubernetes resource that is used to decouple configuration artifacts from image content to keep containerized applications portable. The configuration data is stored as key-value pairs.

 

Headless Service: A headless service is a Kubernetes service resource that won't load balance behind a single service IP. Instead, a headless service returns a list of DNS records that point directly to the pods that back the service. A headless service is defined by declaring the clusterIP property in a service spec and setting the value to None. StatefulSets currently require a headless service to identify pods in the cluster network.

 

Stateful Sets: Similar to Deployments in Kubernetes, StatefulSets manage the deployment and scaling of pods given a container spec.StatefulSets differ from Deployments in that the Pods in a stateful set are not interchangeable. Each pod in a StatefulSet has a persistent identifier that it maintains across any rescheduling. The pods in a StatefulSet are also ordered. This provides a guarantee that one pod can be created before following pods. In this Lab, this is useful for ensuring the MySQL primary is provisioned first.

 

PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs): PVs are Kubernetes resources that represent storage in the cluster. Unlike regular Volumes which exist only until while containing pod exists, PVs do not have a lifetime connected to a pod. Thus, they can be used by multiple pods over time, or even at the same time. Different types of storage can be used by PVs including NFS, iSCSI, and cloud-provided storage volumes, such as AWS EBS volumes. Pods claim PV resources through PVCs.

 

MySQL replication: This Lab uses a single primary, asynchronous replication scheme for MySQL. All database writes are handled by a single primary. The database replicas asynchronously synchronize with the primary. This means the primary will not wait for the data to be copied onto the replicas. This can improve the performance of the primary at the expense of having replicas that are not always exact copies of the primary. Many applications can tolerate slight differences in the data and are able to improve the performance of database read workloads by allowing clients to read from the replicas.



Instructions
1. In your SSH shell, enter the following to declare a ConfigMap to allow primary MySQL pods to be configured differently than replica pods:

- This ConfigMap will be referenced later in the StatefulSet declaration.
- The master.cnf key maps to a value that declares a MySQL configuration which includes replication logs. 
- The slave.cnf key maps to a MySQL configuration that enforces read-only behavior.

cat <<EOF > mysql-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql
  labels:
    app: mysql
data:
  master.cnf: |
   # Apply this config only on the primary.
   [mysqld]
   log-bin
  slave.cnf: |
    # Apply this config only on replicas.
    [mysqld]
    super-read-only
EOF


2. Create the ConfigMap resource:

$ kubectl create -f mysql-configmap.yaml 

$ kubectl get cm
NAME               DATA   AGE
kube-root-ca.crt   1      303d
mysql              2      27s


3. Enter the following command to declare the services for the MySQL application:

cat <<EOF > mysql-services.yaml
# Headless service for stable DNS entries of StatefulSet members.
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql
---
# Client service for connecting to any MySQL instance for reads.
# For writes, you must instead connect to the primary: mysql-0.mysql.
apiVersion: v1
kind: Service
metadata:
  name: mysql-read
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  selector:
    app: mysql
EOF


Two services are defined:

	- A headless service (clusterIP: None) i.e. name: mysql for pod DNS resolution. Because the service is named mysql, pods are accessible via pod-name.mysql.
	
	- A service name mysql-read to connect to for database reads. This service uses the default ServiceType of ClusterIP which assigns an internal IP address that load balances request to all the pods labeled with app: mysql.


- Database writes need to be sent to the primary. The primary is the first pod provisioned in the StatefulSet and assigned a name mysql-0. 
- The pod is thus accessed by the DNS entry in the headless service for mysql-0.mysql.


4. Create the MySQL services:

$ kubectl create -f mysql-services.yaml


$ kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP    303d
mysql        ClusterIP   None           <none>        3306/TCP   6s
mysql-read   ClusterIP   10.100.5.221   <none>        3306/TCP   6s


5. Enter the following command to declare a default storage class that will be used to dynamically provision general-purpose (gp2) EBS volumes for the Kubernetes PVs:

cat <<EOF > mysql-storageclass.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: general
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
EOF


- The built-in aws-ebs storage provision is specified along with the type gp2.


6. Create the storage class:

kubectl create -f mysql-storageclass.yaml


$ kubectl get storageclass
NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
general         kubernetes.io/aws-ebs   Delete          Immediate           false                  8s
gp2 (default)   kubernetes.io/aws-ebs   Delete          Immediate           false                  303d


7. Enter the following command to declare the MySQL StatefulSet:

cat <<'EOF' > mysql-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Generate mysql server-id from pod ordinal index.
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          # Add an offset to avoid reserved server-id=0 value.
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          # Copy appropriate conf.d files from config-map to emptyDir.
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Skip the clone if data already exists.
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          # Skip the clone on primary (ordinal index 0).
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0
          # Clone data from previous peer.
          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          # Prepare the backup.
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
        livenessProbe:
          exec:
            command: ["mysqladmin", "ping"]
          initialDelaySeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          exec:
            # Check we can execute queries over TCP (skip-networking is off).
            command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
          initialDelaySeconds: 5
          timeoutSeconds: 1
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - name: xtrabackup
          containerPort: 3307
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql

          # Determine binlog position of cloned data, if any.
          if [[ -f xtrabackup_slave_info ]]; then
            # XtraBackup already generated a partial "CHANGE MASTER TO" query
            # because we're cloning from an existing replica.
            mv xtrabackup_slave_info change_master_to.sql.in
            # Ignore xtrabackup_binlog_info in this case (it's useless).
            rm -f xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            # We're cloning directly from primary. Parse binlog position.
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm xtrabackup_binlog_info
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi

          # Check if we need to complete a clone by starting replication.
          if [[ -f change_master_to.sql.in ]]; then
            echo "Waiting for mysqld to be ready (accepting connections)"
            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done

            echo "Initializing replication from clone position"
            # In case of container restart, attempt this at-most-once.
            mv change_master_to.sql.in change_master_to.sql.orig
            mysql -h 127.0.0.1 <<EOF
          $(<change_master_to.sql.orig),
            MASTER_HOST='mysql-0.mysql',
            MASTER_USER='root',
            MASTER_PASSWORD='',
            MASTER_CONNECT_RETRY=10;
          START SLAVE;
          EOF
          fi

          # Start a server to send backups when requested by peers.
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 100m
            memory: 50Mi
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 2Gi
      storageClassName: general
EOF


There is a lot going on in the StatefulSet. Don't focus too much on the bash scripts that are performing MySQL-specific tasks. Some highlights to focus on, following the order they appear in the file are:

- init-containers: Run to completion before any containers in the Pod spec

	- init-mysql: Assigns a unique MySQL server ID starting from 100 for the first pod and incrementing by one, as wel		l as copying the appropriate configuration file from the config-map. Note the config-map is mounted via the VolumeMounts section. The ID and appropriate configuration file are persisted on the conf volume.

	- clone-mysql: For pods after the primary, clone the database files from the preceding pod. The xtrabackup tool performs the file cloning and persists the data on the data volume.

- spec.containers: Two containers in the pod

	- mysql: Runs the MySQL daemon and mounts the configuration in the conf volume and the data in the data volume

	- xtrabackup: A sidecar container that provides additional functionality to the mysql container. It starts a server to allow data cloning and begins replication on replicas using the cloned data files.

- spec.volumes: conf and config-map volumes are stored on the node's local disk. They are easily re-generated if a failure occurs and don't require PVs.

	- volumeClaimTemplates: A template for each pod to create a PVC with. ReadWriteOnce accessMode allows the PV to be mounted by only one node at a time in read/write mode. The storageClassName references the AWS EBS gp2 storage class named general that you created earlier. 



8. Create the StatefulSet and start watching the associated pods:


kubectl create -f mysql-statefulset.yaml

$ kubectl get pods -l app=mysql --watch

NAME      READY   STATUS    RESTARTS   AGE
mysql-0   0/2     Pending   0          6s
mysql-0   0/2     Pending   0          10s
mysql-0   0/2     Init:0/2   0          10s
mysql-0   0/2     Init:0/2   0          27s
mysql-0   0/2     Init:1/2   0          39s
mysql-0   0/2     PodInitializing   0          49s
mysql-0   1/2     Running           0          50s
mysql-0   2/2     Running           0          65s
mysql-1   0/2     Pending           0          0s
mysql-1   0/2     Pending           0          0s
mysql-1   0/2     Pending           0          11s
mysql-1   0/2     Init:0/2          0          11s
mysql-1   0/2     Init:0/2          0          16s
mysql-1   0/2     Init:1/2          0          28s
mysql-1   0/2     Init:1/2          0          38s
mysql-1   0/2     PodInitializing   0          48s
mysql-1   1/2     Running           0          49s
mysql-1   2/2     Running           0          55s
mysql-2   0/2     Pending           0          0s
mysql-2   0/2     Pending           0          1s
mysql-2   0/2     Pending           0          11s
mysql-2   0/2     Init:0/2          0          11s
mysql-2   0/2     Init:0/2          0          20s
mysql-2   0/2     Init:1/2          0          21s
mysql-2   0/2     Init:1/2          0          22s
mysql-2   0/2     PodInitializing   0          27s
mysql-2   1/2     Running           0          28s
mysql-2   2/2     Running           0          32s


so, every mysql pod's transition to running 2/2:

mysql-1   0/2     Pending           0          0s
mysql-1   0/2     Init:0/2          0          11s
mysql-1   0/2     Init:1/2          0          28s
mysql-1   0/2     PodInitializing   0          48s
mysql-1   1/2     Running           0          49s
mysql-1   2/2     Running           0          55s

$ kubectl get pods -l app=mysql
NAME      READY   STATUS    RESTARTS   AGE
mysql-0   2/2     Running   0          3m1s
mysql-1   2/2     Running   0          116s
mysql-2   2/2     Running   0          61s


9. Navigate to AWS then and click Elastic Block Store > Volumes in the AWS EC2 Console.

- The 2GiB PVs are listed here as each pod is created. Notice the Name and Tags which relay information about the PV and associated PVC.

will see 3 EBS vols of 2Gb each got created and used:

kubernetes-dynamic-pvc-e541e979-3839-4393-b383-6c2486d19ff3
kubernetes-dynamic-pvc-b544239a-8837-42c3-9312-a64ebae1ab61
kubernetes-dynamic-pvc-a35b3b6c-e9bd-4f0b-9e4d-1aae1fbaccef



10. Return to your SSH shell and press ctrl+C to stop the watch when you see both containers (2/2) in the mysql-2 pod running: 


11. Describe the PVs and PVCs:

kubectl describe pv
kubectl describe pvc


$ kubectl get pv

NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE
pvc-a35b3b6c-e9bd-4f0b-9e4d-1aae1fbaccef   2Gi        RWO            Delete           Bound    default/data-mysql-2   general                 5m30s
pvc-b544239a-8837-42c3-9312-a64ebae1ab61   2Gi        RWO            Delete           Bound    default/data-mysql-1   general                 6m25s
pvc-e541e979-3839-4393-b383-6c2486d19ff3   2Gi        RWO            Delete           Bound    default/data-mysql-0   general                 7m31s



$ kubectl get pvc

NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-mysql-0   Bound    pvc-e541e979-3839-4393-b383-6c2486d19ff3   2Gi        RWO            general        7m51s
data-mysql-1   Bound    pvc-b544239a-8837-42c3-9312-a64ebae1ab61   2Gi        RWO            general        6m46s
data-mysql-2   Bound    pvc-a35b3b6c-e9bd-4f0b-9e4d-1aae1fbaccef   2Gi        RWO            general        5m51s


- The PV descriptions include the AWS VolumeIDs, file system types (FSType), and associated PVC (Claim). The PVC description includes whether the PVC is currently Bound to a pod.


12. Get the StatefulSet to confirm the current number of replicas matches the desired:

$ kubectl get statefulset

NAME    READY   AGE
mysql   3/3     4m5s

$ kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
mysql-0   2/2     Running   0          11m
mysql-1   2/2     Running   0          10m
mysql-2   2/2     Running   0          9m13s

- With MySQL up and running, you will execute some commands against the database to demonstrate everything is working correctly and Kubernetes can gracefully handle failures and other requests.


13. Run a temporary container to use mysql to connect to the primary at mysql-0.mysql and runs a few SQL commands:
- The SQL commands create a mydb database with a notes table in it with one record and finally deletes the pod.

kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\
  /usr/bin/mysql -h mysql-0.mysql -e "CREATE DATABASE mydb; CREATE TABLE mydb.notes (note VARCHAR(250)); INSERT INTO mydb.notes VALUES ('k8s Cloud Academy Lab');"

pod "mysql-client" deleted


here: --rm ensures the Pod is deleted when the shell exits, used to run a one-off container from the command line.


14. Run a query using the mysql-read endpoint to select all of the notes in the table:

kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\
  /usr/bin/mysql -h mysql-read -e "SELECT * FROM mydb.notes"
  
  
+-----------------------+
| note                  |
+-----------------------+
| k8s Cloud Academy Lab |
+-----------------------+
pod "mysql-client" deleted


15. Run an SQL command that outputs the MySQL server's ID to confirm that the requests are distributed to different pods:

- Eventually, a request will be sent to each MySQL server. Recall that the primary has ID of 100.

kubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\
  bash -ic "while sleep 1; do /usr/bin/mysql -h mysql-read -e 'SELECT @@server_id'; done"

+-------------+
| @@server_id |
+-------------+
|         101 |
+-------------+
+-------------+
| @@server_id |
+-------------+
|         102 |
+-------------+
+-------------+
| @@server_id |
+-------------+
|         100 |
+-------------+
+-------------+
| @@server_id |
+-------------+
|         100 |
+-------------+
+-------------+
| @@server_id |
+-------------+
|         101 |
+-------------+
+-------------+
| @@server_id |
+-------------+
|         101 |
+-------------+

^C
pod "mysql-client-loop" deleted
pod default/mysql-client-loop terminated (Error)



16. List the pods including the node name column:

kubectl get pod -o wide

NAME      READY   STATUS    RESTARTS   AGE   IP                NODE                                      NOMINATED NODE   READINESS GATES
mysql-0   2/2     Running   0          21m   192.168.23.129    ip-10-0-0-11.us-west-2.compute.internal   <none>           <none>
mysql-1   2/2     Running   0          20m   192.168.203.129   ip-10-0-0-10.us-west-2.compute.internal   <none>           <none>
mysql-2   2/2     Running   0          19m   192.168.23.130    ip-10-0-0-11.us-west-2.compute.internal   <none>           <none>


17. Enter the following command to simulate taking the node running the mysql-2 pod out of service for maintenance:

node=$(kubectl get pods --field-selector metadata.name=mysql-2 -o=jsonpath='{.items[0].spec.nodeName}')
kubectl drain $node --force --delete-local-data --ignore-daemonsets


- where you replace the node environment variable is set to the name of the node running mysql-2 using a field-selector and jsonpath output to select the nodeName of the Pod. It will resemble ip-10-0-#-#.us-west-2.compute-internal. 

- The drain command prevents new pods from being scheduled on the node and then evicts existing pods scheduled to it.


18. Watch the mysql-2 pod get rescheduled to a different node:

kubectl get pod -o wide --watch


19. Uncordon the node you drained so that pods can be scheduled on it again:

kubectl uncordon $node



20. Delete the mysql-2 pod to simulate a node failure and watch it get automatically rescheduled:

kubectl delete pod mysql-2
kubectl get pod mysql-2 -o wide --watch



21. Scale the number of replicas up to 5:

kubectl scale --replicas=5 statefulset mysql


22. Watch as new pods get scheduled in the cluster:

kubectl get pods -l app=mysql --watch


23. Verify that you see the new MySQL server IDs:

kubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\
  bash -ic "while sleep 1; do /usr/bin/mysql -h mysql-read -e 'SELECT @@server_id'; done"
  
  
24. Confirm that the data is replicated in the new mysql-4 pod:

kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\
  /usr/bin/mysql -h mysql-4.mysql -e "SELECT * FROM mydb.notes"
  


25. Display the internal virtual IP of the mysql-read endpoint:

kubectl get services mysql-read

Recall the mysql-read service used the default type of ClusterIP so it is only accessible inside the cluster.



26. Append a load balancer type to the mysql-read service declaration:

echo "  type: LoadBalancer" >> mysql-services.yaml



27. Apply the changes to the mysql-read service:

kubectl apply -f mysql-services.yaml


- The apply command can update existing resources. It will create resources if they don't already exist. Because you created the services using the create command instead of apply you will see a warning. The warnings can be ignored for this demonstration.

 
 
 
28. Re-display the mysql-read service status:

kubectl get services mysql-read


- Kubernetes is in the process of provisioning an elastic load balancer (ELB) to access the mysql-read service from outside the cluster.


29. After a minute, describe the mysql-read service to find the DNS name of the external load balancer endpoint:


kubectl describe services mysql-read | grep "LoadBalancer Ingress"


30. Use the external load balancer to send some read requests to the cluster:


load_balancer=$(kubectl get services mysql-read -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')
kubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\
  bash -ic "while sleep 1; do /usr/bin/mysql -h $load_balancer -e 'SELECT @@server_id'; done"
  


- where the load_balancer variable stores the ELB DNS name. It will resemble a830e12d78dcd11e79aba028416f4825-905974806.us-west-2.elb.amazonaws.com. You are now accessing the cluster from the ELB which provides access outside of the cluster.


- Note: It can take a minute for the nodes to be added to the load balancer. You will see unknown MySQL host messages until the nodes are added. Let the loop continue to run until you start seeing server IDs being displayed.




  












===============================================================================================

DNS for Services and Pods

===============================================================================================
- Kubernetes creates DNS records for services and pods. You can contact services with consistent DNS names instead of IP addresses


Services
----------
A/AAAA records
"Normal" (not headless) Services are assigned a DNS A or AAAA record, depending on the IP family of the service, for a name of the form my-svc.my-namespace.svc.cluster-domain.example. This resolves to the cluster IP of the Service.

"Headless" (without a cluster IP) Services are also assigned a DNS A or AAAA record, depending on the IP family of the service, for a name of the form my-svc.my-namespace.svc.cluster-domain.example. Unlike normal Services, this resolves to the set of IPs of the pods selected by the Service. Clients are expected to consume the set or else use standard round-robin selection from the set



Pods
----------
A/AAAA records
In general a pod has the following DNS resolution:

pod-ip-address.my-namespace.pod.cluster-domain.example.

For example, if a pod in the default namespace has the IP address 172.17.0.3, and the domain name for your cluster is cluster.local, then the Pod has a DNS name:

172-17-0-3.default.pod.cluster.local.

Any pods exposed by a Service have the following DNS resolution available:

pod-ip-address.service-name.my-namespace.svc.cluster-domain.example



Pod's hostname and subdomain fields
---------------------------------------
Currently when a pod is created, its hostname is the Pod's metadata.name value.

The Pod spec has an optional hostname field, which can be used to specify the Pod's hostname. When specified, it takes precedence over the Pod's name to be the hostname of the pod. For example, given a Pod with hostname set to "my-host", the Pod will have its hostname set to "my-host".

The Pod spec also has an optional subdomain field which can be used to specify its subdomain. For example, a Pod with hostname set to "foo", and subdomain set to "bar", in namespace "my-namespace", will have the fully qualified domain name (FQDN) "foo.bar.my-namespace.svc.cluster-domain.example".

Example:

apiVersion: v1
kind: Service
metadata:
  name: default-subdomain
spec:
  selector:
    name: busybox
  clusterIP: None
  ports:
  - name: foo # Actually, no port is needed.
    port: 1234
    targetPort: 1234
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
  labels:
    name: busybox
spec:
  hostname: busybox-1
  subdomain: default-subdomain
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - "3600"
    name: busybox
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox2
  labels:
    name: busybox
spec:
  hostname: busybox-2
  subdomain: default-subdomain
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - "3600"
    name: busybox
	
If there exists a headless service in the same namespace as the pod and with the same name as the subdomain, the cluster's DNS Server also returns an A or AAAA record for the Pod's fully qualified hostname. For example, given a Pod with the hostname set to "busybox-1" and the subdomain set to "default-subdomain", and a headless Service named "default-subdomain" in the same namespace, the pod will see its own FQDN as "busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example". DNS serves an A or AAAA record at that name, pointing to the Pod's IP. Both pods "busybox1" and "busybox2" can have their distinct A or AAAA records.







===============================================================================================
Updates for Sep 2021 Changes
===============================================================================================


Docker Image Builds
===============================================================================================
docker images
-------------
- list all the images and its details pulled on local docker host.

$ docker images
REPOSITORY                      TAG                 IMAGE ID            CREATED             SIZE
redis                           latest              ccee4cdf984f        8 months ago        105MB
ubuntu                          latest              7e0aa2d69a15        9 months ago        72.7MB
mysql                           latest              0627ec6901db        9 months ago        556MB
nginx                           alpine              a64a6e03b055        9 months ago        22.6MB
alpine                          latest              6dbb9cc54074        9 months ago        5.61MB
nginx                           latest              62d49f9bab67        9 months ago        133MB
postgres                        latest              26c8bcd8b719        9 months ago        314MB
kodekloud/simple-webapp-mysql   latest              129dd9f67367        3 years ago         96.6MB
kodekloud/simple-webapp         latest              c6e3cd9aae36        3 years ago         84.8MB



sample Docker file:
---------------------------
FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]


symtax to build the image locally:

docker build -t <image_name>:<tag_name>   // tag_name is optional. if not given, the value will be latest

$ docker build -t webapp-color .

Sending build context to Docker daemon  121.3kB
Step 1/6 : FROM python:3.6
3.6: Pulling from library/python
0e29546d541c: Pull complete 
9b829c73b52b: Pull complete 
cb5b7ae36172: Pull complete 
6494e4811622: Pull complete 
6f9f74896dfa: Pull complete 
5e3b1213efc5: Pull complete 
9fddfdc56334: Pull complete 
404f02044bac: Pull complete 
c4f42be2be53: Pull complete 
Digest: sha256:f8652afaf88c25f0d22354d547d892591067aa4026a7fa9a6819df9f300af6fc
Status: Downloaded newer image for python:3.6
 ---> 54260638d07c
Step 2/6 : RUN pip install flask
 ---> Running in 37ce7302e20d
Collecting flask
  Downloading Flask-2.0.2-py3-none-any.whl (95 kB)
Collecting Jinja2>=3.0
  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)
Collecting itsdangerous>=2.0
  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)
Collecting Werkzeug>=2.0
  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)
Collecting click>=7.1.2
  Downloading click-8.0.3-py3-none-any.whl (97 kB)
Collecting importlib-metadata
  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)
Collecting MarkupSafe>=2.0
  Downloading MarkupSafe-2.0.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)
Collecting dataclasses
  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)
Collecting typing-extensions>=3.6.4
  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)
Collecting zipp>=0.5
  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)
Installing collected packages: zipp, typing-extensions, MarkupSafe, importlib-metadata, dataclasses, Werkzeug, Jinja2, itsdangerous, click, flask
Successfully installed Jinja2-3.0.3 MarkupSafe-2.0.1 Werkzeug-2.0.2 click-8.0.3 dataclasses-0.8 flask-2.0.2 importlib-metadata-4.8.3 itsdangerous-2.0.1 typing-extensions-4.0.1 zipp-3.6.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.
You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.
Removing intermediate container 37ce7302e20d
 ---> 5b842274fd45
Step 3/6 : COPY . /opt/
 ---> 49a72fadf9be
Step 4/6 : EXPOSE 8080
 ---> Running in 4f75c7639e27
Removing intermediate container 4f75c7639e27
 ---> 1c427e70bea9
Step 5/6 : WORKDIR /opt
 ---> Running in 7879a13507d9
Removing intermediate container 7879a13507d9
 ---> 9df238646257
Step 6/6 : ENTRYPOINT ["python", "app.py"]
 ---> Running in 368ac8315aec
Removing intermediate container 368ac8315aec
 ---> daa92f4c3b7c
Successfully built daa92f4c3b7c
Successfully tagged webapp-color:latest



- Run an instance of the image webapp-color and publish port 8080 on the container to 8282 on the host.
$ docker run -p 8282:8080 webapp-color

 This is a sample web application that displays a colored background. 
 A color can be specified in two ways. 

 1. As a command line argument with --color as the argument. Accepts one of red,green,blue,blue2,pink,darkblue 
 2. As an Environment variable APP_COLOR. Accepts one of red,green,blue,blue2,pink,darkblue 
 3. If none of the above then a random color is picked from the above list. 
 Note: Command line argument precedes over environment variable.


No command line argument or environment variable. Picking a Random Color =red
 * Serving Flask app 'app' (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
 * Running on http://172.12.0.2:8080/ (Press CTRL+C to quit)



- What is the base Operating System used by the python:3.6 image?

$ docker run python:3.6 cat /etc/*release*

PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"


- shorten the size of image:

FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]


$ docker build -t webapp-color:lite .

Sending build context to Docker daemon  122.4kB
Step 1/6 : FROM python:3.6-alpine
3.6-alpine: Pulling from library/python
59bf1c3509f3: Pull complete 
8786870f2876: Pull complete 
acb0e804800e: Pull complete 
52bedcb3e853: Pull complete 
b064415ed3d7: Pull complete 
Digest: sha256:579978dec4602646fe1262f02b96371779bfb0294e92c91392707fa999c0c989
Status: Downloaded newer image for python:3.6-alpine
 ---> 3a9e80fa4606
Step 2/6 : RUN pip install flask
 ---> Running in bbd914fe5835
Collecting flask
  Downloading Flask-2.0.2-py3-none-any.whl (95 kB)
Collecting click>=7.1.2
  Downloading click-8.0.3-py3-none-any.whl (97 kB)
Collecting Jinja2>=3.0
  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)
Collecting Werkzeug>=2.0
  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)
Collecting itsdangerous>=2.0
  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)
Collecting importlib-metadata
  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)
Collecting MarkupSafe>=2.0
  Downloading MarkupSafe-2.0.1-cp36-cp36m-musllinux_1_1_x86_64.whl (29 kB)
Collecting dataclasses
  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)
Collecting typing-extensions>=3.6.4
  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)
Collecting zipp>=0.5
  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)
Installing collected packages: zipp, typing-extensions, MarkupSafe, importlib-metadata, dataclasses, Werkzeug, Jinja2, itsdangerous, click, flask
Successfully installed Jinja2-3.0.3 MarkupSafe-2.0.1 Werkzeug-2.0.2 click-8.0.3 dataclasses-0.8 flask-2.0.2 importlib-metadata-4.8.3 itsdangerous-2.0.1 typing-extensions-4.0.1 zipp-3.6.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.
You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.
Removing intermediate container bbd914fe5835
 ---> 49b436b124ee
Step 3/6 : COPY . /opt/
 ---> 9c9c5105ae3f
Step 4/6 : EXPOSE 8080
 ---> Running in 15e065ec3c8b
Removing intermediate container 15e065ec3c8b
 ---> de4c06e91938
Step 5/6 : WORKDIR /opt
 ---> Running in c51f6bbdede9
Removing intermediate container c51f6bbdede9
 ---> ec70be770823
Step 6/6 : ENTRYPOINT ["python", "app.py"]
 ---> Running in 4c99bfc638a3
Removing intermediate container 4c99bfc638a3
 ---> 2ce50c0feb06
Successfully built 2ce50c0feb06
Successfully tagged webapp-color:lite



$ docker images

REPOSITORY                      TAG                 IMAGE ID            CREATED             SIZE
webapp-color                    lite                2ce50c0feb06        2 minutes ago       51.8MB
webapp-color                    latest              daa92f4c3b7c        20 minutes ago      913MB
python                          3.6                 54260638d07c        4 weeks ago         902MB
python                          3.6-alpine          3a9e80fa4606        7 weeks ago         40.7MB
redis                           latest              ccee4cdf984f        8 months ago        105MB
ubuntu                          latest              7e0aa2d69a15        9 months ago        72.7MB
mysql                           latest              0627ec6901db        9 months ago        556MB
nginx                           alpine              a64a6e03b055        9 months ago        22.6MB
alpine                          latest              6dbb9cc54074        9 months ago        5.61MB
nginx                           latest              62d49f9bab67        9 months ago        133MB
postgres                        latest              26c8bcd8b719        9 months ago        314MB
nginx                           1.14-alpine         8a2fb25a19f5        2 years ago         16MB
kodekloud/simple-webapp-mysql   latest              129dd9f67367        3 years ago         96.6MB
kodekloud/simple-webapp         latest              c6e3cd9aae36        3 years ago         84.8MB



- Run an instance of the new image webapp-color:lite and publish port 8080 on the container to 8383 on the host.

$ docker run -d -p 8383:8080 webapp-color:lite
abb5590f3e6246bddf0a920a68ac9074a19a0940249356dcc1ca74edf8ce8ca3


$ docker ps

CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                    NAMES
abb5590f3e62        webapp-color:lite   "python app.py"     27 seconds ago      Up 25 seconds       0.0.0.0:8383->8080/tcp   interesting_turing





Authentication, Authorization and Admission Control
=======================================================================================================

- Kubernetes becoming the goto platform for hosting production grade application, securing the kubernetes cluster is very important.

- There various components in the cluster, where the security should be implemented.

1/ Securing the hosts/nodes of the cluster.
	- access to these hosts must be secured.
	- root access should be disabled.
	- password based authN disabled and only SSH based authN is allowed.
	- all other measure to secure the physical or virtual infra that host kubernetes.
	

2/ Secure Kubernetes (kube apiserver)
	- what are the risks and what measures need to take to secure the cluster.
	- kube-apiserver is the center of all operation in kubernetes cluster. we interratct with it bu kubectl or accessing the API directly and we can perform almost any operations on the cluster.
	- controlling access to the kube-apiserver will be the 1st line of defence.
	

Authentication:
- who can access the cluster. different ways to authN to apiserver.
	- username and passwords stored in static files.
	- username and tokens stored in static files.
	- certificates
	- integration with external auth providers like LDAP
	- service accounts for machines.

Authorization:
- once a user get access to the cluster, AuthZ defines what can the user do?
	- RBAC - Role Based Access Control
	- ABAC - Attribute Based Access Control
	- Node Authz
	- Webhook Mode
	
- all communication between the cluster components such as etcd storage, agent runing on worker nodes i.e. kubelet, kube proxy, kube scheduler, kube controller manager, Kube apiserver is secured using TLS encryption certificates.

3/ secure communication between applications running within cluster
	- by default all pods can access others within the cluster. can restrict access between them using network policies.
	




===================================================================================================

Authentication in Kubernetes cluster

===================================================================================================

- There are 4 types of users who access the kubernetes cluster.
	- Admins: access clsuter to perform admin tasks.
	- Developers: to test or deploy application
	- End User: access apps running on the cluster by accessing the hosts. security of end user access is controlled by the apps logic. 
	- Bots (system user): 3rd party apps accessing cluster for integration purpose.
	
- But its only 2 categories of users.
	- Human (Admins and Developers)
	- System (processess, 3rd part apps, bots etc)
	

- Kubernetes does not manage users or user accounts natively. it relies on external source like a static file with user details or certificates, or 3rd part identity service like LDAP to manage these users.

- So we can not create users like "kubectl create user user_1" or view the users "kubectl list users". it does not work.

- But Kubernets manage service account user. "kubectl create serviceaccount sa1" or "kubectl get serviceaccount"

- all users access to cluster via kubectl or direct API access (curl https://<kuber-master>/6443/api) is managed by the kube-apiserver. The kube-apiserver authenticates the request before processing it.



Authentication mechanism:
---------------------------------------
- list of username and password in a static password file: ex: user_pass.csv, last column is optional to specify the group of the user
	
user_pass.csv
password123,username1,userid1,group1
password123,username1,userid1,group1
password123,username1,userid1,group2
password123,username1,userid1,group2

then pass this file name while setting up the kube-apiserver (--basic-auth-file=user_pass.csv)	
	

- list of usernames and tokens in a static token file: ex: user_token.csv, last column is optional to specify the group.
	- pass the file during starting kube-apiserver (--token-auth-file=user_token.csv)	
	
user_token.csv
hkda98d7nd2898du2kd0jd2d2md9d2hfjf,username1,userid1,group1
hkda98d7nd2898du2kd0jd2d2md9d2hfjf,username1,userid1,group1
hkda98d7nd2898du2kd0jd2d2md9d2hfjf,username1,userid1,group1
	
- certificates
	
- Identity service: connect 3rd part service like LDAP and authN user.
	


Authenticate User
-------------------
- to authenticate using the basic credentials (i.e. username and its password) while accessing the api server, specify the user and password in a curl command (-u "username1:password1")

curl -v -k https://<kube-master-node-ip>:6443/api/v1/pods -u "username1:password1"


- to authenticate using tokens:

curl -v -k https://<kube-master-node-ip>:6443/api/v1/pods --header "Authorization: Bearer hkda98d7nd2898du2kd0jd2d2md9d2hfjf"

- authN using certificates:

curl https://<kube-master-node-ip>:6443/api/v1/pods \
	--key admin.key
	--cert admin.crt
	--cacert ca.crt





======================================================================================================

Security - KubeConfig

======================================================================================================

- so far we have seen, user access kubernets using direct kube API via certificate authN:

ex: 
curl command to list all pods using REST API:

$ curl https://<kube-master>:6443/api/v1/pods \
	--key admin.key
	--cert admin.crt
	--cacert ca.crt
	
- this request is then validated by kube-apiserver to authenticate the user.

- How authN works while accessing cluster via kubectl command.

> kubectl get pods
	--server kube-master:6443
	--client-key admin.key
	--client-certificate admin.crt
	--certificate-authority ca.crt
	
- but typing these info/creds everytime is tedious job. so we move these creds into a config file called KubeConfig ($HOME/.kube/config) and then specify that file as --kubeconfig if the name of file is other than "config"

> kubectl get pods --kubeconfig <custom_config_file_name>   // just the name of the file if the file is inside ($HOME/.kube/config) else the full path.

ex:
kubectl get pods --kubeconfig /root/my-kube-config

Or

kubectl get pods --kubeconfig my-kube-config     // if my-kube-config is kept in $HOME/.kube/



- kubectl tool look for a file config under user's $HOME/.kube/config, bydefault it refers to this config file hence we dont have to specify --kubeconfig option explicitly in kubectl command.



KubeConfig file structure ($HOME/.kube/config)
------------------------------------------------
- the file has 3 sections and its an yaml file. each of these accept array to specify multiple clusters/users/contexts
	
	- Clusters: various k8s cluster that we need access to. i.e. "--server kube-master:6443" goes this section.
	
	- Users: various user (admin user, dev/prd user) accounts with which we have access to these various clusters. these users have different privileges on different clusters. i.e. "--client-key admin.key", "--client-certoficate admin.crt" goes in this section.
		
	- Contexts: context maps Clusters to its Users. i.e. which user account to be used to access which cluster. ex: create a context named admin@Production to map admin user accessing Production cluster.
	i.e.
	
	
sample config file:
-------------------
apiVersion: v1
kind: Config

current-context: dev-user@gcp   // optional. this get set using kubectl use-context command.

clusters:

- name: my-kube-master
  cluster: 
    certificate-authority: ca.crt
	server:  https://my-kube-master:6443

- name: development
- name: production
- name: gcp
..
..

contexts:

- name: my-kube-admin@my-kube-master
  context:
    cluster: my-kube-master    // name of the cluster from cluster section
	user: my-kube-admin		  // name of the user from users section

- name: dev-user@gcp
- name: prd-user@production
- name: admin-user@development

users:

- name: my-kube-admin
  user:
    client-certificate: admin.crt
	client-key: admin.key

- name: admin-user
- name: dev-user
- name: prd-user
- name: gcp-user
..
..


*** NOTE: we dont create object using this file with kubectl tool. the file is read by kubectl automatically.


- Now how kubectl know which context to use out of many, add the field: "current-context: dev-user@gcp" to config file.


kubectl command to view/edit KubeConfig file
--------------------------------------------------

- to view file, bydefault it looks in $HOME/.kube/config file.

> kubectl config view


- to specify a different location

> kubectl config view --kubeconfig=my-custom-config


- to add cluster details to your configuration file
> kubectl config set-cluster development --server=https://1.2.3.4 --certificate-authority=fake-ca-file
> kubectl config --kubeconfig=config-demo set-cluster scratch --server=https://5.6.7.8 --insecure-skip-tls-verify


- Add user details to your configuration file:

> kubectl config set-credentials developer --client-certificate=fake-cert-file --client-key=fake-key-seefile
> kubectl config --kubeconfig=config-demo set-credentials experimenter --username=exp --password=some-password


- Add context details to your configuration file:

> kubectl config set-context development@frontend --cluster=development --namespace=frontend --user=developer
> kubectl config set-context development@storage --cluster=development --namespace=storage --user=developer
> kubectl config --kubeconfig=config-demo set-context experimenter@scratch --cluster=scratch --namespace=default --user=experimenter


- setting namespace to current context is option which can also be set later. set namespace to current-context to avoid passing -n for every kubectl
$ kubectl config set-context --current --namespace=<namespace_name>


- Use a specific context

>  kubectl config use-context development@frontend


- In a particular context there could be multiple namaspaces, it is possible to add any specific namespace along with the context so that when we use that context, all kubectl command will query on that specified namespace only.

contexts:
- name: my-kube-admin@my-kube-master
  context:
    cluster: my-kube-master    // name of the cluster from cluster section
	user: my-kube-admin		  // name of the user from users section
	namespace: finance        // all kubectl command will query on this namespace only.
	

- to know the current context:
$ kubectl config --kubeconfig=my-kube-config current-context 
dev-user@test-cluster-1

$ kubectl config --kubeconfig=/root/my-kube-config current-context  // if the custom-config file is different location.



Certificates in KubeConfig
-------------------------------
- we add the certificate for a given cluster like below:

clusters:
- name: my-kube-master
  cluster: 
    certificate-authority: ca.crt    // name of the certificate. default path: /etc/kubernetes/pki/
	server:  https://my-kube-master:6443


- we should give the complete path:

clusters:
- name: my-kube-master
  cluster: 
    certificate-authority: /etc/kubernetes/pki/ca.crt


- Now the usual content of any certificate will be something like below:

------ BEGIN CERTIFICATE --------
kkdwkjkljw982klf93lkd9ldncs9ffwhj8910373bdjagfri
nbfgjd729nkdhka8203fkf82092u9dkfaewtflf00275n0nd
nbfgjd729nkdhka8203fkf82092u9dkfaewtflf00275n0nd
nbfgjd729nkdhka8203fkf82092u9dkfaewtflf00275n0nd
nbfgjd729nkdhka8203fkf82092u9dkfaewtflf00275n0nd
nbfgjd729nkdhka8203fkf8==
------ END CERTIFICATE --------

- another way to add certificate is directly add the certificate content instead of path, but first convert the certificate content to base64 format.

> cat ca.crt | base64
LHGUFJJ&88992LGHFFHKK
LHGUFJJ&88992LGHFFHKK
LHGUFJJ&88992LGHFFHKK
LHGUFJJ&88992LGHFFHKK


clusters:
- name: my-kube-master
  cluster: 
    certificate-authority-data: LHGUFJJ&88992LGHFFHKK
								LHGUFJJ&88992LGHFFHKK
								LHGUFJJ&88992LGHFFHKK
								LHGUFJJ&88992LGHFFHKK
								

Practice KubeConfig
-----------------------------
- sample config file: /root/.kube/config

apiVersion: v1
kind: Config
preferences: {}
current-context: kubernetes-admin@kubernetes
clusters:
- cluster:
    server: https://controlplane:6443
    certificate-authority-data:		LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQ
						URBTkJnaNkSG05K2RLdXdWVlRiSW1RQ2pWNjB3WUd6bklzT0VuQ1lKZ0t0Rjc5bEVYaFIwCjF2VnF
  name: kubernetes
  
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes

users:
- name: kubernetes-admin
  user:
    client-certificate-data: 	LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURFekNDQWZ1Z0F3SUJBZ0lJUjZO
				UZ3MHlNekF4TWpBeE1EUTFNalZhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd
    client-key-data:	LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcEFJQkFBS0NBUUVBNWRmREZreEVl
				ldZbWVYN2x3TkExcEpjbVhBVWV3NjM0TGtJRXRkClJ5dnBVWE96K1dhOVg4TG9TQnh0RXdnMUE2NWhXcElte




- sample custom kubeconfig:

$ pwd
/root/.kube
$ ls -ltr
total 16
-rw------- 1 root root 5568 Jan 20 10:45 config
drwxr-x--- 4 root root 4096 Jan 20 10:49 cache
-rw-r--r-- 1 root root 1456 Jan 20 11:04 my-kube-config

my-kube-config
--------------
apiVersion: v1
kind: Config
current-context: test-user@development
preferences: {}

clusters:
- name: production
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: development
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: kubernetes-on-aws
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: test-cluster-1
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

contexts:
- name: test-user@development
  context:
    cluster: development
    user: test-user

- name: aws-user@kubernetes-on-aws
  context:
    cluster: kubernetes-on-aws
    user: aws-user

- name: test-user@production
  context:
    cluster: production
    user: test-user

- name: research
  context:
    cluster: test-cluster-1
    user: dev-user

users:
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key
- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key
- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key



- in case the ser cred are setp using auth-providers, the config looks like below:

users:
- name: oidc
  auth-provider:
    config:
	  client-id: <>
	  client-secret: <>
	  id-token: <>
	  refresh-token: <>
	  idp_issuer_url: <>



-- use the dev-user to access test-cluster-1. Set the current context to the right one

$ kubectl config --kubeconfig=my-kube-config set-context dev-user@test-cluster-1 --cluster=test-cluster-1 --user=dev-user
Context "dev-user@test-cluster-1" created.

- context:
    cluster: test-cluster-1
    user: dev-user
  name: dev-user@test-cluster-1


$ kubectl config --kubeconfig=my-kube-config use-context dev-user@test-cluster-1 
Switched to context "dev-user@test-cluster-1".


- to know the current context
$ kubectl config --kubeconfig=my-kube-config current-context 
dev-user@test-cluster-1






==============================================================================================

Authorization

==============================================================================================

- once a user human (admins/developers) or system (bots via service accounts) users gain access to the kubernetes cluster, controlling what they can do is managed by AuthZ.

- Admin user have super access to the cluster like view objcets, create objects and even adding/delete nodes to the cluster. but there are other users like developres, testers, monitorring 3rd party apps.

- But the developer user should not have rights for modifying the clusters like adding/deleting nodes etc, or storage or networking configs. we an allow them to view but not modify.

- similary, for service account, very minimal level of access to perform specific operation like viewing certain type of cluster metrics or healths stats and not viewing any other objects like pods,services,deployments etc.

- AuthZ can also help in applying restriction rules for certain user to have access to only their namepsace and not any other namespace used by other teams in a shared cluster.


AuthZ Mechanism
---------------------------------------
- Node AuthZ:

	- kube-apiserver is accessed by users (admins/devs) as well as by kubelets agents running on worker nodes within the cluster. kubelets access the api-server to read/write info like:
	read: Services, Endpoints, Nodes, Pods
	write: Node status, Pod status, events
	
	- these access request are handled by a special authrozer knwon as "Node Authorizer".
	
	- kubelets are part of system node group and have names prefixed with system-node*, so any access request coming from a user with a name starting with system-node and part of system-node group is authorized by this type of authZ and granted specific privileges required by the kubelets.
	

- ABAC (Attribute Based Access Control)

	- associate a user or a group of users with a set of permissions. ex: we say a dev-user can view/create/delete pod. we do this by creating a policy file with a set of policies defined in JSON and pass this file to apiserver.
	
	{ "kind": "Policy", "spec": { "user": "dev-user", "namespace": "", "resource": "pods", "apiGroup": "*" } }
	
	- similary we can create this type of policy definition file for each group or user.
	
	- everytime we need to add or make a change in the policy, we must edit these policy file manually and restart kube-apiserver which is very tedious.


- RBAC (Role Based Access Control)
	- with RBAC, instead of directly associating the users or the group to the set of permissions, we define a role i.e. create a role ex: developer with a set of privileges (view/create/delete pod) required for developers, then provision that role to all the developer type users in the cluster. Same for security user, Security Role (view/approve CSR).
	
	- any change needed on the privileges, is done at Role level and same reflects to all its associated users immediately.


- Webhook
	- to integrate with 3rd party identity providers. If we want to out-source these whole authZ mechanism to any external service and not use any of these above built-in k8s solution. ex: Open Policy Agent is a 3rd part tool that helps in admission control and authZ, we can have kube-apiserver make an external api call to this agent to know if a given user access/operation can be granted. based on agent's response, kube-apiserver allow the operation.
	


AuthZ Mode
---------------------------------------
- in addition to the above 4 authZ types (Node, ABAC, RBAC, Webhook) there are 2 additional authZ modes/types.

- AlwaysAllow: allows all request without performing any authZ checks.
- AlwaysDeny: Denies all request.

- The type of AuthZ mechanism is set using the "--authorization-mode" config field to kube-apiserver. Its "AlwaysAllow" bydefault if we dont pass this config field (--authorization-mode=AlwaysAllow)

ExecStart=/usr/local/bin/kube-apiserver \\
--advertise-address=${INTERNAL_IP} \\
--allow-privileged=true \\
--apiserver-count=3 \\
--authorization-mode=Node,RBAC,Webhook \\

- we may provide multiple types comma separated, that we wish to use.

- when we have multiple authZ mode/type configured, any access request is passed through each of these modes in the order it is specified. 

USER --->	NODE Authz	--->	RBAC Authz	--->	Webhook Authz

ex: when a dev user send a access request, in this case it gets first handled by Node AuthZ which handles only node request, hence it denies. whenevr a module denies a request, the same request is forwared to the next module in the chain, here RBAC performs its checks and grants the access to dev-user. authZ is complete and the user is given access to the requtes operation. 

- ** every time a module denies a request, it goes to the next one in the chain and as soon as any module approves the request, no more checks are done and the user is granted permission.


- Check the authorization modes configured on the cluster. (look for --authorization-mode=Node,RBAC)

$ kubectl describe pod kube-apiserver-controlplane -n kube-system

Name:                 kube-apiserver-controlplane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 controlplane/10.51.138.6
Start Time:           Thu, 20 Jan 2022 18:43:53 +0000
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.51.138.6:6443
                      kubernetes.io/config.hash: c89ea5e7cca5b55e748e8e4ddd432c41
                      kubernetes.io/config.mirror: c89ea5e7cca5b55e748e8e4ddd432c41
                      kubernetes.io/config.seen: 2022-01-20T18:43:50.959500112Z
                      kubernetes.io/config.source: file
Status:               Running
IP:                   10.51.138.6
IPs:
  IP:           10.51.138.6
Controlled By:  Node/controlplane
Containers:
  kube-apiserver:
    Container ID:  docker://0c3d7f5f50f173190708eec67252945f88f0ee4d52e30946e51bcd170691b26c
    Image:         k8s.gcr.io/kube-apiserver:v1.20.0
    Image ID:      docker-pullable://k8s.gcr.io/kube-apiserver@sha256:8b8125d7a6e4225b08f04f65ca947b27d0cc86380bf09fab890cc80408230114
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=10.51.138.6
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
	  --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --insecure-port=0
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/12
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Thu, 20 Jan 2022 18:43:35 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://10.51.138.6:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://10.51.138.6:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://10.51.138.6:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>




============================================================================================

Role Based Access Controls

============================================================================================

https://kubernetes.io/docs/reference/access-authn-authz/rbac/

Role and ClusterRole object
----------------------------
- An RBAC Role or ClusterRole contains rules that represent a set of permissions.
- Role: A Role always sets permissions within a particular namespace; when you create a Role, you have to specify the namespace it belongs in.
- ClusterRole: ClusterRole, by contrast, is a non-namespaced resource. The resources have different names (Role and ClusterRole) because a Kubernetes object always has to be either namespaced or not namespaced; it can't be both.

- Role and RoleBinding (to asscociate user to Role) are namespaced i.e. they are created on a specific namespace. if namespace is not provided it will be created in default namespace. Hence it controls access within that namespace alone.

- Role and RoleBinding (to asscociate user to Role) are used to authorize namespaced resources.

- various resources/objects in kubernetes are categorized either to Namespaced (pod, service, deployment, role, rolebinding etc) OR to Cluster scoped (ex: node can not belong to a namespace)

- namespaced resources are always created in a given namespace, if not provided, gets created in default namespace.

- to list all the namespaces resources:

$ kubectl api-resources --namespaced=true



- cluster scoped resources are those where we dont specify namespace like nodes, PV, clusterrole, clusterrolebindings, namespaces etc.

- to list all the cluster scoped resources:

$ kubectl api-resources --namespaced=false


STEP-1
- a Role is created using Role object.
- can add multiple rule for each type of resources under a single role.

developer-role.yaml
--------------------
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: developer-role
rules:
- apiGroups: [""] 				// "" indicates the core API group, any other group, specify the group name.
  resources: ["pods"]
  verbs: ["get", "watch", "list", "create", "delete", "update"]
- apiGroups: [""] 				// "" indicates the core API group, any other group, specify the group name.
  resources: ["configmaps"]
  verbs: ["create"] 


- same can also be created like: 
$ kubectl create role developer-role --namespace=default --verb=list,create,delete --resource=pods

** NOTE: within a given a resource, if we want to restrict to only some specific pod by its name, add "resourceNames" field.
ex:

rules:
- apiGroups: [""] 				// "" indicates the core API group, any other group, specify the group name.
  resources: ["pods"]
  verbs: ["create", "delete"]
  resourceNames: ["pob_name_blue", "pod_name_green"]  // give access for create/delete on 2 pods only named: pob_name_blue and pod_name_green


$ kubectl create -f developer-role.yaml
role.rbac.authorization.k8s.io/developer created

STEP-2
- associate the user to tis role. hence create RoleBinding object.
- multiple user can be bound to single role.

devuser-developer-rolebinding.yaml
----------------------------------
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-rolebinding
  namespace: finance   // specifying namespace, will limit this mapping of devuser to developer-role on specific namespace. if not provided, its default namespace.
subjects:
- kind: User    // it can also be Group for group of user
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer-role # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
  
 
$ kubectl create -f  devuser-developer-rolebinding.yaml
rolebinding.rbac.authorization.k8s.io/dev-user-binding created


- the same can also created like:
$ kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user


View RBAC
-------------
- to view the created roles.

$ kubectl get roles


NAME			AGE
developer-role	4s


$ kubectl get role --all-namespaces

NAMESPACE     NAME                                             CREATED AT
blue          developer                                        2022-01-20T18:48:15Z
kube-public   kubeadm:bootstrap-signer-clusterinfo             2022-01-20T18:43:48Z
kube-public   system:controller:bootstrap-signer               2022-01-20T18:43:47Z
kube-system   extension-apiserver-authentication-reader        2022-01-20T18:43:47Z
kube-system   kube-proxy                                       2022-01-20T18:43:49Z
kube-system   kubeadm:kubelet-config-1.20                      2022-01-20T18:43:47Z
kube-system   kubeadm:nodes-kubeadm-config                     2022-01-20T18:43:47Z
kube-system   system::leader-locking-kube-controller-manager   2022-01-20T18:43:47Z
kube-system   system::leader-locking-kube-scheduler            2022-01-20T18:43:47Z
kube-system   system:controller:bootstrap-signer               2022-01-20T18:43:47Z
kube-system   system:controller:cloud-provider                 2022-01-20T18:43:47Z
kube-system   system:controller:token-cleaner                  2022-01-20T18:43:47Z


- to list rolebindings:

$ kubectl get rolebindings

NAME							AGE
devuser-developer-rolebinding	24s

ex:

$ kubectl get rolebinding -n kube-system 
NAME                                                ROLE                                                  AGE
kube-proxy                                          Role/kube-proxy                                       16m
kubeadm:kubelet-config-1.20                         Role/kubeadm:kubelet-config-1.20                      16m
kubeadm:nodes-kubeadm-config                        Role/kubeadm:nodes-kubeadm-config                     16m
system::extension-apiserver-authentication-reader   Role/extension-apiserver-authentication-reader        16m
system::leader-locking-kube-controller-manager      Role/system::leader-locking-kube-controller-manager   16m
system::leader-locking-kube-scheduler               Role/system::leader-locking-kube-scheduler            16m
system:controller:bootstrap-signer                  Role/system:controller:bootstrap-signer               16m
system:controller:cloud-provider                    Role/system:controller:cloud-provider                 16m
system:controller:token-cleaner                     Role/system:controller:token-cleaner                  16m



- to see the details.

$ kubectl describe role <role_name>

ex:
$ kubectl describe role kube-proxy -n kube-system
Name:         kube-proxy
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources   Non-Resource URLs  Resource Names  Verbs
  ---------   -----------------  --------------  -----
  configmaps  []                 [kube-proxy]    [get]


- to see details about role bindings.

$ kubectl describe rolebinding <role_binding_name>
ex:
kubectl describe rolebinding devuser-developer-rolebinding


$ kubectl describe rolebinding kube-proxy -n kube-system 

Name:         kube-proxy
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  Role
  Name:  kube-proxy
Subjects:
  Kind   Name                                             Namespace
  ----   ----                                             ---------
  Group  system:bootstrappers:kubeadm:default-node-token 



Edit RBAC
-----------
$ kubectl edit role developer -n blue
role.rbac.authorization.k8s.io/developer edited



- Grant the dev-user permissions to create deployments in the blue namespace.
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: blue
  name: deploy-role
rules:
- apiGroups: ["apps", "extensions"]
  resources: ["deployments"]
  verbs: ["create"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-deploy-binding
  namespace: blue
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: deploy-role
  apiGroup: rbac.authorization.k8s.io




Check Access for a given user (auth can-i command)
---------------------------------------------------
- to just check access for the current user for an operaion on given resource.

$ kubectl auth can-i <operation verbs i.e. get/create/delete> <resource_name>

ex:
$ kubectl auth can-i create deployments
yes


$ kubectl auth can-i delete node
no


- in order to test access previleges for any other user other than current logged-in user.
$ kubectl auth can-i create deployments --as dev-user
no

$ kubectl auth can-i create pods --as dev-user
yes


- we can also test the same for a given namespace.

$ kubectl auth can-i create pods --as dev-user --namespace test  // dev-user is not setup to have privileges on test namespace.
no






=============================================================================================================

Cluster Roles

=============================================================================================================

- we now know about Role and RoleBinding.

- cluster scoped resources are those where we dont specify namespace like nodes, PV, clusterrole,clusterrolebindings, namespaces etc.

- to list all the cluster scoped resources:

$ kubectl api-resources --namespaced=false


- ClusterRole and ClusterRoleBinding (to asscociate user to ClusterRole) are used to authorize cluster scoped resources (like nodes, PV etc.)

- ClusterRole are just like Role except they are for controlling access to Cluster scoped resources.
ex:  
1/ "ClusterAdmin" role can be created to provision a cluster admin user and give permission to view/create/delete node in the cluster across namespace.

2/ "StorageAdmin" role can be created to provision a cluster storage admin user and give permission to view/create/delete PV and PVC in the cluster across namespace.


*** NOTE: ClusterRole can also be used to control access for namespaced scoped resources. BUT then that ClusterRole will be for entire cluster acrosss namespaces.

STEP-1
- create a ClusterRole object.

cluster-admin-role.yaml
-----------------------
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  namespace: default
  name: cluster-administrator
rules:
- apiGroups: [""] 				// "" indicates the core API group, any other group, specify the group name.
  resources: ["nodes"]
  verbs: ["get", "list", "create", "delete"]
  
$ kubectl create -f cluster-admin-role.yaml



STEP-2
- link the cluster admin user to this ClusterRole.

cluster-admin-role-binding.yaml
----------------------------------
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-bind
  namespace: finance   // specifying namespace, will limit this mapping of devuser to developer-role on specific namespace. if not provided, its default namespace.
subjects:
- kind: User
  name: cluster-admin-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-administrator # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io


$ kubectl create -f cluster-admin-role-binding.yaml





===================================================================================

Admission Controllers

===================================================================================

- everytime we run a command using kubectl utility, it sends the request (say create a pod), it goes to kube-apiserver and then the pod gets created, and this info is finally persisted in etcd database.

- when the request hits the kube-apiserver, it goes through an Authentication process and this usually done through certificates. if the request is submitted via kubectl utility, it uses the creds/certs defined in $HOME/.kube/config file. The AuthN process is responsible to indentify the request, and makes sure the user is valid.

- then the request goes through an Authorization process, where it checks if the authenticated user has right permission to perform the operation. this is done through Role Based Access Control.


USER	-->		Authentication		--> 		Authorization	-->		Admission Controllers		-->		create pod. 


- as we can see, most of thes rules that we can create using RBAC is at the kubernetes API level, i.e. user is allowed what operations on what API i.e. create/view/list on pods

- Admission Controllers comes after Authorization stage, to apply more granular rules in how any object will be created. 
ex:
when a pod creation request comes in, we would like to review the config file and 
1/ look at the image name and say dont allow images from public registry. i.e. only permit images from the internal registry, 
OR enforce, we must never use the latest tag for images.
2/ do not permit runAs root user.
3/ only permit certain capabilities only. in this case adding "MAC_ADMIN" capabilities should not be allowed.
4/ ensure metadata section always contains labels.

web-pod.yaml
-------------
apiVersion: v1
kind: Pod
metadata: 
  name: web-pod
spec:
  containers:
  - name: ubuntu
    image: ubuntu:latest
	command: ["sleep", "3600"]
	securityContext: 
	  runAsUser: 0
	capabilities:
	  add: ["MAC_ADMIN"]
	  
	  
- So these are some of fine grained rules that cant be achieved using RBAC, hence Admission Controllers helps implementing better security measures to enforce how a cluster is used.

- apart from simply validating the requested definition, it can also change the request itself or perform additional operations before the pod gets created. 

- There are no. of pre built-in Admission controllers. ex: 
	- AlwaysPullImages: ensure everytime a pod is created the image is always pulled afresh
	- DefaultStorageClass: observes creation of PVC and automatically adds a default storage class to them if one is not specified. 
	- EventRateLimit: set a limit that the kube-apiserver can handle at a time and prevent apiserver from flooding multiple request.
	- NamespaceAutoProvision: disabled by default. it automatically creates the namespace if it does not exists. 
	- NamespaceExists: rejects request to create any objects on a namespace if the namepsace does not exist. enabled bydefault.


NamespaceExists Admission Controller
-------------------------------------------
- say we have a namespace "blue" that does not exists.

- running the below command will throw error: <Error from server (NotFound): namespaces "blue" not found>

> kubectl run nginx --image nginx --namespace blue

- here this request gets authenticated and then authorized and it then goes though the built-in admission controllers. the "NamespaceExists" admission controllers handles the request and checks if the blue namespace exists, if it is not then the request gets rejected. 

- the "NamespaceExists" is built-in admission controller that is enabled by default.


NamespaceAutoProvision Admission Controller
-------------------------------------------	
- NamespaceAutoProvision is another built-in admission controllers that is not enabled by default, it automatically creates the namespace if it foes not exists. 


NamespaceLifecycle Admission Controller
-------------------------------------------	
** NOTE: Note that the NamespaceExists and NamespaceAutoProvision admission controllers are deprecated and now replaced by NamespaceLifecycle admission controller.

- The NamespaceLifecycle admission controller will make sure that requests to a non-existent namespace is rejected and that the default namespaces such as default, kube-system and kube-public cannot be deleted.


View all by-default enabled Admission Controllers
--------------------------------------------------
$ kube-apiserver -h | grep enable-admission-plugins


- Since the kube-apiserver is running as pod you can check the process to see enabled and disabled plugins.

$ ps -ef | grep kube-apiserver | grep admission-plugins

root     18825 18808  0 20:35 ?        00:00:05 kube-apiserver --advertise-address=10.55.41.9 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision --disable-admission-plugins=DefaultStorageClass --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key


- to add a new admission controller, update the --enable-admission-plugins flag on the kube-apiserver

- to disable any of the controllers, use the --disable-admission-plugins flag on the kube-apiserver



- Which admission controller is enabled in this cluster which is normally disabled?
Check enable-admission-plugins in /etc/kubernetes/manifests/kube-apiserver.yaml

$ cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep enable-admission-plugins
    - --enable-admission-plugins=NodeRestriction
	

- enable the NamespaceAutoProvision admission controller

Add NamespaceAutoProvision admission controller to --enable-admission-plugins list to /etc/kubernetes/manifests/kube-apiserver.yaml
API server will automatically restart and pickup this configuration.


- Disable DefaultStorageClass admission controller

Update /etc/kubernetes/manifests/kube-apiserver.yaml as below

   - --disable-admission-plugins=DefaultStorageClass

Note: Once you update kube-apiserver yaml then please wait few mins for the kube-apiserver to restart completely.




Validating and Mutating Admission Controllers
===================================================================================================

- 2 types of admission controller. Validating Admission Controllers and Mutating Admission Controllers
	- Validating Admission Controllers: those that can validate the request and then allow/deny.
	- Mutating Admission Controllers: change the incoming request itself before its created.

- the "NamespaceExists" is built-in admission controller, helps validating if the namespace exists and rejects the request if it does not exists. this is known as a validating admission controller. enabeld by default.

- the "DefaultStorageClass" built-in admission controller, intercepts any PVC creation request, checks if it has any StorageClass defined, if not, it will modify the definition and add a default Storage class. this is known as a Mutating admission controller. it can change the object itself before its created.

pvc-definition.yaml
--------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  myclaim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
  storageClassName: default
  
  
- there can be an admission controller that can do both validate as well as mutate.

- generally mutating admission controller are invoked first followed by Validating admission controllers, this is so that any changes made by Mutating controller can be validated by Validating admission controller.
ex: "NamespaceAutoProvision" Mutating admission controller is invoked firts followed by "NamespaceExists" Validating admission controller. the reverse order would cause always rejecting the request if the namespace does not exists by "NamespaceExists", and "NamespaceAutoProvision" would never get invoked.

- when a request goes thorugh these admission controllers, if any admission controller rejects the request, the request is REJECTED and an error message is shown to the user.




Custom Admission Controller (Dynamic Admission Control)
====================================================================
- How we can configure our own admission controller with our own validation or mutation logic.

- admission plugins can be developed as extensions and run as webhooks configured at runtime.


admission webhooks
--------------------------
- Admission webhooks are HTTP callbacks that receive admission requests and do something with them. two types of admission webhooks, validating admission webhook and mutating admission webhook. 

- with the help of Admission webhooks, we can write our own admission controller.

- we can configure these webhooks to point to a Server, thats hosted either within the kubernetes cluster or outside, and that server will have its 
Admission webhook service running with its own custom logic. 

- after any request to kube-apiserver goes through to all built-in admission controllers, it hits the Webhook if configured. once it hits the webhook, it makes a call to the Admission Webhook server by passing the admission reveiw object in JSON format, this oJSON object has all the details about the request such as the user that made the request, type of operation on what object and details about the object etc.

- on receiving the admission object, the admission webhook server running our custom logic should respond an admission review object with a result of whether the request is allowed or not. if the "allowed" field is set to "true" then the request is allowed else denied.



- How do we set this up:

STEP-1

- Develop our own custom admission webhook server is a service that we deploy contains the logic or the code to permit/reject a request and it must be able to receive and respond with the appropriate response that the admission webhook and/or mutating admission webhook expects.


- there will be 2 calls, a/ validate and b/ mutate

- sample sudo code in python: 

/* validates the request, rejects request if the username and objectname to be created are equal else accept */

@app.route("/validate", methods=["POST"])
dev validate():
	object_name = request.json["request"]["object"]["metadata"]["name"]
	user_name = request.json["request"]["userInfo"]["name"]
	status = True
	if object_name == user_name:
		message = "You can not create object with your own name"
		status = False
	
	return jsonify (
		{
			"response": {
				"allowed": status,
				"uid": request.json["request"]["uid"],
				"status": {"message": message}
			}
		}
	)
	
	

- the mutate function gets the object and responds with a patch object wrapped in it. a Patch json object is a list of patch operation being add/remove/replace etc, then specify the path within the json object that needs to targetted for change, and then the value that needs to be added.

- this patch then gets encoded as base64 and then sent as part of the response.

/* adds the username as a label inside metadata */

@app.route("/mutate", methods=["POST"])
dev mutate():
	user_name = request.json["request"]["userInfo"]["name"]
	patch = [{ "op": "add", "path": "/metadata/labels/users", "value": user_name }]
	return jsonify (
		{
			"response": {
				"allowed": True,
				"uid": request.json["request"]["uid"],
				"patch": base64.b64encode(patch),
				"patchtype": "JSONPatch"
			}
		}
	)
	
	
	

STEP-2
- once our webhook server is ready, the next step is to host it.
- we either run it as a server somewhere or containerize and deploy within kubernetes cluster as deployment, if deployed as a deployment (webhook-deployment) in a kubernets cluster, then it needs a service (webhook-service) to be accessed.


STEP-3
- we then configure this webhook in kubernetes by creating the respective Webhook configuration object (ValidatingWebhookConfiguration/MutatingWebhookConfiguration).

Prerequisites:

- Ensure that the Kubernetes cluster is at least as new as v1.16 (to use admissionregistration.k8s.io/v1), or v1.9 (to use admissionregistration.k8s.io/v1beta1).

- Ensure that MutatingAdmissionWebhook and ValidatingAdmissionWebhook admission controllers are enabled. 

- Ensure that the admissionregistration.k8s.io/v1 or admissionregistration.k8s.io/v1beta1 API is enabled.


custom-validate-webhook-config.yaml
---------------------------------------
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: "pod-policy.example.com"
webhooks:
- name: "pod-policy.example.com"
  clientConfig:
    service:
      namespace: "webhook-namespace"
      name: "webhook-service"
    caBundle: "Ci0tLS0tQk...haha"
  rules:
  - apiGroups:   [""]
    apiVersions: ["v1"]
    operations:  ["CREATE"]
    resources:   ["pods"]
    scope:       "Namespaced"


- Here, in clientConfig, we configure the location of our admission webhook server, if deployed externally. the provide the url like below:

clientConfig:
  url: https://external-server.example.com
  
if deployed on kubernets cluster, give the name of the service and the namespace where its deployed.

- we also need to add the certificate as caBundle used for communication between kube-apiserver to our custom admission webhook service.


- Here, in rules we specify set of rules to config exactly when to call our webhook server like which particular object and which operation. ex: we only wanted to be called during creating of a new pod request.

Exercise:
----------
- Create TLS secret named webhook-server-tls

$ kubectl -n webhook-demo create secret tls webhook-server-tls \
	--cert "/root/keys/webhook-server-tls.crt" \
	--key "/root/keys/webhook-server-tls.key"
	
secret/webhook-server-tls created


- Create webhook deployment 

webhook-deployment.yaml
-----------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webhook-server
  namespace: webhook-demo
  labels:
    app: webhook-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webhook-server
  template:
    metadata:
      labels:
        app: webhook-server
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1234
      containers:
      - name: server
        image: stackrox/admission-controller-webhook-demo:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8443
          name: webhook-api
        volumeMounts:
        - name: webhook-tls-certs
          mountPath: /run/secrets/tls
          readOnly: true
      volumes:
      - name: webhook-tls-certs
        secret:
          secretName: webhook-server-tls
		  

$ kubectl create -f webhook-deployment.yaml 
deployment.apps/webhook-server created


- Create webhook service now so that admission controller can communicate with webhook

webhook-service.yaml
--------------------
apiVersion: v1
kind: Service
metadata:
  name: webhook-server
  namespace: webhook-demo
spec:
  selector:
    app: webhook-server
  ports:
    - port: 443
      targetPort: webhook-api


$ kubectl create -f webhook-service.yaml 
service/webhook-server created

$ kubectl get all -n webhook-demo 
NAME                                  READY   STATUS    RESTARTS   AGE
pod/webhook-server-7c8b68dccc-57stp   1/1     Running   0          3m57s

NAME                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/webhook-server   ClusterIP   10.107.105.186   <none>        443/TCP   2m20s

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/webhook-server   1/1     1            1           3m57s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/webhook-server-7c8b68dccc   1         1         1       3m57s


-  added MutatingWebhookConfiguration under /root/webhook-configuration.yaml
webhook-configuration.yaml
--------------------------
apiVersion: admissionregistration.k8s.io/v1beta1
kind: MutatingWebhookConfiguration
metadata:
  name: demo-webhook
webhooks:
  - name: webhook-server.webhook-demo.svc
    clientConfig:
      service:
        name: webhook-server
        namespace: webhook-demo
        path: "/mutate"
      caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURQekN
	  RWUpLb1pJaHZjTkFRRUwKQlFBd0x6RXRNQ3NHQT
	  heUJFWlcxdgpJRU5CTUI0WERUSXlNREV5TVRFer
    rules:
      - operations: [ "CREATE" ]
        apiGroups: [""]
        apiVersions: ["v1"]
        resources: ["pods"]
		

$ kubectl create -f webhook-configuration.yaml
mutatingwebhookconfiguration.admissionregistration.k8s.io/demo-webhook created



- In previous steps we have deployed demo webhook which does below
- Denies all request for pod to run as root in container if no securityContext is provided.
- If no value is set for runAsNonRoot, a default of true is applied, and the user ID defaults to 1234
- Allow to run containers as root if runAsNonRoot set explicitly to false in the securityContext


- Deploy a pod with no securityContext specified.
pod-with-defaults.yaml	  
----------------------
# A pod with no securityContext specified.
# Without the webhook, it would run as user root (0). The webhook mutates it
# to run as the non-root user with uid 1234.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-defaults
  labels:
    app: pod-with-defaults
spec:
  restartPolicy: OnFailure
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]
	  

$ kubectl create -f pod-with-defaults.yaml 
pod/pod-with-defaults created

below gets added after pod is created.

securityContext:
    runAsNonRoot: true
    runAsUser: 1234	  



- Deploy pod with a securityContext explicitly allowing it to run as root
pod-with-override.yaml
----------------------
 A pod with a securityContext explicitly allowing it to run as root.
# The effect of deploying this with and without the webhook is the same. The
# explicit setting however prevents the webhook from applying more secure
# defaults.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-override
  labels:
    app: pod-with-override
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: false
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]
	  

$ kubectl create -f pod-with-override.yaml 
pod/pod-with-override created



- Deploy a pod with a conflicting securityContext i.e. pod running with a user id of 0 (root).
- Mutating webhook should reject the request as its asking to run as root user without setting runAsNonRoot: false
pod-with-conflict.yaml
----------------------
# A pod with a conflicting securityContext setting: it has to run as a non-root
# user, but we explicitly request a user id of 0 (root).
# Without the webhook, the pod could be created, but would be unable to launch
# due to an unenforceable security context leading to it being stuck in a
# 'CreateContainerConfigError' status. With the webhook, the creation of
# the pod is outright rejected.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-conflict
  labels:
    app: pod-with-conflict
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: true
    runAsUser: 0
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]
	  
# A pod with a conflicting securityContext setting: it has to run as a non-root
# user, but we explicitly request a user id of 0 (root).
# Without the webhook, the pod could be created, but would be unable to launch
# due to an unenforceable security context leading to it being stuck in a
# 'CreateContainerConfigError' status. With the webhook, the creation of
# the pod is outright rejected.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-conflict
  labels:
    app: pod-with-conflict
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: true
    runAsUser: 0
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]
	  

$ kubectl create -f pod-with-conflict.yaml 

Error from server: error when creating "pod-with-conflict.yaml": admission webhook "webhook-server.webhook-demo.svc" denied the request: runAsNonRoot specified, but runAsUser set to 0 (the root user)








================================================================================================

API Groups

================================================================================================

- all operation performed on the kubernetes cluster is done by kube-apiserver via kubectl utility or directly via REST

- The API group is specified in a REST path and in the apiVersion field of a serialized object.

- to check the version of cluster:

- There are several API groups in Kubernetes:
	- The core (also called legacy) group is found at REST path /api/v1. The core group is not specified as part of the apiVersion field, for example, apiVersion: v1.
	
	- The named groups are at REST path /apis/$GROUP_NAME/$VERSION and use apiVersion: $GROUP_NAME/$VERSION (for example, apiVersion: batch/v1, apps/vi).
	
- Enabling or disabling API groups
	- Certain resources and API groups are enabled by default.
	- You can enable or disable them by setting --runtime-config on the API server. The --runtime-config flag accepts comma separated <key>[=<value>] pairs describing the runtime configuration of the API server. 
	- If the =<value> part is omitted, it is treated as if =true is specified.
	ex:
	- to disable batch/v1, set --runtime-config=batch/v1=false
	- to enable batch/v2alpha1, set --runtime-config=batch/v2alpha1 OR --runtime-config=batch/v2alpha1=true

$ curl https://kube-master:6443/version 

{
	"major": "1",
	"minor": "13",
	"gitVersion": "v1.13.0",
	"gitCommit": "ddf47ac13c1a9483ea035a79cd7c10005ff21a6d",
	"gitTreeState": "clean",
	"buildDate": "2018-12-03T20:56:12Z",
	"goVersion": "go1.11.2",
	"compiler": "gc",
	"platform": "linux/amd64"
}


- to get the list of pods

$ curl https://kube-master:6443/api/v1/pods 
{
"kind": "PodList",
"apiVersion": "v1",
"metadata": {
"selfLink": "/api/v1/pods",
"resourceVersion": "153068"
},
"items": [
{
	"metadata": {
	"name": "nginx-5c7588df-ghsbd",
	"generateName": "nginx-5c7588df-",
	"namespace": "default",
	"creationTimestamp": "2019-03-20T10:57:48Z",
	"labels": {
	"app": "nginx",
	"pod-template-hash": "5c7588df"
},
"ownerReferences": [
{
	"apiVersion": "apps/v1",
	"kind": "ReplicaSet",
	"name": "nginx-5c7588df",
	"uid": "398ce179-4af9-11e9-beb6-020d3114c7a7",
	"controller": true,
	"blockOwnerDeletion": true
}
]
},
..
..



- all kubernets APIs are grouped together under the group name URI such as:
/metrics  	: monitoring
/healthz	: monitoring
/version	
/api		: core api for cluster functionalities
/apis		: named api for cluster functionalities
/logs		: for integrating with 3rd party logging application


ex:
$ curl http://localhost:6443 -k
{
"paths": [
"/api",
"/api/v1",
"/apis",
"/apis/",
"/healthz",
"/logs",
"/metrics",
"/openapi/v2",
"/swagger-2.0.0.json",
..
..
}



- core APIs:
/api (core) -> /v1 ->
/namaspaces
/pods
/rc
/events
/endpoints
/nodes
/bindings
/PV
/PVC
/configmaps
/secrets
/services



- named group APIs (/apis) are more organized, going forward, all the new features will come under this.

ex:
$ curl http://localhost:6443/apis -k | grep "name"    // lists all supported resources under apis group.
"name": "extensions",
"name": "apps",
"name": "events.k8s.io",
"name": "authentication.k8s.io",
"name": "authorization.k8s.io",
"name": "autoscaling",
"name": "batch",
"name": "certificates.k8s.io",
"name": "networking.k8s.io",
"name": "policy",
"name": "rbac.authorization.k8s.io",
"name": "storage.k8s.io",
"name": "admissionregistration.k8s.io",
"name": "apiextensions.k8s.io",
"name": "scheduling.k8s.io",


/apis/apps
	/v1 
		/deployments (resources)  --> list/get/create/delete/update/watch  (verbs)
		/replicasets
		/statefulsets

/extensions

/networking.k8s.io
	/v1
		/networkpolicies
		
/storage.k8s.io
/authentication.k8s.io
/certificates.k8s.io
	/v1
		/certificatesingingrequests
		

NOTE: 
/v1alpha1 is for Alpha
/v1beta1 is for Beta 
/v1 resource group is for GA/stable version.

- If we login to any cluster and try to curl the APIs like below:
$ curl https://<kube-master>:6443 -k8s

{
	"kind": "Status",
	"apiVersion": "v1",
	"metadata": {},
	"status": "Failure",
	"message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
	"reason": "Forbidden",
	"details": {},
	"code": 403
}

- we see 403 authentication error, we will not be allowed access except for certain API like https://<kube-master>:6443/version, as we have not specified any authN params in the request.

- we need to pass like: (<kube-master> can be localhost in case single node cluster)

$ curl http://<kube-master>:6443 –k
--key admin.key
--cert admin.crt 
--cacert ca.crt


{
	"paths": [
		"/api",
		"/api/v1",
		"/apis",
		"/apis/",
		"/healthz",
		"/logs",
		.
		..
	]
}



kubectl proxy
---------------
- kubectl proxy command launches a proxy service locally on port 8001 (default) and use credentials and certificates present in your kube config file ($HOME/.kube/config) to access the cluster, that way we dont have to specy the certs in curl command. so this proxy stamps the creds from kubeconfig and forwards the request to kube-apiserver.


$ kubectl proxy

Starting to serve on 127.0.0.1:8001



$ curl http://localhost:8001 -k

{
	"paths": [
		"/api",
		"/api/v1",
		"/apis",
		"/apis/",
		"/healthz",
		"/logs",
		"/metrics",
		"/openapi/v2",
		"/swagger-2.0.0.json",
		..
		.
	]
}



kube proxy vs kubectl proxy
---------------------------------------------
- they are not same.
- kube proxy: used to enable connectivity between pods and services acorss different nodes in the cluster.
- kubectl proxy: is a HTTP proxy service created by kubectl utility to access the kube-apiserver via REST






================================================================================

API Versions

================================================================================

- Each api group have 3 types of versioning. Different API versions indicate different levels of stability and support.

- Alpha

	- The version names contain alpha (for example, v1alpha1, v2alpha2 etc).
	- this feature is disabled by default
	- may contain bugs.
	- support for a feature may be dropped at any time without notice.
	
- Beta

	- The version names contain beta (for example, v2beta3).
	- well tested. enabled by default.
	- support for a feature will not be dropped, though the details may change.
	- not recommended for production uses
	- available for test users


- v1 (GA i.e. Generally Available) i.e Stable

	- The version name is vX where X is an integer.
	- avalable for all
	

ex:
/apis/apps/v1alpha1/deployments/<verbs>
/apis/apps/v1beta1/deployments/<verbs>
/apis/apps/v1/deployments/<verbs>
	

/apis/betworking.k8s.io/v1alpha1/<resource>/<verbs>


API Groups versions:
--------------------
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#-strong-api-groups-strong-

- An API Group can support multiple versions at the same time. ex: the apps API group can have v1alpha1, v1beta1 or v1 at the same time, means we will be to create the same object using any of these versions in yaml file.
ex: creating deployment in all 3 versions.

- Even through there are multiple versions supported at the same time, only 1 can be the PREFERRED or STORAGE version.

nginx-deployment.yaml
---------------------
apiVersion: apps/v1alpha1
kind: Deployment
metedata:
  name: nginx
spec:
 ..


nginx-deployment.yaml
---------------------
apiVersion: apps/v1beta1
kind: Deployment
metedata:
  name: nginx
spec:
 ..
 
 
nginx-deployment.yaml
---------------------
apiVersion: apps/v1
kind: Deployment
metedata:
  name: nginx
spec:
 ..



Preferred Version
------------------
- is the default version set to Kubernetes apiserver used when we retrieve information through k8s API using kubectl get command etc. 

ex: kubectl get deployment ----> which version this command going to query. thats defined by the preferred version set to kube-apiserver. in this case if v1 is set to preffred version then that version of API will be queried.

- kubectl explain deployment  ---> shows the preferred api version.

- preferred version is listed when we list the API 
ex: https://<kube-master>:6443/apis/batch
{
	kind: APIGroup
	
}


Storage Version
------------------	
- is the default version set to Kubernetes apiserver used when an object is finally created and stored in etcd database, its irrespective of the version used in definition yaml file.

- if storage version set in kubernetes is different than the version used in yaml file, then the definition object will be converted to the same storage version and the the object will be created and stored in etcd.

- if multiple versions are enabled i.e. v1alpha1, v1beta1, v1, at any point, only any one version can be set as Storage version.

- as of now, its not possible to see which is the storage version of particular API though an api or a command
	
	- way to find. query etcd database.
	
	ex: 
	$ etcdctl get "registry/deployments/default/blue" --print-value-only
	k8s
	
	apps/v1
	Deployment
	

NOTE: one one version for preferred or Storage version, usually both of these are same but they can be different.
 





=====================================================================================================

API Deprecations

=====================================================================================================

- along with kubernets own release version cycles every API groups also followes certain versioning.

- a single API group can support multiple versions at a time, why do we need multiple versions and how many old versions we should support, when can we remove an older version that is no longer required -- all this is answered by API deprication policy.

ex: a new API being developed:
/apis/kodekloud.com
	/v1alpha1
		/course
		/webinar
		
	/v1alpha2
		/course
		/webinar X  (dropped on this version.)


at this stage, if we can set the preferred/storage version to v1alpha2, so any old yaml creating an object of type course of v1alpha1 ver., will get converted to the preferred storage version v1slpha2 and then created.

Rules:
- an API featire can only be removed by incrementing the version of the API group. ex: v1alpha1 ver. has /webinar, to remove this feature, we need v1alpha2 ver. created and keep only /course

- other than the most recent api versions in each track, older api versions must be supported for durations:
	- GA: 12 months or 3 release whichever is longer.
	- Beta: 9 months or 3 release whichever is longer.
	- Alpha: 0 releases.
	
	
kubectl convert command
--------------------------
- whwn a kubernetes cluster being upgraded, we have new API being added and old ones are deprecated and removed. when old apis are removed, we need to uupgrade existing old manifest files to new version.

- ex: a yaml file with Deployment version of v1beta1, now, when kubernetes is upgraded, the beta1 ver. of Deployment is removed and we need to use the v1 ver. going forward. however we may have a lot of yaml files in the old manifest which old v1beta1 in them.

$ kubectl convert -f <old_file> --output-version <new_api_ver>

ex:
$ kubectl convert -f nginx-deployment.yaml --output-version apps/v1

NOTE: the kubectl convert is a separate plugin and may not be readily installed on your k8s cluster. need to install.
$ curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   154  100   154    0     0   8105      0 --:--:-- --:--:-- --:--:--  8105
100 51.8M  100 51.8M    0     0   150M      0 --:--:-- --:--:-- --:--:--  150M

$ chmod +x kubectl-convert 

$ mv kubectl-convert /usr/local/bin/kubectl-convert

$ cat ingress-old.yaml 
---
# Deprecated API version
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: ingress-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /video-service
        pathType: Prefix
        backend:
          serviceName: ingress-svc
          servicePort: 80


$ kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
..
..

$ kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1 > ingress-new.yaml


Exercise:
---------
1/ Enable the v1alpha1 version for rbac.authorization.k8s.io API group on the controlplane node.

- As a good practice, take a backup of that apiserver manifest file before going to make any changes. In case, if anything happens due to misconfiguration you can replace it with the backup file.

root@controlplane:~# cp -v /etc/kubernetes/manifests/kube-apiserver.yaml /root/kube-apiserver.yaml.backup

- Now, open up the kube-apiserver manifest file and add the --runtime-config flag in the command field as follows :-

 - command:
    - kube-apiserver
    - --advertise-address=10.18.17.8
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --runtime-config=rbac.authoriza

- After that kubelet will detect the new changes and will recreate the apiserver pod.

root@controlplane:~# kubectl get po -n kube-system





===============================================================================================

Custom Resource Definition (CRD)

===============================================================================================

- Resource:
	- when we create a resource say Deployment in yaml file, kubernetes creates a deployment object and stores its information in etcd datastore.
	- we can list the deployment
	- we can run the delete command to delete the deployment object.
	- all of these going to add a create, list and delete info for the deployment object in etcd datastore. 
	- Now, when a deployment object is created, the corresponding Deployment Controller is responsible to create the ReplicaSet object which in turn create the no. of pods defined.
	- Deployment controller is built-in that comes along with kubernetes.
	
- Resource Controller:
	- runs in the background and continously monitor the status of the resources that it supposed to manage. when we add create/update/delete commands to etcd datastore, it makes necessary changes in the cluster to match the state.
	- written in Go language.. is part of deployment src code of kubernetes.
	
- every resource types in kubernetes has its corresponding controllers.
- ReplicaSet, Deployment, Job, CronJob, StatefulSet, Namespace these resources has its controllers responsible to watch its status and makes needed changes to match the state as expected.

example of creating a custom resource:

flightticket.yaml
------------------
apiVersion: flights.com/v1
kind: FlightTicket
metadata:
  name: my-flight-ticket
spec:
  from: Mumbai
  to: London
  number: 2
  
- to create a flightticket resource in kubernetes:
$ kubectl create -f flightticket.yaml

- to list all the flightticket objects:
$ kubectl get flightticket

- to delete:
$ kubetcl delete -f flightticket.yaml


- all these commands will create/delete the object in etcd datastore, BUT its not actually booked the flight ticket yet. ex: we have an api to book flights at https://book-flight.com/api 
- now we are going to need a controller (flightticket_controller.go), will watch for creating or updation of flight ticket resource, and on creation/deletetion of it, the controller contacts the https://book-flight.com/api to book or make necessary changes.

	
- Now, before we able to run the kubectl create command for this new custom resource, we need to register that resource in kubernetes using Custom Resource Definition.

- Custom resources can appear and disappear in a running cluster through dynamic registration, and cluster admins can update custom resources independently of the cluster itself. 

- Once a custom resource is installed, users can create and access its objects using kubectl

- When you create a new CustomResourceDefinition (CRD), the Kubernetes API Server creates a new RESTful resource path for each version you specify.

- The CRD can be either namespaced or cluster-scoped, as specified in the CRD's scope field.

- CustomResourceDefinitions themselves are non-namespaced and are available to all namespaces.


flightticker-crd-definition.yaml
---------------------------------
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  # name must match the spec fields below, and be in the form: <plural>.<group>
  name: flighttickets.flights.com
spec:
  # group name to use for REST API: /apis/<group>/<version>
  group: flights.com
  
  # either Namespaced or Cluster
  scope: Namespaced
  names:
    # plural name to be used in the URL: /apis/<group>/<version>/<plural>
    plural: flighttickets
    # singular name to be used as an alias on the CLI and for display
    singular: flightticket
    # kind is normally the CamelCased singular type. Your resource manifests use this.
    kind: FlightTicket
    # shortNames allow shorter string to match your resource on the CLI ex: kubectl get ft
    shortNames:
    - ft
	
  # list of versions supported by this CustomResourceDefinition
  versions:
    - name: v1
      # Each version can be enabled/disabled by Served flag.
      served: true
      # One and only one version must be marked as the storage version.
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                from:
                  type: string
                to:
                  type: string
                number:
                  type: integer
				  minimum: 1    // optional
				  maximum: 10   // optional
				  

$ kubectl create -f flightticker-crd-definition.yaml

- once the CRD is created, we can now create the flightticket object.

- use CRD to create any kind of resource type and specify a schema and add validations. BUT its only going to create these objcet in etcd datastore, its not actually going to do anything about these resource objectives, as we have not created the corresponding custom controller.






==========================================================================================

Custom Controllers

==========================================================================================

- A custom controller is needed to keep monitoring the cluster for creation/updation of its custom resource type objects and take action accordingly. ex: book or cancel flight ticket using the booking api website.

- a control loop is a non-terminating loop that regulates the state of a system.

- In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.


- any process or code that runs in a loop, continously monitoring the kubernetes cluster and listening to events of specific objects being changed (ex: flightticket object), can write in python or Go.

- developing custom controllers in python may be challenging as the internal calls to the k8s api may become expensive also, need to create our own queuing and caching mechanism, developing in go with kubernetes Go client, provides support for other libraries like shared-informer that provides caching and queuing mechanism and helps build controllers easicu in right way.

- kubernets has sample controller github repo, clone and use.

- install Go on machine.

- git clone https://github.com/kubernetes/sample-controller.git

> cd sample-controller

> customize controller.go as per your own logic.

> go build -o sample-controller .

> ./sample-controller -kubeconfig=$HOME/.kube/config
- this authenticated using kube config with the cluster and start watching locally and once any object is created of same type, it makes the necessary actions.

- once the controller code is ready, we package the custom controller in docker image and then run it inside kubernetes cluster as POD or Deployment.






======================================================================================

Deployment Strategy - Blue Green

======================================================================================

Deployment Strategy
-------------------
- 2 basic deployment strategies, kubernetes supports via its config 
	- Recreate strategy: 
		- Upgrade to newer version all at once. NOT the default.
		- the application will be down in between.
		- .spec.strategy.type==Recreate
		
	- RollingUpdate 
		- we do not destroy all of them at once. Instead we take down the older version and bring up a newer version one by one. 
		- RollingUpdate is the default Deployment Strategy.
		- .spec.strategy.type==RollingUpdate
		
Other than these 2, there are few additional deployment strategies that we can not specify as configurations in kubernetes yaml file but can be achieved.

- Blue/Green is a deployment strategy where we have the new version deployed along side the old version. old version is called BLUE and new ver, is GREEN. 100% of the traffic is still routed to the old version, tests are run on new version, once all tests are passed, we switch traffic all at once from BLUE to GREEN.

- best implemented using Service Mesh with Istio.

- can be implemented using native kubernetes deployments and services.

- 2 deployment yamal, blue and green, blue has old ver. v1 and green has new ver. v2
- 1 service, first pointing to old ver. blue (image: myapp-image:1.0) using pod selector as v1, once the green is tested, point to green (image: myapp-image:2.0) using pod selector as v2

myapp-deployment-blue.yaml
---------------------------
apiVresion: v1
kind: Deployment
metadata:
  name: myapp-blue
  labels:
    app: myapp
	type: front-end
spec:
  template:
    metadata:
	  name: myapp-pod
	  labels:
	    version: v1
	spec:
	  containers:
	  - name: app-container
	    image: myapp-image:1.0
	replicas: 5
	selector:
	  matchLabels:
	    type: front-end
		

myapp-service.yaml
------------------
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    version: v1

myapp-deployment-green.yaml
---------------------------
apiVersion: v1
kind: Deployment
metadata:
  name: myapp-green
  labels:
    app: myapp
	type: front-end
spec:
  template:
    metadata:
	  name: myapp-pod
	  labels:
	    version: v2
	spec:
	  containers:
	  - name: app-container
	    image: myapp-image:2.0
	replicas: 5
	selector:
	  matchLabels:
	    type: front-end
		
myapp-service.yaml
------------------
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    version: v2
	




===============================================================================================

Deployment Strategy - Canary

===============================================================================================

- In this strategy we deploy a new version and route only a small percentage to it. the majority of traffic is being routed to the older version. at this we on test and if everything looks good, we upgrade the original deployment with the new version, we may use RollingUpdate to upgrade. then we get rid of the canary deployment.


- this can be achieved using native kubernetes deployments and services, BUT best implemented using Servuce Mesh using Istio.

- 1st deployment named: deployment-primary, where the pod label: version = v1 and a common label: app = front-end with replicas set to 5.
- 2nd deployment named: deployment-canary, where the pod label: version = v2 and a common label: app = front-end with replicas set to 1.
- 1 service pointing to primary initially using pod selector: version = v1
- then after the canary deployment is deployed, point the service to both using pod selector: app = front-end

- Now, Service equally distributes the traffic among the total no. of target pods, hence in this case, total no. pod = 6, hence 83% of traffic gets routed to primary ver. and only 17% of traffic routed to canary deployment.

- implementing canary deployment strategy using depoyment and service, we have limited control over traffic between each deployment, traffic split is always going to get governed by the no. of pods present in each deployment. ex: we can not say just route 1% of traffic to canary deployment, to achieve this we need to have atleast 100 pods all together. thats why Service Mesh like Istio comes with better control with Istio, we can define the exact no. of traffic to be routed between each deployment and it does not depends on no. of pods etc.



myapp-deployment-primary.yaml
---------------------------
apiVresion: v1
kind: Deployment
metadata:
  name: myapp-primary
  labels:
    app: myapp
	type: front-end
spec:
  template:
    metadata:
	  name: myapp-pod
	  labels:
	    version: v1
		app: front-end
	spec:
	  containers:
	  - name: app-container
	    image: myapp-image:1.0
	replicas: 5
	selector:
	  matchLabels:
	    type: front-end
		

myapp-service.yaml
------------------
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: front-end

myapp-deployment-canary.yaml
---------------------------
apiVresion: v1
kind: Deployment
metadata:
  name: myapp-canary
  labels:
    app: myapp
	type: front-end
spec:
  template:
    metadata:
	  name: myapp-pod
	  labels:
	    version: v2
		app: front-end
	spec:
	  containers:
	  - name: app-container
	    image: myapp-image:2.0
	replicas: 1
	selector:
	  matchLabels:
	    type: front-end
		

	

- few kubetcl commands to scale down old ver. on primary deployment and scale up the new canary.

$ kubectl scale deployment frontend --replicas=0 
$ kubectl scale deployment frontend-v2 --replicas=5

- then delete the old deployment
$ kubectl delete deployment frontend




Helm Introduction
=========================================================================
- applications we deploy into kubernetes cluster can become complex. a typical app is usually made up of collection of objects, that need to interconnect to make everything work.

ex: a simple wordpress app may need object like below:

- a Deployment to declare the pod that we may want to run. like mysql DB server, webserver
- a PV to store the DB
- a PVC
- a Service to expose the webserver running in pods.
- a Secret to store the admin password.

and may even more if we want extra stuff like periodic backup etc..

- for every objects we need a separate yaml and kubectl apply to every yaml files. this can be a tedious task. ex: we downloaded the wordpress yaml files and changes certain stuff as per our need like PV to be more that 20Gi, we change 20Gi to 100Gi as weneed more space, hence for changing more, we may have to open every yaml file and change according to our needs.

- updating multiple yaml file with great cara is tedious. 

- Even to delete the app, we need to remember each object that belongs and delete them all one by one.

- we can also write all objects in a single yaml file but that can even become a huge file of 1000 lines and managing becomes again tedious. hence keeping them i separate yaml will better organized.

- kubernets does not care about our app as a whole, it looks all objects as individual objects that needs to be created and not as a group of objects that belong to a single app called Wordpress.

- Helm is package manager for kubernets, it combines each of these object and make a single package to be installed/uninstalled as a whole. instead of editing multiple yaml object files, we have a single file (values.yaml) to declare every custom settings (change PV size, change website name, admin password etc).

- upgrade our app using helm upgrade workpress, helm would know what individual object needs to be changed to make this upgarde. also cal rollback. "helm rollback wordpress"

- "helm uninstall wordpress" to delete all the objects in one shot, instaed of multiple separate command to remove everything.


Install Helm
---------------
prerequisite:
- a kubernetes cluster
- kubectl utility installed and configured on your local desktop with the righ login cred setup on kube config to work with the intented target kubernetes cluster.
- helm can be installed on windows/mac/unix system.

- Identity the name of OS:

$ cat /etc/*release*

DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION="Ubuntu 18.04.5 LTS"
NAME="Ubuntu"
VERSION="18.04.5 LTS (Bionic Beaver)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 18.04.5 LTS"
VERSION_ID="18.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic




- Install the helm package.

root@controlplane:~# curl https://baltocdn.com/helm/signing.asc | apt-key add -

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1700  100  1700    0     0  12686      0 --:--:-- --:--:-- --:--:-- 12686
OK

root@controlplane:~# apt-get install apt-transport-https --yes

Reading package lists... Done
Building dependency tree       
Reading state information... Done
apt-transport-https is already the newest version (1.6.14).
0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.

root@controlplane:~# echo "deb https://baltocdn.com/helm/stable/debian/ all main" | tee /etc/apt/sources.list.d/helm-stable-debian.list

deb https://baltocdn.com/helm/stable/debian/ all main

root@controlplane:~# apt-get update
root@controlplane:~# apt-get install helm

Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following NEW packages will be installed:
  helm
0 upgraded, 1 newly installed, 0 to remove and 53 not upgraded.
Need to get 13.8 MB of archives.
After this operation, 45.7 MB of additional disk space will be used.
Get:1 https://baltocdn.com/helm/stable/debian all/main amd64 helm amd64 3.7.2-1 [13.8 MB]
Fetched 13.8 MB in 0s (54.7 MB/s)
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package helm.
(Reading database ... 15210 files and directories currently installed.)
Preparing to unpack .../helm_3.7.2-1_amd64.deb ...
Unpacking helm (3.7.2-1) ...
Setting up helm (3.7.2-1) ...



$ helm version
version.BuildInfo{Version:"v3.7.2", GitCommit:"663a896f4a815053445eec4153677ddc24a0a361", GitTreeState:"clean", GoVersion:"go1.16.10"}


$ helm help




Helm Concepts
----------------------------------------------------------
- So we have multiple yaml files: deployment.yaml, secret.yaml, pv.yaml, pvc.yaml, service.yaml ecah woth its own component of wordpress application. No we know that each of these yaml might have some params that needs to be changes as per the need.
	- different PV storage size declared in pv.yaml and pvc.yaml
	- separate admin password in secret.yaml etc,
	- diff wordpress image versions in deployment.yaml
	
STEP-1
	- convert these yaml files into templates where these params becomes variables. {{ .Values.image }}, {{ .Values.storage }}, {{ .Values.password }}

ex:

secret.yaml
---------------------
apiVersion: v1
kind: Secret
metadata:
  name: wordpress-admin-password
data:
  key: sGdhdu7829303mcnfjf

	
templates/secret.yaml
---------------------
apiVersion: v1
kind: Secret
metadata:
  name: wordpress-admin-password
data:
  key: {{ .Values.passwordEncoded }}
  

- these values are fetched from values.yaml of helm, this way anyone wants to customize the installation can simply change these valuse in one single file rather than going to multiple files and keeping a track of all the changes.

values.yaml
-----------
image: wordpress:4.8-apache
storage: 20Gi
passwordEncoded: sGdhdu7829303mcnfjf

- hence, combination of the template yaml and values.yaml gives the final objects to be deployed in kubernetes cluster.

- the templates and the values together form a helm chart.




Helm CHART
-------------------------------------------------------
- a single helm chart may be used to deploy a simple app like wordpress,and it will have 
	- all the necessary template files for different objects
	- the values.yaml file with the variables
	- chart.yaml having info about the chart itself. such as name of the chart, version, description, some keywords associated with the application, info about the maintainers.
	
- we can create our own chart for our own application.

- we can download readily available charts for standard apps like Wordpress/Joomla etc from the ArtifactHUB.io 



helm search hub
------------------- 
- ArtifactHUB.io is a community driven repo that stores helm charts. we can serach charts either on this portal or using helm search command.

$ helm search hub <keyword>  // hub to indicate we wish to search in ArtifactHUB.io

ex: helm search hub wordpress




helm repo add
------------------- 
- there are other repo as well, such as bitnami helm repo. to search charts in repo other than ArtifactHUB.io, we need to add the repo to our local help setup.

$ helm repo add bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories




helm repo list
------------------- 
- to list the added repo to your help installation:

$ helm repo list
NAME    URL                               
bitnami https://charts.bitnami.com/bitnami




helm search repo
------------------- 
- Now we can search in all the added repo using:  // use repo instead of hub to search on added repo.

$ helm search repo wordpress

NAME                    CHART VERSION   APP VERSION     DESCRIPTION                                       
bitnami/wordpress       13.0.4          5.8.3           WordPress is the world's most popular blogging 



- to search for the joomla package from the added repository

$ helm search repo joomla

NAME            CHART VERSION   APP VERSION     DESCRIPTION                                       
bitnami/joomla  12.0.3          4.0.6           Joomla! is an award winning open source CMS pla...

$ helm search repo drupal

NAME            CHART VERSION   APP VERSION     DESCRIPTION                                       
bitnami/drupal  11.0.4          9.3.3           One of the most versatile open source content m...


helm install
------------------- 
- Install drupal helm chart from the bitnami repository.
Release name should be bravo.
Chart name should be bitnami/drupal


- the helm install command downloads the chart in tar format, extract and install on kubernetes cluster. Each installation of a chart is called a release and each release has its name. 
- we can install the same helm chart multiple times using multiple release names on the same kubernetes cluster. each release is completely independent of each other.

syntax: helm install <release_name> <chart_name_for_drupal>

$ helm install release-1 bitnami/drupal
$ helm install release-2 bitnami/drupal
$ helm install release-3 bitnami/drupal

$ helm install bravo bitnami/drupal

NAME: bravo
LAST DEPLOYED: Sun Jan 23 10:53:40 2022
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: drupal
CHART VERSION: 11.0.4
APP VERSION: 9.3.3** Please be patient while the chart is being deployed **


 
 
helm list
------------------- 
- command to list the installed packages:

$ helm list

NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
bravo   default         1               2022-01-23 10:53:40.085322542 +0000 UTC deployed        drupal-11.0.4   9.3.3





helm uninstall
------------------- 
- Uninstall the drupal helm package

$ helm uninstall <installed_pck_release_name>

ex: helm uninstall bravo
release "bravo" uninstalled

 
 


helm pull
------------------- 
- Download the bitnami apache package under the /root directory and not install.

$ helm search repo bitnami/apache
NAME            CHART VERSION   APP VERSION     DESCRIPTION                 
bitnami/apache  9.0.1           2.4.52          Chart for Apache HTTP Server

$ helm pull --untar bitnami/apache

$ ls -ltr
total 4
drwxr-xr-x 6 root root 4096 Jan 23 10:58 apache


$ ls -ltr apache/
total 96
-rw-r--r-- 1 root root 26300 Jan 23 10:58 values.yaml
-rw-r--r-- 1 root root  1345 Jan 23 10:58 values.schema.json
drwxr-xr-x 2 root root  4096 Jan 23 10:58 templates
drwxr-xr-x 3 root root  4096 Jan 23 10:58 files
drwxr-xr-x 2 root root  4096 Jan 23 10:58 ci
drwxr-xr-x 3 root root  4096 Jan 23 10:58 charts
-rw-r--r-- 1 root root 37440 Jan 23 10:58 README.md
-rw-r--r-- 1 root root   619 Jan 23 10:58 Chart.yaml






helm install from already pulled charts
------------------------------------------ 
- Install the apache from the downloaded helm package.
Release name: mywebapp

$ helm install mywebapp ./apache

$ helm list     
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART       APP VERSION
mywebapp        default         1               2021-10-06 16:46:40.278694926 +0000 UTC deployed        apache-8.8.32.4.50




Shortcuts
=========================================================================
po 		--------->		PODs
rs		--------->		ReplicaSet
deploy	--------->		Deployments
svc		--------->		Sevice
ns		--------->		Namespace
netpol	--------->		Network Policies
pv		--------->		Persistent Volumes
pvc		--------->		Persistent Volumes Claims
sa 		--------->		Service accounts
cm		--------->		ConfigMap



===========================================================================
K8S Labs
------------

$ watch kubectl get nodes


- Load kubectl shell completions for your current shell session:
With completions loaded, you can press tab to autocomplete or list available completions as you enter kubectl commands.

$ source <(kubectl completion bash)



- Use the explain command to get an explanation of a Pod's spec (press space to page through the output):
$ kubectl explain Pod.spec | more
$ kubectl explain <Resource_Kind>.<Path_To_Field>
$ kubectl explain Pod.spec.containers.image


$ kubectl get po --show-labels
NAME             READY   STATUS    RESTARTS   AGE   LABELS
blue-backend     1/1     Running   0          30s   color=blue,tier=backend
green-frontend   1/1     Running   0          30s   color=green,tier=frontend
red-backend      1/1     Running   0          30s   color=red,tier=backend
red-frontend     1/1     Running   0          30s   color=red,tier=frontend


$ kubectl get po -L color,tier
NAME             READY   STATUS    RESTARTS   AGE   COLOR   TIER
blue-backend     1/1     Running   0          62s   blue    backend
green-frontend   1/1     Running   0          62s   green   frontend
red-backend      1/1     Running   0          62s   red     backend
red-frontend     1/1     Running   0          62s   red     frontend


- Use the -l (or --selector) option to select all Pods with a color label:
Specifying only a label key as a selector will select all resources with that label defined. 

$ kubectl get pods -L color,tier -l color


- Select all Pods that do not have a color label:
$ kubectl get po -L color,tier -l '!color'
No resources found in labels namespace.


- Select all Pods that have the color red:

$ kubectl get po --selector color=red
NAME           READY   STATUS    RESTARTS   AGE
red-backend    1/1     Running   0          4m50s
red-frontend   1/1     Running   0          4m50s


$ kubectl get po -l color=red
NAME           READY   STATUS    RESTARTS   AGE
red-backend    1/1     Running   0          5m4s
red-frontend   1/1     Running   0          5m4s



- Select all Pods that have the color red and are not in frontend tier:

$ kubectl get po -L color,tier -l 'color=red,tier!=frontend'
NAME          READY   STATUS    RESTARTS   AGE     COLOR   TIER
red-backend   1/1     Running   0          8m32s   red     backend


- Select all Pods with green or blue color:

$ kubectl get po -L color,tier -l 'color in (blue,green)'
NAME             READY   STATUS    RESTARTS   AGE   COLOR   TIER
blue-backend     1/1     Running   0          10m   blue    backend
green-frontend   1/1     Running   0          10m   green   frontend


- Use the describe command to display the annotation for the red-frontend Pod:
$ kubectl describe po red-frontend | grep Annotation
Annotations:  Lab: Kubernetes Pod Design for Application Developers


Remove the Pod's Lab annotation and verify it no longer exists:

kubectl annotate pod red-frontend Lab- &&
kubectl describe pod red-frontend | grep Annotations -A 2

kubectl annotate --help
kubectl label --help







